
==> Audit <==
|---------|--------------------------------|----------|---------------|---------|---------------------|---------------------|
| Command |              Args              | Profile  |     User      | Version |     Start Time      |      End Time       |
|---------|--------------------------------|----------|---------------|---------|---------------------|---------------------|
| stop    |                                | minikube | santhasoruban | v1.34.0 | 28 Nov 24 19:20 IST | 28 Nov 24 19:21 IST |
| start   | --nodes 3                      | minikube | santhasoruban | v1.34.0 | 28 Nov 24 19:21 IST | 28 Nov 24 19:21 IST |
| node    | add                            | minikube | santhasoruban | v1.34.0 | 28 Nov 24 19:22 IST | 28 Nov 24 19:23 IST |
| node    | remove minikube-03             | minikube | santhasoruban | v1.34.0 | 28 Nov 24 19:26 IST |                     |
| node    | delete minikube-03             | minikube | santhasoruban | v1.34.0 | 28 Nov 24 19:26 IST |                     |
| node    | delete minikube-03             | minikube | santhasoruban | v1.34.0 | 28 Nov 24 19:27 IST |                     |
| node    | delete minikube-03             | minikube | root          | v1.34.0 | 28 Nov 24 19:27 IST |                     |
| node    | delete minikube-m03            | minikube | santhasoruban | v1.34.0 | 28 Nov 24 19:27 IST | 28 Nov 24 19:27 IST |
| node    | add                            | minikube | santhasoruban | v1.34.0 | 28 Nov 24 19:28 IST | 28 Nov 24 19:28 IST |
| node    | delete minikube-m03            | minikube | santhasoruban | v1.34.0 | 28 Nov 24 19:30 IST | 28 Nov 24 19:30 IST |
| node    | list                           | minikube | santhasoruban | v1.34.0 | 28 Nov 24 19:51 IST |                     |
| start   |                                | minikube | santhasoruban | v1.34.0 | 29 Nov 24 11:40 IST | 29 Nov 24 11:40 IST |
| stop    |                                | minikube | santhasoruban | v1.34.0 | 29 Nov 24 15:31 IST | 29 Nov 24 15:31 IST |
| delete  |                                | minikube | santhasoruban | v1.34.0 | 29 Nov 24 15:31 IST | 29 Nov 24 15:31 IST |
| start   |                                | minikube | santhasoruban | v1.34.0 | 29 Nov 24 15:31 IST | 29 Nov 24 15:32 IST |
| stop    |                                | minikube | santhasoruban | v1.34.0 | 29 Nov 24 17:04 IST | 29 Nov 24 17:04 IST |
| start   |                                | minikube | santhasoruban | v1.34.0 | 29 Nov 24 17:09 IST | 29 Nov 24 17:10 IST |
| start   |                                | minikube | santhasoruban | v1.34.0 | 30 Nov 24 13:59 IST | 30 Nov 24 14:00 IST |
| addons  | enable ingress                 | minikube | santhasoruban | v1.34.0 | 30 Nov 24 17:09 IST | 30 Nov 24 17:09 IST |
| addons  | list                           | minikube | santhasoruban | v1.34.0 | 30 Nov 24 17:09 IST | 30 Nov 24 17:09 IST |
| stop    |                                | minikube | santhasoruban | v1.34.0 | 30 Nov 24 17:10 IST | 30 Nov 24 17:11 IST |
| start   |                                | minikube | santhasoruban | v1.34.0 | 30 Nov 24 17:11 IST | 30 Nov 24 17:11 IST |
| addons  | disable ingress                | minikube | santhasoruban | v1.34.0 | 30 Nov 24 17:15 IST | 30 Nov 24 17:15 IST |
| addons  | enable ingress                 | minikube | santhasoruban | v1.34.0 | 30 Nov 24 17:16 IST | 30 Nov 24 17:16 IST |
| ip      |                                | minikube | santhasoruban | v1.34.0 | 30 Nov 24 17:42 IST | 30 Nov 24 17:42 IST |
| service | aqua-svc                       | minikube | santhasoruban | v1.34.0 | 30 Nov 24 17:48 IST | 30 Nov 24 17:50 IST |
| ip      |                                | minikube | santhasoruban | v1.34.0 | 30 Nov 24 17:57 IST | 30 Nov 24 17:57 IST |
| service | -n ingress-nginx               | minikube | santhasoruban | v1.34.0 | 30 Nov 24 18:02 IST |                     |
|         | nginx-ingress-controller       |          |               |         |                     |                     |
| service | -n ingress-nginx               | minikube | santhasoruban | v1.34.0 | 30 Nov 24 18:02 IST |                     |
|         | nginx-ingress-controller       |          |               |         |                     |                     |
| service | -n default                     | minikube | santhasoruban | v1.34.0 | 30 Nov 24 18:02 IST |                     |
|         | nginx-ingress-controller       |          |               |         |                     |                     |
| service | -n ingress-nginx nginx-ingress | minikube | santhasoruban | v1.34.0 | 30 Nov 24 18:07 IST | 30 Nov 24 18:08 IST |
| service | -n ingress-nginx nginx-ingress | minikube | santhasoruban | v1.34.0 | 30 Nov 24 18:08 IST | 30 Nov 24 18:09 IST |
| service | aqua-svc                       | minikube | santhasoruban | v1.34.0 | 30 Nov 24 18:12 IST | 30 Nov 24 18:13 IST |
| service | olive-svc                      | minikube | santhasoruban | v1.34.0 | 30 Nov 24 18:13 IST | 30 Nov 24 18:13 IST |
| service | maroon-svc                     | minikube | santhasoruban | v1.34.0 | 30 Nov 24 18:13 IST | 30 Nov 24 18:13 IST |
| ip      |                                | minikube | santhasoruban | v1.34.0 | 30 Nov 24 18:21 IST | 30 Nov 24 18:21 IST |
| stop    |                                | minikube | santhasoruban | v1.34.0 | 30 Nov 24 18:23 IST | 30 Nov 24 18:23 IST |
| start   |                                | minikube | santhasoruban | v1.34.0 | 30 Nov 24 18:23 IST | 30 Nov 24 18:24 IST |
| stop    |                                | minikube | santhasoruban | v1.34.0 | 30 Nov 24 18:24 IST | 30 Nov 24 18:25 IST |
| start   |                                | minikube | santhasoruban | v1.34.0 | 01 Dec 24 10:41 IST | 01 Dec 24 10:41 IST |
| ip      |                                | minikube | santhasoruban | v1.34.0 | 01 Dec 24 13:22 IST | 01 Dec 24 13:22 IST |
| stop    |                                | minikube | santhasoruban | v1.34.0 | 01 Dec 24 14:05 IST | 01 Dec 24 14:05 IST |
| start   |                                | minikube | santhasoruban | v1.34.0 | 02 Dec 24 09:09 IST | 02 Dec 24 09:10 IST |
| service | aqua-svc                       | minikube | santhasoruban | v1.34.0 | 02 Dec 24 11:25 IST | 02 Dec 24 11:25 IST |
| service | olive-svc                      | minikube | santhasoruban | v1.34.0 | 02 Dec 24 11:25 IST | 02 Dec 24 11:26 IST |
| ip      |                                | minikube | santhasoruban | v1.34.0 | 02 Dec 24 11:30 IST | 02 Dec 24 11:30 IST |
| service | awua-svc                       | minikube | santhasoruban | v1.34.0 | 02 Dec 24 11:33 IST |                     |
| service | aquaqua-svc                    | minikube | santhasoruban | v1.34.0 | 02 Dec 24 11:33 IST |                     |
| service | aqua-svc                       | minikube | santhasoruban | v1.34.0 | 02 Dec 24 11:33 IST | 02 Dec 24 11:33 IST |
| stop    |                                | minikube | santhasoruban | v1.34.0 | 02 Dec 24 11:36 IST | 02 Dec 24 11:37 IST |
| start   |                                | minikube | santhasoruban | v1.34.0 | 02 Dec 24 12:31 IST | 02 Dec 24 12:31 IST |
| start   |                                | minikube | santhasoruban | v1.34.0 | 03 Dec 24 08:57 IST | 03 Dec 24 08:57 IST |
| start   |                                | minikube | santhasoruban | v1.34.0 | 03 Dec 24 10:04 IST | 03 Dec 24 10:05 IST |
| ip      |                                | minikube | santhasoruban | v1.34.0 | 03 Dec 24 13:36 IST | 03 Dec 24 13:36 IST |
| stop    |                                | minikube | santhasoruban | v1.34.0 | 03 Dec 24 13:41 IST | 03 Dec 24 13:41 IST |
| stop    |                                | minikube | santhasoruban | v1.34.0 | 03 Dec 24 13:41 IST |                     |
| start   |                                | minikube | santhasoruban | v1.34.0 | 03 Dec 24 13:42 IST | 03 Dec 24 13:42 IST |
| start   |                                | minikube | santhasoruban | v1.34.0 | 04 Dec 24 08:32 IST | 04 Dec 24 08:32 IST |
| start   |                                | minikube | santhasoruban | v1.34.0 | 17 Dec 24 10:17 IST |                     |
| start   |                                | minikube | santhasoruban | v1.34.0 | 17 Dec 24 10:18 IST | 17 Dec 24 10:18 IST |
|---------|--------------------------------|----------|---------------|---------|---------------------|---------------------|


==> Last Start <==
Log file created at: 2024/12/17 10:18:04
Running on machine: EPINHYDW1154
Binary: Built with gc go1.22.5 for linux/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I1217 10:18:04.519270     270 out.go:345] Setting OutFile to fd 1 ...
I1217 10:18:04.519407     270 out.go:358] Setting ErrFile to fd 2...
I1217 10:18:04.519656     270 root.go:338] Updating PATH: /home/santhasoruban/.minikube/bin
I1217 10:18:04.519951     270 out.go:352] Setting JSON to false
I1217 10:18:04.520845     270 start.go:129] hostinfo: {"hostname":"EPINHYDW1154","uptime":46,"bootTime":1734410839,"procs":11,"os":"linux","platform":"ubuntu","platformFamily":"debian","platformVersion":"18.04","kernelVersion":"5.10.102.1-microsoft-standard-WSL2","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"guest","hostId":"6f7348ae-6282-449b-a491-e0081a8bf6b6"}
I1217 10:18:04.520908     270 start.go:139] virtualization:  guest
I1217 10:18:04.525198     270 out.go:177] 😄  minikube v1.34.0 on Ubuntu 18.04 (amd64)
I1217 10:18:04.527852     270 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.31.0
I1217 10:18:04.527917     270 driver.go:394] Setting default libvirt URI to qemu:///system
I1217 10:18:04.528826     270 notify.go:220] Checking for updates...
I1217 10:18:04.561875     270 docker.go:123] docker version: linux-24.0.2:Docker Engine - Community
I1217 10:18:04.561979     270 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1217 10:18:04.809863     270 info.go:266] docker info: {ID:934445b9-4a1e-4f95-a887-54a2b325f36f Containers:4 ContainersRunning:0 ContainersPaused:0 ContainersStopped:4 Images:12 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:24 OomKillDisable:true NGoroutines:40 SystemTime:2024-12-17 10:18:04.802165784 +0530 IST LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:0 KernelVersion:5.10.102.1-microsoft-standard-WSL2 OperatingSystem:Ubuntu 18.04.6 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:18 MemTotal:16487514112 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:EPINHYDW1154 Labels:[] ExperimentalBuild:false ServerVersion:24.0.2 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:3dce8eb055cbb6872793272b4f20ed16117344f8 Expected:3dce8eb055cbb6872793272b4f20ed16117344f8} RuncCommit:{ID:v1.1.7-0-g860f061 Expected:v1.1.7-0-g860f061} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=builtin] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.10.5] map[Name:compose Path:/usr/libexec/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.18.1]] Warnings:<nil>}}
I1217 10:18:04.809934     270 docker.go:318] overlay module found
I1217 10:18:04.816553     270 out.go:177] ✨  Using the docker driver based on existing profile
I1217 10:18:04.818476     270 start.go:297] selected driver: docker
I1217 10:18:04.818488     270 start.go:901] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:docker.io/kicbase/stable:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 Memory:3900 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true ingress:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/santhasoruban:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I1217 10:18:04.818571     270 start.go:912] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I1217 10:18:04.818672     270 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1217 10:18:04.865779     270 info.go:266] docker info: {ID:934445b9-4a1e-4f95-a887-54a2b325f36f Containers:4 ContainersRunning:0 ContainersPaused:0 ContainersStopped:4 Images:12 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:24 OomKillDisable:true NGoroutines:40 SystemTime:2024-12-17 10:18:04.858879919 +0530 IST LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:0 KernelVersion:5.10.102.1-microsoft-standard-WSL2 OperatingSystem:Ubuntu 18.04.6 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:18 MemTotal:16487514112 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:EPINHYDW1154 Labels:[] ExperimentalBuild:false ServerVersion:24.0.2 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:3dce8eb055cbb6872793272b4f20ed16117344f8 Expected:3dce8eb055cbb6872793272b4f20ed16117344f8} RuncCommit:{ID:v1.1.7-0-g860f061 Expected:v1.1.7-0-g860f061} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=builtin] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.10.5] map[Name:compose Path:/usr/libexec/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.18.1]] Warnings:<nil>}}
I1217 10:18:04.866635     270 cni.go:84] Creating CNI manager for ""
I1217 10:18:04.866648     270 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1217 10:18:04.866685     270 start.go:340] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:docker.io/kicbase/stable:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 Memory:3900 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true ingress:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/santhasoruban:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I1217 10:18:04.869442     270 out.go:177] 👍  Starting "minikube" primary control-plane node in "minikube" cluster
I1217 10:18:04.871476     270 cache.go:121] Beginning downloading kic base image for docker with docker
I1217 10:18:04.873594     270 out.go:177] 🚜  Pulling base image v0.0.45 ...
I1217 10:18:04.875460     270 preload.go:131] Checking if preload exists for k8s version v1.31.0 and runtime docker
I1217 10:18:04.875502     270 preload.go:146] Found local preload: /home/santhasoruban/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.31.0-docker-overlay2-amd64.tar.lz4
I1217 10:18:04.875507     270 cache.go:56] Caching tarball of preloaded images
I1217 10:18:04.875564     270 image.go:79] Checking for docker.io/kicbase/stable:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 in local docker daemon
I1217 10:18:04.875580     270 preload.go:172] Found /home/santhasoruban/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.31.0-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I1217 10:18:04.875586     270 cache.go:59] Finished verifying existence of preloaded tar for v1.31.0 on docker
I1217 10:18:04.875710     270 profile.go:143] Saving config to /home/santhasoruban/.minikube/profiles/minikube/config.json ...
W1217 10:18:04.894423     270 image.go:95] image docker.io/kicbase/stable:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 is of wrong architecture
I1217 10:18:04.894431     270 cache.go:149] Downloading docker.io/kicbase/stable:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 to local cache
I1217 10:18:04.894537     270 image.go:63] Checking for docker.io/kicbase/stable:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 in local cache directory
I1217 10:18:04.895121     270 image.go:66] Found docker.io/kicbase/stable:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 in local cache directory, skipping pull
I1217 10:18:04.895126     270 image.go:135] docker.io/kicbase/stable:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 exists in cache, skipping pull
I1217 10:18:04.895133     270 cache.go:152] successfully saved docker.io/kicbase/stable:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 as a tarball
I1217 10:18:04.895137     270 cache.go:162] Loading docker.io/kicbase/stable:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 from local cache
I1217 10:18:05.935998     270 cache.go:164] successfully loaded and using docker.io/kicbase/stable:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 from cached tarball
I1217 10:18:05.936037     270 cache.go:194] Successfully downloaded all kic artifacts
I1217 10:18:05.936086     270 start.go:360] acquireMachinesLock for minikube: {Name:mkc20bb33483c1d8302ed231142bb22377815639 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I1217 10:18:05.936901     270 start.go:364] duration metric: took 773.736µs to acquireMachinesLock for "minikube"
I1217 10:18:05.936938     270 start.go:96] Skipping create...Using existing machine configuration
I1217 10:18:05.936954     270 fix.go:54] fixHost starting: 
I1217 10:18:05.937199     270 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1217 10:18:05.960283     270 fix.go:112] recreateIfNeeded on minikube: state=Stopped err=<nil>
W1217 10:18:05.960304     270 fix.go:138] unexpected machine state, will restart: <nil>
I1217 10:18:05.964081     270 out.go:177] 🔄  Restarting existing docker container for "minikube" ...
I1217 10:18:05.967522     270 cli_runner.go:164] Run: docker start minikube
I1217 10:18:06.526183     270 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1217 10:18:06.551512     270 kic.go:430] container "minikube" state is running.
I1217 10:18:06.552287     270 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1217 10:18:06.573059     270 profile.go:143] Saving config to /home/santhasoruban/.minikube/profiles/minikube/config.json ...
I1217 10:18:06.573246     270 machine.go:93] provisionDockerMachine start ...
I1217 10:18:06.573290     270 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1217 10:18:06.592671     270 main.go:141] libmachine: Using SSH client type: native
I1217 10:18:06.594309     270 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82f9c0] 0x832720 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I1217 10:18:06.594319     270 main.go:141] libmachine: About to run SSH command:
hostname
I1217 10:18:06.595700     270 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: read tcp 127.0.0.1:43626->127.0.0.1:32772: read: connection reset by peer
I1217 10:18:09.757078     270 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I1217 10:18:09.757101     270 ubuntu.go:169] provisioning hostname "minikube"
I1217 10:18:09.757168     270 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1217 10:18:09.776187     270 main.go:141] libmachine: Using SSH client type: native
I1217 10:18:09.776377     270 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82f9c0] 0x832720 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I1217 10:18:09.776384     270 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I1217 10:18:09.942888     270 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I1217 10:18:09.942979     270 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1217 10:18:09.958719     270 main.go:141] libmachine: Using SSH client type: native
I1217 10:18:09.958840     270 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82f9c0] 0x832720 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I1217 10:18:09.958847     270 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I1217 10:18:10.098753     270 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I1217 10:18:10.098797     270 ubuntu.go:175] set auth options {CertDir:/home/santhasoruban/.minikube CaCertPath:/home/santhasoruban/.minikube/certs/ca.pem CaPrivateKeyPath:/home/santhasoruban/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/santhasoruban/.minikube/machines/server.pem ServerKeyPath:/home/santhasoruban/.minikube/machines/server-key.pem ClientKeyPath:/home/santhasoruban/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/santhasoruban/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/santhasoruban/.minikube}
I1217 10:18:10.098844     270 ubuntu.go:177] setting up certificates
I1217 10:18:10.098854     270 provision.go:84] configureAuth start
I1217 10:18:10.098920     270 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1217 10:18:10.131735     270 provision.go:143] copyHostCerts
I1217 10:18:10.133130     270 exec_runner.go:144] found /home/santhasoruban/.minikube/ca.pem, removing ...
I1217 10:18:10.133146     270 exec_runner.go:203] rm: /home/santhasoruban/.minikube/ca.pem
I1217 10:18:10.133210     270 exec_runner.go:151] cp: /home/santhasoruban/.minikube/certs/ca.pem --> /home/santhasoruban/.minikube/ca.pem (1099 bytes)
I1217 10:18:10.133834     270 exec_runner.go:144] found /home/santhasoruban/.minikube/cert.pem, removing ...
I1217 10:18:10.133843     270 exec_runner.go:203] rm: /home/santhasoruban/.minikube/cert.pem
I1217 10:18:10.133889     270 exec_runner.go:151] cp: /home/santhasoruban/.minikube/certs/cert.pem --> /home/santhasoruban/.minikube/cert.pem (1139 bytes)
I1217 10:18:10.134611     270 exec_runner.go:144] found /home/santhasoruban/.minikube/key.pem, removing ...
I1217 10:18:10.134621     270 exec_runner.go:203] rm: /home/santhasoruban/.minikube/key.pem
I1217 10:18:10.134659     270 exec_runner.go:151] cp: /home/santhasoruban/.minikube/certs/key.pem --> /home/santhasoruban/.minikube/key.pem (1675 bytes)
I1217 10:18:10.135153     270 provision.go:117] generating server cert: /home/santhasoruban/.minikube/machines/server.pem ca-key=/home/santhasoruban/.minikube/certs/ca.pem private-key=/home/santhasoruban/.minikube/certs/ca-key.pem org=santhasoruban.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I1217 10:18:10.266933     270 provision.go:177] copyRemoteCerts
I1217 10:18:10.266998     270 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I1217 10:18:10.267034     270 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1217 10:18:10.287671     270 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/santhasoruban/.minikube/machines/minikube/id_rsa Username:docker}
I1217 10:18:10.396842     270 ssh_runner.go:362] scp /home/santhasoruban/.minikube/machines/server.pem --> /etc/docker/server.pem (1196 bytes)
I1217 10:18:10.444203     270 ssh_runner.go:362] scp /home/santhasoruban/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I1217 10:18:10.477563     270 ssh_runner.go:362] scp /home/santhasoruban/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1099 bytes)
I1217 10:18:10.510318     270 provision.go:87] duration metric: took 411.452834ms to configureAuth
I1217 10:18:10.510337     270 ubuntu.go:193] setting minikube options for container-runtime
I1217 10:18:10.510473     270 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.31.0
I1217 10:18:10.510509     270 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1217 10:18:10.533979     270 main.go:141] libmachine: Using SSH client type: native
I1217 10:18:10.534109     270 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82f9c0] 0x832720 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I1217 10:18:10.534113     270 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I1217 10:18:10.624570     270 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I1217 10:18:10.624585     270 ubuntu.go:71] root file system type: overlay
I1217 10:18:10.624682     270 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I1217 10:18:10.624737     270 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1217 10:18:10.649542     270 main.go:141] libmachine: Using SSH client type: native
I1217 10:18:10.649844     270 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82f9c0] 0x832720 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I1217 10:18:10.649933     270 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I1217 10:18:10.817157     270 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I1217 10:18:10.817264     270 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1217 10:18:10.846994     270 main.go:141] libmachine: Using SSH client type: native
I1217 10:18:10.847221     270 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82f9c0] 0x832720 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I1217 10:18:10.847236     270 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I1217 10:18:11.022909     270 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I1217 10:18:11.022938     270 machine.go:96] duration metric: took 4.449680408s to provisionDockerMachine
I1217 10:18:11.022956     270 start.go:293] postStartSetup for "minikube" (driver="docker")
I1217 10:18:11.022972     270 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I1217 10:18:11.023063     270 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I1217 10:18:11.023202     270 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1217 10:18:11.056115     270 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/santhasoruban/.minikube/machines/minikube/id_rsa Username:docker}
I1217 10:18:11.185870     270 ssh_runner.go:195] Run: cat /etc/os-release
I1217 10:18:11.189888     270 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I1217 10:18:11.189919     270 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I1217 10:18:11.189925     270 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I1217 10:18:11.189932     270 info.go:137] Remote host: Ubuntu 22.04.4 LTS
I1217 10:18:11.189944     270 filesync.go:126] Scanning /home/santhasoruban/.minikube/addons for local assets ...
I1217 10:18:11.190502     270 filesync.go:126] Scanning /home/santhasoruban/.minikube/files for local assets ...
I1217 10:18:11.191232     270 start.go:296] duration metric: took 168.263195ms for postStartSetup
I1217 10:18:11.191318     270 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I1217 10:18:11.191355     270 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1217 10:18:11.214077     270 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/santhasoruban/.minikube/machines/minikube/id_rsa Username:docker}
I1217 10:18:11.327026     270 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I1217 10:18:11.332588     270 fix.go:56] duration metric: took 5.395639294s for fixHost
I1217 10:18:11.332600     270 start.go:83] releasing machines lock for "minikube", held for 5.395680132s
I1217 10:18:11.332655     270 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1217 10:18:11.355079     270 ssh_runner.go:195] Run: cat /version.json
I1217 10:18:11.355114     270 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1217 10:18:11.355199     270 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I1217 10:18:11.355291     270 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1217 10:18:11.374681     270 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/santhasoruban/.minikube/machines/minikube/id_rsa Username:docker}
I1217 10:18:11.377736     270 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/santhasoruban/.minikube/machines/minikube/id_rsa Username:docker}
I1217 10:18:11.478206     270 ssh_runner.go:195] Run: systemctl --version
I1217 10:18:11.489699     270 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I1217 10:18:11.842542     270 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I1217 10:18:11.873157     270 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I1217 10:18:11.873211     270 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I1217 10:18:11.885560     270 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I1217 10:18:11.885579     270 start.go:495] detecting cgroup driver to use...
I1217 10:18:11.885609     270 detect.go:187] detected "cgroupfs" cgroup driver on host os
I1217 10:18:11.885734     270 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I1217 10:18:11.906689     270 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10"|' /etc/containerd/config.toml"
I1217 10:18:11.920343     270 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I1217 10:18:11.932556     270 containerd.go:146] configuring containerd to use "cgroupfs" as cgroup driver...
I1217 10:18:11.932613     270 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I1217 10:18:11.942042     270 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I1217 10:18:11.953047     270 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I1217 10:18:11.963974     270 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I1217 10:18:11.973820     270 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I1217 10:18:11.984818     270 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I1217 10:18:11.996535     270 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I1217 10:18:12.007604     270 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I1217 10:18:12.018813     270 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I1217 10:18:12.028452     270 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I1217 10:18:12.036945     270 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1217 10:18:12.167014     270 ssh_runner.go:195] Run: sudo systemctl restart containerd
I1217 10:18:12.317287     270 start.go:495] detecting cgroup driver to use...
I1217 10:18:12.317346     270 detect.go:187] detected "cgroupfs" cgroup driver on host os
I1217 10:18:12.317402     270 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I1217 10:18:12.336458     270 cruntime.go:279] skipping containerd shutdown because we are bound to it
I1217 10:18:12.336514     270 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I1217 10:18:12.356768     270 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I1217 10:18:12.378129     270 ssh_runner.go:195] Run: which cri-dockerd
I1217 10:18:12.382465     270 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I1217 10:18:12.393226     270 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (190 bytes)
I1217 10:18:12.413652     270 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I1217 10:18:12.683597     270 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I1217 10:18:13.176708     270 docker.go:574] configuring docker to use "cgroupfs" as cgroup driver...
I1217 10:18:13.176860     270 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I1217 10:18:13.284967     270 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1217 10:18:13.408870     270 ssh_runner.go:195] Run: sudo systemctl restart docker
I1217 10:18:14.722039     270 ssh_runner.go:235] Completed: sudo systemctl restart docker: (1.31313243s)
I1217 10:18:14.722100     270 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I1217 10:18:14.733922     270 ssh_runner.go:195] Run: sudo systemctl stop cri-docker.socket
I1217 10:18:14.748451     270 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I1217 10:18:14.758987     270 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I1217 10:18:14.862621     270 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I1217 10:18:14.945888     270 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1217 10:18:15.079508     270 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I1217 10:18:15.101698     270 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I1217 10:18:15.118664     270 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1217 10:18:15.238224     270 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I1217 10:18:15.684934     270 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock
I1217 10:18:15.685010     270 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I1217 10:18:15.688695     270 start.go:563] Will wait 60s for crictl version
I1217 10:18:15.688736     270 ssh_runner.go:195] Run: which crictl
I1217 10:18:15.692545     270 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I1217 10:18:15.878950     270 start.go:579] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  27.2.0
RuntimeApiVersion:  v1
I1217 10:18:15.879021     270 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1217 10:18:16.021474     270 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1217 10:18:16.060749     270 out.go:235] 🐳  Preparing Kubernetes v1.31.0 on Docker 27.2.0 ...
I1217 10:18:16.060949     270 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I1217 10:18:16.079717     270 ssh_runner.go:195] Run: grep 192.168.49.1	host.minikube.internal$ /etc/hosts
I1217 10:18:16.084003     270 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.49.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1217 10:18:16.096993     270 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1217 10:18:16.113974     270 kubeadm.go:883] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:docker.io/kicbase/stable:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 Memory:3900 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true ingress:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/santhasoruban:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I1217 10:18:16.114077     270 preload.go:131] Checking if preload exists for k8s version v1.31.0 and runtime docker
I1217 10:18:16.114115     270 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1217 10:18:16.139719     270 docker.go:685] Got preloaded images: -- stdout --
bitnami/wordpress:6.7.1-debian-12-r1
bitnami/nginx:1.27.3-debian-12-r0
bitnami/mariadb:11.4.4-debian-12-r0
registry.k8s.io/ingress-nginx/controller:<none>
registry.k8s.io/ingress-nginx/controller:<none>
registry.k8s.io/ingress-nginx/kube-webhook-certgen:<none>
registry.k8s.io/kube-controller-manager:v1.31.0
registry.k8s.io/kube-scheduler:v1.31.0
registry.k8s.io/kube-apiserver:v1.31.0
registry.k8s.io/kube-proxy:v1.31.0
registry.k8s.io/etcd:3.5.15-0
registry.k8s.io/pause:3.10
registry.k8s.io/coredns/coredns:v1.11.1
gcr.io/k8s-minikube/storage-provisioner:v5
sbeliakou/color:v1
sbeliakou/web-pod-info:v1
busybox:1.28
busybox:1.27
strm/helloworld-http:latest

-- /stdout --
I1217 10:18:16.139772     270 docker.go:615] Images already preloaded, skipping extraction
I1217 10:18:16.139892     270 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1217 10:18:16.166652     270 docker.go:685] Got preloaded images: -- stdout --
bitnami/wordpress:6.7.1-debian-12-r1
bitnami/nginx:1.27.3-debian-12-r0
bitnami/mariadb:11.4.4-debian-12-r0
registry.k8s.io/ingress-nginx/controller:<none>
registry.k8s.io/ingress-nginx/controller:<none>
registry.k8s.io/ingress-nginx/kube-webhook-certgen:<none>
registry.k8s.io/kube-controller-manager:v1.31.0
registry.k8s.io/kube-apiserver:v1.31.0
registry.k8s.io/kube-scheduler:v1.31.0
registry.k8s.io/kube-proxy:v1.31.0
registry.k8s.io/etcd:3.5.15-0
registry.k8s.io/pause:3.10
registry.k8s.io/coredns/coredns:v1.11.1
gcr.io/k8s-minikube/storage-provisioner:v5
sbeliakou/color:v1
sbeliakou/web-pod-info:v1
busybox:1.28
busybox:1.27
strm/helloworld-http:latest

-- /stdout --
I1217 10:18:16.166665     270 cache_images.go:84] Images are preloaded, skipping loading
I1217 10:18:16.166672     270 kubeadm.go:934] updating node { 192.168.49.2 8443 v1.31.0 docker true true} ...
I1217 10:18:16.166762     270 kubeadm.go:946] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.31.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I1217 10:18:16.166814     270 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I1217 10:18:16.410545     270 cni.go:84] Creating CNI manager for ""
I1217 10:18:16.410563     270 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1217 10:18:16.410575     270 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I1217 10:18:16.410594     270 kubeadm.go:181] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.31.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I1217 10:18:16.410715     270 kubeadm.go:187] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.31.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I1217 10:18:16.410764     270 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.31.0
I1217 10:18:16.426357     270 binaries.go:44] Found k8s binaries, skipping transfer
I1217 10:18:16.426479     270 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I1217 10:18:16.439502     270 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I1217 10:18:16.464626     270 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I1217 10:18:16.489080     270 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2150 bytes)
I1217 10:18:16.516976     270 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I1217 10:18:16.522184     270 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1217 10:18:16.535687     270 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1217 10:18:16.655103     270 ssh_runner.go:195] Run: sudo systemctl start kubelet
I1217 10:18:16.674551     270 certs.go:68] Setting up /home/santhasoruban/.minikube/profiles/minikube for IP: 192.168.49.2
I1217 10:18:16.674562     270 certs.go:194] generating shared ca certs ...
I1217 10:18:16.674577     270 certs.go:226] acquiring lock for ca certs: {Name:mka5141aaf5d92ecdf3b30f0d5d683e07c187bef Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1217 10:18:16.675733     270 certs.go:235] skipping valid "minikubeCA" ca cert: /home/santhasoruban/.minikube/ca.key
I1217 10:18:16.676242     270 certs.go:235] skipping valid "proxyClientCA" ca cert: /home/santhasoruban/.minikube/proxy-client-ca.key
I1217 10:18:16.676254     270 certs.go:256] generating profile certs ...
I1217 10:18:16.676997     270 certs.go:359] skipping valid signed profile cert regeneration for "minikube-user": /home/santhasoruban/.minikube/profiles/minikube/client.key
I1217 10:18:16.677332     270 certs.go:359] skipping valid signed profile cert regeneration for "minikube": /home/santhasoruban/.minikube/profiles/minikube/apiserver.key.7fb57e3c
I1217 10:18:16.678021     270 certs.go:359] skipping valid signed profile cert regeneration for "aggregator": /home/santhasoruban/.minikube/profiles/minikube/proxy-client.key
I1217 10:18:16.678129     270 certs.go:484] found cert: /home/santhasoruban/.minikube/certs/ca-key.pem (1679 bytes)
I1217 10:18:16.678154     270 certs.go:484] found cert: /home/santhasoruban/.minikube/certs/ca.pem (1099 bytes)
I1217 10:18:16.678170     270 certs.go:484] found cert: /home/santhasoruban/.minikube/certs/cert.pem (1139 bytes)
I1217 10:18:16.678184     270 certs.go:484] found cert: /home/santhasoruban/.minikube/certs/key.pem (1675 bytes)
I1217 10:18:16.679399     270 ssh_runner.go:362] scp /home/santhasoruban/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I1217 10:18:16.711435     270 ssh_runner.go:362] scp /home/santhasoruban/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I1217 10:18:16.743357     270 ssh_runner.go:362] scp /home/santhasoruban/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I1217 10:18:16.773241     270 ssh_runner.go:362] scp /home/santhasoruban/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I1217 10:18:16.799598     270 ssh_runner.go:362] scp /home/santhasoruban/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I1217 10:18:16.826128     270 ssh_runner.go:362] scp /home/santhasoruban/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I1217 10:18:16.853860     270 ssh_runner.go:362] scp /home/santhasoruban/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I1217 10:18:16.879159     270 ssh_runner.go:362] scp /home/santhasoruban/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I1217 10:18:16.900375     270 ssh_runner.go:362] scp /home/santhasoruban/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I1217 10:18:16.920717     270 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I1217 10:18:16.935892     270 ssh_runner.go:195] Run: openssl version
I1217 10:18:16.944092     270 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I1217 10:18:16.953567     270 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I1217 10:18:16.956454     270 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Nov 26 07:19 /usr/share/ca-certificates/minikubeCA.pem
I1217 10:18:16.956485     270 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I1217 10:18:16.961615     270 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I1217 10:18:16.969491     270 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I1217 10:18:16.972264     270 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I1217 10:18:16.977783     270 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I1217 10:18:16.983713     270 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I1217 10:18:16.988982     270 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I1217 10:18:16.995036     270 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I1217 10:18:17.000959     270 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I1217 10:18:17.006977     270 kubeadm.go:392] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:docker.io/kicbase/stable:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 Memory:3900 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true ingress:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/santhasoruban:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I1217 10:18:17.007127     270 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I1217 10:18:17.168232     270 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I1217 10:18:17.267686     270 kubeadm.go:408] found existing configuration files, will attempt cluster restart
I1217 10:18:17.267703     270 kubeadm.go:593] restartPrimaryControlPlane start ...
I1217 10:18:17.267766     270 ssh_runner.go:195] Run: sudo test -d /data/minikube
I1217 10:18:17.287278     270 kubeadm.go:130] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I1217 10:18:17.287384     270 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1217 10:18:17.324884     270 kubeconfig.go:125] found "minikube" server: "https://127.0.0.1:32769"
I1217 10:18:17.354363     270 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I1217 10:18:17.378367     270 kubeadm.go:630] The running cluster does not require reconfiguration: 127.0.0.1
I1217 10:18:17.378396     270 kubeadm.go:597] duration metric: took 110.687696ms to restartPrimaryControlPlane
I1217 10:18:17.378405     270 kubeadm.go:394] duration metric: took 371.443891ms to StartCluster
I1217 10:18:17.378425     270 settings.go:142] acquiring lock: {Name:mk7796ea32867b203ba922d5b5b6d95675c39083 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1217 10:18:17.379241     270 settings.go:150] Updating kubeconfig:  /home/santhasoruban/.kube/config
I1217 10:18:17.379780     270 lock.go:35] WriteFile acquiring /home/santhasoruban/.kube/config: {Name:mk7b2cebf629592b910bfb8d976ef3a061a68b0c Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1217 10:18:17.379981     270 start.go:235] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I1217 10:18:17.380306     270 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.31.0
I1217 10:18:17.380283     270 addons.go:507] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:true ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false]
I1217 10:18:17.380341     270 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I1217 10:18:17.380355     270 addons.go:69] Setting default-storageclass=true in profile "minikube"
I1217 10:18:17.380362     270 addons.go:234] Setting addon storage-provisioner=true in "minikube"
W1217 10:18:17.380368     270 addons.go:243] addon storage-provisioner should already be in state true
I1217 10:18:17.380379     270 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I1217 10:18:17.380389     270 host.go:66] Checking if "minikube" exists ...
I1217 10:18:17.380389     270 addons.go:69] Setting ingress=true in profile "minikube"
I1217 10:18:17.380409     270 addons.go:234] Setting addon ingress=true in "minikube"
W1217 10:18:17.380415     270 addons.go:243] addon ingress should already be in state true
I1217 10:18:17.380442     270 host.go:66] Checking if "minikube" exists ...
I1217 10:18:17.380635     270 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1217 10:18:17.380656     270 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1217 10:18:17.380804     270 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1217 10:18:17.385893     270 out.go:177] 🔎  Verifying Kubernetes components...
I1217 10:18:17.390056     270 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1217 10:18:17.406818     270 addons.go:234] Setting addon default-storageclass=true in "minikube"
W1217 10:18:17.406835     270 addons.go:243] addon default-storageclass should already be in state true
I1217 10:18:17.406876     270 host.go:66] Checking if "minikube" exists ...
I1217 10:18:17.407208     270 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1217 10:18:17.411067     270 out.go:177]     ▪ Using image registry.k8s.io/ingress-nginx/controller:v1.11.2
I1217 10:18:17.413435     270 out.go:177]     ▪ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I1217 10:18:17.420398     270 out.go:177]     ▪ Using image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.3
I1217 10:18:17.420512     270 addons.go:431] installing /etc/kubernetes/addons/storage-provisioner.yaml
I1217 10:18:17.420529     270 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I1217 10:18:17.420601     270 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1217 10:18:17.426920     270 out.go:177]     ▪ Using image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.3
I1217 10:18:17.427759     270 addons.go:431] installing /etc/kubernetes/addons/storageclass.yaml
I1217 10:18:17.427773     270 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I1217 10:18:17.427833     270 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1217 10:18:17.431225     270 addons.go:431] installing /etc/kubernetes/addons/ingress-deploy.yaml
I1217 10:18:17.431233     270 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/ingress-deploy.yaml (16078 bytes)
I1217 10:18:17.431271     270 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1217 10:18:17.441654     270 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/santhasoruban/.minikube/machines/minikube/id_rsa Username:docker}
I1217 10:18:17.448078     270 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/santhasoruban/.minikube/machines/minikube/id_rsa Username:docker}
I1217 10:18:17.449281     270 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/santhasoruban/.minikube/machines/minikube/id_rsa Username:docker}
I1217 10:18:17.768784     270 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/ingress-deploy.yaml
I1217 10:18:17.785513     270 ssh_runner.go:195] Run: sudo systemctl start kubelet
I1217 10:18:17.876328     270 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I1217 10:18:17.877436     270 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I1217 10:18:18.969524     270 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/ingress-deploy.yaml: (1.200711928s)
W1217 10:18:18.969550     270 addons.go:457] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/ingress-deploy.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/ingress-deploy.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1217 10:18:18.969582     270 retry.go:31] will retry after 261.165449ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/ingress-deploy.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/ingress-deploy.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1217 10:18:18.969625     270 ssh_runner.go:235] Completed: sudo systemctl start kubelet: (1.184097658s)
I1217 10:18:18.969681     270 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1217 10:18:18.972501     270 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: (1.095037673s)
W1217 10:18:18.972534     270 addons.go:457] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1217 10:18:18.972554     270 retry.go:31] will retry after 170.496016ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1217 10:18:18.972967     270 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: (1.096605417s)
W1217 10:18:18.972996     270 addons.go:457] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1217 10:18:18.973009     270 retry.go:31] will retry after 149.405017ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1217 10:18:18.994485     270 api_server.go:52] waiting for apiserver process to appear ...
I1217 10:18:18.994548     270 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1217 10:18:19.123666     270 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I1217 10:18:19.144404     270 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I1217 10:18:19.231027     270 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/ingress-deploy.yaml
I1217 10:18:19.495361     270 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1217 10:18:19.573593     270 addons.go:457] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1217 10:18:19.573619     270 retry.go:31] will retry after 292.131116ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
W1217 10:18:19.578799     270 addons.go:457] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1217 10:18:19.578832     270 retry.go:31] will retry after 469.838305ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
W1217 10:18:19.675288     270 addons.go:457] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/ingress-deploy.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/ingress-deploy.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1217 10:18:19.675312     270 retry.go:31] will retry after 344.200166ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/ingress-deploy.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/ingress-deploy.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1217 10:18:19.866930     270 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I1217 10:18:19.995334     270 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1217 10:18:20.020591     270 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/ingress-deploy.yaml
I1217 10:18:20.050026     270 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
W1217 10:18:20.171743     270 addons.go:457] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1217 10:18:20.171766     270 retry.go:31] will retry after 526.814383ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
W1217 10:18:20.467999     270 addons.go:457] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/ingress-deploy.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/ingress-deploy.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1217 10:18:20.468024     270 retry.go:31] will retry after 760.386121ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/ingress-deploy.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/ingress-deploy.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
W1217 10:18:20.468188     270 addons.go:457] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1217 10:18:20.468205     270 retry.go:31] will retry after 392.469664ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1217 10:18:20.495375     270 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1217 10:18:20.698893     270 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I1217 10:18:20.862040     270 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I1217 10:18:20.995025     270 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1217 10:18:21.168578     270 addons.go:457] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1217 10:18:21.168606     270 retry.go:31] will retry after 606.076339ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1217 10:18:21.228714     270 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/ingress-deploy.yaml
W1217 10:18:21.276853     270 addons.go:457] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1217 10:18:21.276875     270 retry.go:31] will retry after 969.818056ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1217 10:18:21.276915     270 api_server.go:72] duration metric: took 3.896914023s to wait for apiserver process to appear ...
I1217 10:18:21.276921     270 api_server.go:88] waiting for apiserver healthz status ...
I1217 10:18:21.276935     270 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I1217 10:18:21.277711     270 api_server.go:269] stopped: https://127.0.0.1:32769/healthz: Get "https://127.0.0.1:32769/healthz": read tcp 127.0.0.1:57856->127.0.0.1:32769: read: connection reset by peer
W1217 10:18:21.490132     270 addons.go:457] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/ingress-deploy.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/ingress-deploy.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1217 10:18:21.490154     270 retry.go:31] will retry after 1.103965206s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/ingress-deploy.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/ingress-deploy.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1217 10:18:21.775974     270 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I1217 10:18:21.778014     270 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I1217 10:18:21.779261     270 api_server.go:269] stopped: https://127.0.0.1:32769/healthz: Get "https://127.0.0.1:32769/healthz": EOF
I1217 10:18:22.247327     270 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
W1217 10:18:22.274650     270 addons.go:457] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1217 10:18:22.274672     270 retry.go:31] will retry after 764.611311ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1217 10:18:22.277829     270 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I1217 10:18:22.280925     270 api_server.go:269] stopped: https://127.0.0.1:32769/healthz: Get "https://127.0.0.1:32769/healthz": EOF
I1217 10:18:22.595216     270 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/ingress-deploy.yaml
W1217 10:18:22.664321     270 addons.go:457] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1217 10:18:22.664346     270 retry.go:31] will retry after 966.642657ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1217 10:18:22.777518     270 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I1217 10:18:22.780776     270 api_server.go:269] stopped: https://127.0.0.1:32769/healthz: Get "https://127.0.0.1:32769/healthz": EOF
W1217 10:18:22.972925     270 addons.go:457] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/ingress-deploy.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/ingress-deploy.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1217 10:18:22.972957     270 retry.go:31] will retry after 900.202795ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/ingress-deploy.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/ingress-deploy.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1217 10:18:23.040844     270 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I1217 10:18:23.277664     270 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I1217 10:18:23.631275     270 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I1217 10:18:23.873904     270 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/ingress-deploy.yaml
I1217 10:18:28.278335     270 api_server.go:269] stopped: https://127.0.0.1:32769/healthz: Get "https://127.0.0.1:32769/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1217 10:18:28.278397     270 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I1217 10:18:30.370878     270 api_server.go:279] https://127.0.0.1:32769/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W1217 10:18:30.370908     270 api_server.go:103] status: https://127.0.0.1:32769/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I1217 10:18:30.370922     270 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I1217 10:18:30.378136     270 api_server.go:279] https://127.0.0.1:32769/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[-]poststarthook/start-apiextensions-controllers failed: reason withheld
[-]poststarthook/crd-informer-synced failed: reason withheld
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[-]poststarthook/start-service-ip-repair-controllers failed: reason withheld
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[-]poststarthook/priority-and-fairness-config-producer failed: reason withheld
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1217 10:18:30.378179     270 api_server.go:103] status: https://127.0.0.1:32769/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[-]poststarthook/start-apiextensions-controllers failed: reason withheld
[-]poststarthook/crd-informer-synced failed: reason withheld
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[-]poststarthook/start-service-ip-repair-controllers failed: reason withheld
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[-]poststarthook/priority-and-fairness-config-producer failed: reason withheld
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1217 10:18:30.774121     270 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: (7.733244984s)
I1217 10:18:30.777205     270 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I1217 10:18:30.784272     270 api_server.go:279] https://127.0.0.1:32769/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1217 10:18:30.784289     270 api_server.go:103] status: https://127.0.0.1:32769/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1217 10:18:31.277483     270 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I1217 10:18:31.373271     270 api_server.go:279] https://127.0.0.1:32769/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1217 10:18:31.373301     270 api_server.go:103] status: https://127.0.0.1:32769/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1217 10:18:31.777877     270 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I1217 10:18:31.871339     270 api_server.go:279] https://127.0.0.1:32769/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1217 10:18:31.871366     270 api_server.go:103] status: https://127.0.0.1:32769/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1217 10:18:32.278303     270 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I1217 10:18:32.370451     270 api_server.go:279] https://127.0.0.1:32769/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1217 10:18:32.370477     270 api_server.go:103] status: https://127.0.0.1:32769/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1217 10:18:32.778101     270 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I1217 10:18:32.869274     270 api_server.go:279] https://127.0.0.1:32769/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1217 10:18:32.869316     270 api_server.go:103] status: https://127.0.0.1:32769/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1217 10:18:33.277077     270 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I1217 10:18:33.282217     270 api_server.go:279] https://127.0.0.1:32769/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1217 10:18:33.282245     270 api_server.go:103] status: https://127.0.0.1:32769/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1217 10:18:33.777156     270 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I1217 10:18:33.968264     270 api_server.go:279] https://127.0.0.1:32769/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1217 10:18:33.968282     270 api_server.go:103] status: https://127.0.0.1:32769/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1217 10:18:34.278026     270 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I1217 10:18:34.367080     270 api_server.go:279] https://127.0.0.1:32769/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1217 10:18:34.367105     270 api_server.go:103] status: https://127.0.0.1:32769/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1217 10:18:34.568877     270 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: (10.937573185s)
I1217 10:18:34.777211     270 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I1217 10:18:34.783411     270 api_server.go:279] https://127.0.0.1:32769/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1217 10:18:34.783429     270 api_server.go:103] status: https://127.0.0.1:32769/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1217 10:18:35.278269     270 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I1217 10:18:35.370538     270 api_server.go:279] https://127.0.0.1:32769/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1217 10:18:35.370559     270 api_server.go:103] status: https://127.0.0.1:32769/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1217 10:18:35.777162     270 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I1217 10:18:35.870807     270 api_server.go:279] https://127.0.0.1:32769/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1217 10:18:35.870834     270 api_server.go:103] status: https://127.0.0.1:32769/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1217 10:18:36.277916     270 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I1217 10:18:36.368940     270 api_server.go:279] https://127.0.0.1:32769/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1217 10:18:36.368963     270 api_server.go:103] status: https://127.0.0.1:32769/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1217 10:18:36.777066     270 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I1217 10:18:36.866800     270 api_server.go:279] https://127.0.0.1:32769/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1217 10:18:36.866820     270 api_server.go:103] status: https://127.0.0.1:32769/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1217 10:18:37.277326     270 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I1217 10:18:37.368245     270 api_server.go:279] https://127.0.0.1:32769/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1217 10:18:37.368272     270 api_server.go:103] status: https://127.0.0.1:32769/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1217 10:18:37.779128     270 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I1217 10:18:37.873605     270 api_server.go:279] https://127.0.0.1:32769/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1217 10:18:37.873634     270 api_server.go:103] status: https://127.0.0.1:32769/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1217 10:18:38.277693     270 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I1217 10:18:38.283398     270 api_server.go:279] https://127.0.0.1:32769/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1217 10:18:38.283459     270 api_server.go:103] status: https://127.0.0.1:32769/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1217 10:18:38.777908     270 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I1217 10:18:38.871500     270 api_server.go:279] https://127.0.0.1:32769/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1217 10:18:38.871526     270 api_server.go:103] status: https://127.0.0.1:32769/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1217 10:18:39.277550     270 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I1217 10:18:39.372511     270 api_server.go:279] https://127.0.0.1:32769/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1217 10:18:39.372542     270 api_server.go:103] status: https://127.0.0.1:32769/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1217 10:18:39.777302     270 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I1217 10:18:39.975453     270 api_server.go:279] https://127.0.0.1:32769/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1217 10:18:39.975478     270 api_server.go:103] status: https://127.0.0.1:32769/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1217 10:18:40.277987     270 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I1217 10:18:40.371948     270 api_server.go:279] https://127.0.0.1:32769/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1217 10:18:40.371975     270 api_server.go:103] status: https://127.0.0.1:32769/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1217 10:18:40.777571     270 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I1217 10:18:40.871456     270 api_server.go:279] https://127.0.0.1:32769/healthz returned 200:
ok
I1217 10:18:40.873364     270 api_server.go:141] control plane version: v1.31.0
I1217 10:18:40.873401     270 api_server.go:131] duration metric: took 19.59647179s to wait for apiserver health ...
I1217 10:18:40.873414     270 system_pods.go:43] waiting for kube-system pods to appear ...
I1217 10:18:41.078476     270 system_pods.go:59] 7 kube-system pods found
I1217 10:18:41.078523     270 system_pods.go:61] "coredns-6f6b679f8f-7sjhg" [bd1f070e-fe3f-4e53-a0f3-7c82fd7fc4d4] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I1217 10:18:41.078538     270 system_pods.go:61] "etcd-minikube" [5109201f-17f7-458b-908f-15c85074aa27] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I1217 10:18:41.078547     270 system_pods.go:61] "kube-apiserver-minikube" [744b35b5-3157-4dc3-92b3-171c486d44fb] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I1217 10:18:41.078555     270 system_pods.go:61] "kube-controller-manager-minikube" [28f401eb-e785-47b0-9f56-8a54b9ce3126] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I1217 10:18:41.078560     270 system_pods.go:61] "kube-proxy-gdfs7" [e4675888-a664-4f0f-9b8a-f2360aba5e4e] Running / Ready:ContainersNotReady (containers with unready status: [kube-proxy]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-proxy])
I1217 10:18:41.078564     270 system_pods.go:61] "kube-scheduler-minikube" [53e975e6-baa3-406e-b394-959bc687edd9] Running
I1217 10:18:41.078569     270 system_pods.go:61] "storage-provisioner" [f39d2542-0b37-4407-b74c-7be5b1ae4548] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I1217 10:18:41.078580     270 system_pods.go:74] duration metric: took 205.158165ms to wait for pod list to return data ...
I1217 10:18:41.078594     270 kubeadm.go:582] duration metric: took 23.698591455s to wait for: map[apiserver:true system_pods:true]
I1217 10:18:41.078614     270 node_conditions.go:102] verifying NodePressure condition ...
I1217 10:18:41.280081     270 node_conditions.go:122] node storage ephemeral capacity is 263174212Ki
I1217 10:18:41.280106     270 node_conditions.go:123] node cpu capacity is 18
I1217 10:18:41.280133     270 node_conditions.go:105] duration metric: took 201.513359ms to run NodePressure ...
I1217 10:18:41.280148     270 start.go:241] waiting for startup goroutines ...
I1217 10:18:43.375691     270 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/ingress-deploy.yaml: (19.501732213s)
I1217 10:18:43.375827     270 addons.go:475] Verifying addon ingress=true in "minikube"
I1217 10:18:43.385083     270 out.go:177] 🔎  Verifying ingress addon...
I1217 10:18:43.391209     270 kapi.go:75] Waiting for pod with label "app.kubernetes.io/name=ingress-nginx" in ns "ingress-nginx" ...
I1217 10:18:43.470970     270 kapi.go:86] Found 3 Pods for label selector app.kubernetes.io/name=ingress-nginx
I1217 10:18:43.470997     270 kapi.go:107] duration metric: took 79.795318ms to wait for app.kubernetes.io/name=ingress-nginx ...
I1217 10:18:43.481481     270 out.go:177] 🌟  Enabled addons: default-storageclass, storage-provisioner, ingress
I1217 10:18:43.574962     270 addons.go:510] duration metric: took 26.194610547s for enable addons: enabled=[default-storageclass storage-provisioner ingress]
I1217 10:18:43.575020     270 start.go:246] waiting for cluster config update ...
I1217 10:18:43.575039     270 start.go:255] writing updated cluster config ...
I1217 10:18:43.576751     270 ssh_runner.go:195] Run: rm -f paused
I1217 10:18:44.418471     270 start.go:600] kubectl: 1.31.0, cluster: 1.31.0 (minor skew: 0)
I1217 10:18:44.424952     270 out.go:177] 🏄  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
Dec 17 04:48:27 minikube cri-dockerd[1687]: time="2024-12-17T04:48:27Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"aqua-69c54df8b9-9btjb_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"45f2ef58c18b7290d80c5fb196de881ce0dd6ecb2b4e8151125c0d545f9d7849\""
Dec 17 04:48:28 minikube cri-dockerd[1687]: time="2024-12-17T04:48:28Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"my-nginx-7bc4c7bc9c-69b5w_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"bdab30e3162cf3bc7058a244d25b3c78b74ab6aecd590f0a479602056894245a\""
Dec 17 04:48:28 minikube cri-dockerd[1687]: time="2024-12-17T04:48:28Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"pod-info-app-65f569fd44-wbm4r_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"3397534ed3fb7c21c62bc727881a6e827a2a5eef5642b0ced01e982d8bfc7fff\""
Dec 17 04:48:28 minikube cri-dockerd[1687]: time="2024-12-17T04:48:28Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"olive-7d4998978f-rcrh4_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"cc00d08be9b113057abbda443b9ded69e17e1fafd7d27dfb9ae7332f77493475\""
Dec 17 04:48:28 minikube cri-dockerd[1687]: time="2024-12-17T04:48:28Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"lime-color-6bdd87b769-wn6nr_trouble-1\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"809d1270765002f46c5a536c8bd26c2ef9c2c2dab341233a16e285f5b8fe3157\""
Dec 17 04:48:28 minikube cri-dockerd[1687]: time="2024-12-17T04:48:28Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"ingress-nginx-controller-bc57996ff-vwprn_ingress-nginx\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"28899da4d3b4dfb64d87efba7dee9f92ab36611cdd515d5372fc1fbb3c5b6dbd\""
Dec 17 04:48:28 minikube cri-dockerd[1687]: time="2024-12-17T04:48:28Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"olive-7d4998978f-nmnqr_ingress-nginx\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"3fbb9c406ac0fdc8479d4b52e5ac36031e94cf8bc05726add0a367e3fdfbf78b\""
Dec 17 04:48:28 minikube cri-dockerd[1687]: time="2024-12-17T04:48:28Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"pod-info-app-65f569fd44-kz4h5_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"39adba191652ca245ce85fdbf590457a401294de7f2f5028544e0b99a8807fa2\""
Dec 17 04:48:28 minikube cri-dockerd[1687]: time="2024-12-17T04:48:28Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"olive-7d4998978f-cb2f7_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"61b45b3e4a224158620aabc5302126461be0f0a965892a333370d860a78012b2\""
Dec 17 04:48:29 minikube cri-dockerd[1687]: time="2024-12-17T04:48:29Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"pod-info-app-65f569fd44-824nt_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"5c276a28838baa2a9dffd6c5ab31b8dedbf602bddd0d1749f9c5707624eff36a\""
Dec 17 04:48:29 minikube cri-dockerd[1687]: time="2024-12-17T04:48:29Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"fuchsia-color-7f46c97957-59r62_trouble-3\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"956e257c5f5d0a86d4560b0355e9068ef5a8e2e119a2709f3e3105fa1d04cc67\""
Dec 17 04:48:29 minikube cri-dockerd[1687]: time="2024-12-17T04:48:29Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"purple-color-6994fb8f65-5cp9b_trouble-1\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"96a3ce6ec641ab8b27887a1ec5797594e055b173041e100339c566efc2d89f94\""
Dec 17 04:48:29 minikube cri-dockerd[1687]: time="2024-12-17T04:48:29Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"pod-info-app-65f569fd44-6xklz_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"89a229bf2d09bcc8ce0b02b112ce85f9a13107d127366355383496395b1af1df\""
Dec 17 04:48:29 minikube cri-dockerd[1687]: time="2024-12-17T04:48:29Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"hello-hello-648574587d-54s9h_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"3e9b86ac1fa6d56dc66ae49c4cbddb55649c4a6fe05ddd8ed9af0f63a6242120\""
Dec 17 04:48:29 minikube cri-dockerd[1687]: time="2024-12-17T04:48:29Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-6f6b679f8f-7sjhg_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"88eea4fa633e4f5b09f07d9f5a4d68be1fee1a059214ef3a995d60554d68f269\""
Dec 17 04:48:29 minikube cri-dockerd[1687]: time="2024-12-17T04:48:29Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"maroon-6f56fdcfd9-q7blx_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"0299e9338640463ed3acf2556436949f708c33adb27fa5f85871a59008b6466e\""
Dec 17 04:48:30 minikube cri-dockerd[1687]: time="2024-12-17T04:48:30Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"aqua-69c54df8b9-9tcgb_ingress-nginx\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"89b7123565e297a57d5072e479c5992a63a57703bac0c1bf553e11c0e38181b9\""
Dec 17 04:48:30 minikube cri-dockerd[1687]: time="2024-12-17T04:48:30Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"fuchsia-color-7f46c97957-cfxfl_trouble-3\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"ed52e187c599c591865a040ed4b3849de4b865185ca5ba4e74563252d427b2a9\""
Dec 17 04:48:30 minikube cri-dockerd[1687]: time="2024-12-17T04:48:30Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"olive-7d4998978f-d9lkn_ingress-nginx\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"109c36413350953e03618c6193a4240d1e554029add84a09bf1009770fbec13d\""
Dec 17 04:48:30 minikube cri-dockerd[1687]: time="2024-12-17T04:48:30Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}"
Dec 17 04:48:34 minikube cri-dockerd[1687]: time="2024-12-17T04:48:34Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/299e476689e9dabacc4ce984166dba6b4f5b63881b4d62faf69d5030f77e2e38/resolv.conf as [nameserver 192.168.1.20 options ndots:0]"
Dec 17 04:48:35 minikube cri-dockerd[1687]: time="2024-12-17T04:48:35Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/10280409a581cf57e788d1f343e1534b438c4f26e3ff11eebad6cb40bf13f6bf/resolv.conf as [nameserver 192.168.1.20 options ndots:0]"
Dec 17 04:48:35 minikube cri-dockerd[1687]: time="2024-12-17T04:48:35Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/67a54e2a58c677b83e766e46f9c41567b069632efa3a6bee802819aedb84a0d0/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Dec 17 04:48:36 minikube cri-dockerd[1687]: time="2024-12-17T04:48:36Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/499255d27edb61d6f3b1cc6bae49b03b64c85aa1386f795c9ff7c134e4ca6431/resolv.conf as [nameserver 10.96.0.10 search trouble-1.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Dec 17 04:48:36 minikube cri-dockerd[1687]: time="2024-12-17T04:48:36Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/afa9539df364fdb76c8526930205a6b2d1bef7c84fb3063c0a05e5752a91379b/resolv.conf as [nameserver 10.96.0.10 search trouble-3.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Dec 17 04:48:36 minikube cri-dockerd[1687]: time="2024-12-17T04:48:36Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/22914414498f6db87c24ac6209d69430e390e1258e6b778f7b8697dad66aea6b/resolv.conf as [nameserver 10.96.0.10 search trouble-1.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Dec 17 04:48:37 minikube cri-dockerd[1687]: time="2024-12-17T04:48:37Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/291526bb8c1b8447a931808606453a6a744c691714a344c0e66e5e72071844a4/resolv.conf as [nameserver 10.96.0.10 search ingress-nginx.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Dec 17 04:48:38 minikube cri-dockerd[1687]: time="2024-12-17T04:48:38Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/892cf2a28376274b1c59b68145352d5c86ebfa267457aa7927af850247dfb436/resolv.conf as [nameserver 10.96.0.10 search trouble-1.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Dec 17 04:48:38 minikube cri-dockerd[1687]: time="2024-12-17T04:48:38Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/551270d84ce97b84595a95b08926d3a099af762ee452fda3a1e0f8866f830053/resolv.conf as [nameserver 10.96.0.10 search ingress-nginx.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Dec 17 04:48:38 minikube cri-dockerd[1687]: time="2024-12-17T04:48:38Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/cf9cb049a7174c593f50d730e90576ee0d8863b3f0a0f81e25fecba9abc62463/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Dec 17 04:48:38 minikube cri-dockerd[1687]: time="2024-12-17T04:48:38Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/2c75eaa10af5cd0bad2eb48cd3677207be000055190da305fb8dd4506a75d8c2/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Dec 17 04:48:39 minikube cri-dockerd[1687]: time="2024-12-17T04:48:39Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/cab7d57d82008f75fd72bbd641f103e7f5cae617d72a5de4cbebed0a65e47102/resolv.conf as [nameserver 10.96.0.10 search trouble-2.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Dec 17 04:48:39 minikube cri-dockerd[1687]: time="2024-12-17T04:48:39Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/96903513068eae48f07cae55dfc8ebfa0e35a0b78ab95fb3bd3b9bbd33db33b4/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Dec 17 04:48:40 minikube cri-dockerd[1687]: time="2024-12-17T04:48:40Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/94dad2e2d50f34abf5fecb92de3ac59b2209b665a6eefb95e45b3c80c0483d51/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Dec 17 04:48:40 minikube cri-dockerd[1687]: time="2024-12-17T04:48:40Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/b933fdfafd389ac963868382b3ffe61622c583b77c4a142eed214480c0f4cf6a/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Dec 17 04:48:40 minikube cri-dockerd[1687]: time="2024-12-17T04:48:40Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/e2fc15979ceb90d3fe199f38f9e5c0d3c559e8e19a298391ca07a9e9a8abfd67/resolv.conf as [nameserver 10.96.0.10 search ingress-nginx.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Dec 17 04:48:40 minikube dockerd[1282]: time="2024-12-17T04:48:40.669483553Z" level=info msg="ignoring event" container=52700b42ce1cdb8a9652c8ded65d48add90c97664178888cb47e491d57ecba66 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 17 04:48:41 minikube cri-dockerd[1687]: time="2024-12-17T04:48:41Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/e0eab8b8e984df16e9f2fb67b0629a0ab1be27b4b37b95090e3386c9fcf84641/resolv.conf as [nameserver 10.96.0.10 search ingress-nginx.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Dec 17 04:48:41 minikube cri-dockerd[1687]: time="2024-12-17T04:48:41Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/53438b5b77dfe55ecb0e9da98907907ea6a74f8e5a4958ccec4d865e00b13865/resolv.conf as [nameserver 10.96.0.10 search trouble-2.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Dec 17 04:48:42 minikube cri-dockerd[1687]: time="2024-12-17T04:48:42Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/0f3c93a9870bbec7a711aee5bd977afd557b1837993aae5ff44892371a7f4ce7/resolv.conf as [nameserver 10.96.0.10 search trouble-3.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Dec 17 04:48:42 minikube dockerd[1282]: time="2024-12-17T04:48:42.772837059Z" level=info msg="ignoring event" container=88ef00d17293d0bee6016f451de54d7df437a12a7e706927f65d1464008a329e module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 17 04:48:42 minikube cri-dockerd[1687]: time="2024-12-17T04:48:42Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/ed58f30bf7cdbcfe07cea44824d001da7434ae67da95f4ad92e5fb5cb2a2fca7/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Dec 17 04:48:43 minikube cri-dockerd[1687]: time="2024-12-17T04:48:43Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/f0d3aec273d53d2abd1417d35cdf09f9b03be1c0116e96ebc1720f3be97c2fb5/resolv.conf as [nameserver 10.96.0.10 search trouble-2.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Dec 17 04:48:44 minikube cri-dockerd[1687]: time="2024-12-17T04:48:44Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/6a498b56af51d94769a3b77cf960ce1692c94d20f827dd35706d2966d627b222/resolv.conf as [nameserver 192.168.1.20 options ndots:0]"
Dec 17 04:48:44 minikube cri-dockerd[1687]: time="2024-12-17T04:48:44Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/449f2dfd7aeb3d6225619d179d1d1f9b44466dd32c85843ea4d7deeed65f0e13/resolv.conf as [nameserver 10.96.0.10 search trouble-1.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Dec 17 04:48:44 minikube cri-dockerd[1687]: time="2024-12-17T04:48:44Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/c45ea422989c8f169378c5dd2c7f80c13620ebce2a5e15e048cd18d6a65753ad/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Dec 17 04:48:45 minikube cri-dockerd[1687]: time="2024-12-17T04:48:45Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/e8aa0ab81ec4aa52ba57b1204060a5e3652ced4c24551085aadaaf90e9c91e24/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Dec 17 04:48:45 minikube cri-dockerd[1687]: time="2024-12-17T04:48:45Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/9c12581679726fb84064e295d84a49a4cfebaaac4a7fb725271ba7244eca180d/resolv.conf as [nameserver 10.96.0.10 search ingress-nginx.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Dec 17 04:48:46 minikube cri-dockerd[1687]: time="2024-12-17T04:48:46Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/466a33a89be81de3546c771b98415ccb671440498d188d44f770d950c6580043/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Dec 17 04:48:47 minikube cri-dockerd[1687]: time="2024-12-17T04:48:47Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/90c1b60ad209bb931bf0ab7bd493a62c8e449c55f8fdccb2883cc84b0f96399d/resolv.conf as [nameserver 10.96.0.10 search trouble-1.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Dec 17 04:48:48 minikube cri-dockerd[1687]: time="2024-12-17T04:48:48Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/bb2b3a1f8a789fcb9125f0cbef04261803e9c2439fb4e23e8e89f60989b5dc08/resolv.conf as [nameserver 10.96.0.10 search ingress-nginx.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Dec 17 04:48:48 minikube cri-dockerd[1687]: time="2024-12-17T04:48:48Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/60cf7c2c88ff8e4167d11fe89faf171e2496b4404a0d5d013f987fc5849112ef/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Dec 17 04:48:48 minikube cri-dockerd[1687]: time="2024-12-17T04:48:48Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/303edb7474d921f1b21b451cfe67e956e0544502cd601afb1d162d2940d2980c/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Dec 17 04:48:48 minikube cri-dockerd[1687]: time="2024-12-17T04:48:48Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/136ebb854d9354b276b98f5f3c6f9b446e128854c0f7bf83764283fbbd16d972/resolv.conf as [nameserver 10.96.0.10 search ingress-nginx.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Dec 17 04:48:48 minikube cri-dockerd[1687]: time="2024-12-17T04:48:48Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/1871d604438a7bd75e643a87cf24372942ceaf5786bfdc97f8931423645fbe06/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Dec 17 04:48:48 minikube cri-dockerd[1687]: time="2024-12-17T04:48:48Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/23bffbb064521f59dd67ddaa82a064b0f9f0bf1af7e6ae4c6650406b45c3d023/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Dec 17 04:48:49 minikube cri-dockerd[1687]: time="2024-12-17T04:48:49Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/84bdb78053446a0ec7811cfa78c478a563fd84ce77961557e18a1259c7891d26/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Dec 17 04:48:49 minikube cri-dockerd[1687]: time="2024-12-17T04:48:49Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/4e408eb62d7f57b553fe2084078577c82a9857451a480384da1ca5d9deeb1a17/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Dec 17 04:48:52 minikube dockerd[1282]: time="2024-12-17T04:48:52.675366453Z" level=info msg="ignoring event" container=36061d7729a06607fd6d9832af0e3990f53772cd20668b21fe14c7d4f9fb74c9 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 17 04:48:52 minikube cri-dockerd[1687]: time="2024-12-17T04:48:52Z" level=info msg="Stop pulling image strm/helloworld-http:latest: Status: Image is up to date for strm/helloworld-http:latest"


==> container status <==
CONTAINER           IMAGE                                                                                          CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
95973e19ae440       d42eb6c65d81d                                                                                  37 minutes ago      Running             nginx                     4                   23bffbb064521       my-nginx-7bc4c7bc9c-69b5w
0e94c4563ca5e       6e38f40d628db                                                                                  37 minutes ago      Running             storage-provisioner       24                  10280409a581c       storage-provisioner
5cf207d4db219       045733566833c                                                                                  37 minutes ago      Running             kube-controller-manager   14                  2e8696657ae4b       kube-controller-manager-minikube
5d4cfd31799b7       strm/helloworld-http@sha256:bd44b0ca80c26b5eba984bf498a9c3bab0eb1c59d30d8df3cb2c073937ee4e45   37 minutes ago      Running             hello-hello               12                  c45ea422989c8       hello-hello-648574587d-54s9h
36061d7729a06       d42eb6c65d81d                                                                                  37 minutes ago      Exited              preserve-logs-symlinks    4                   23bffbb064521       my-nginx-7bc4c7bc9c-69b5w
a06b30c0d0f56       a41cca3e15032                                                                                  37 minutes ago      Running             pod-info-app              12                  4e408eb62d7f5       pod-info-app-65f569fd44-gq624
6bafa1bc0ee72       19fded7fd82cb                                                                                  37 minutes ago      Running             color                     10                  1871d604438a7       olive-7d4998978f-cb2f7
4fc3e319759ae       19fded7fd82cb                                                                                  37 minutes ago      Running             color                     9                   136ebb854d935       aqua-69c54df8b9-9tcgb
961922c316dcb       a41cca3e15032                                                                                  37 minutes ago      Running             pod-info-app              5                   303edb7474d92       pod-info-app-65f569fd44-wbm4r
4888bd91c7c86       a41cca3e15032                                                                                  37 minutes ago      Running             pod-info-app              5                   84bdb78053446       pod-info-app-65f569fd44-6xklz
fd8cdb4a8353f       19fded7fd82cb                                                                                  37 minutes ago      Running             color                     9                   bb2b3a1f8a789       aqua-69c54df8b9-vg99d
3f7dfc26fd782       19fded7fd82cb                                                                                  37 minutes ago      Running             color                     10                  60cf7c2c88ff8       aqua-69c54df8b9-dz6z2
2d76f830b4305       19fded7fd82cb                                                                                  37 minutes ago      Running             webapp-color              10                  90c1b60ad209b       lime-color-6bdd87b769-tmczz
a9e2efb2b55d1       8c811b4aec35f                                                                                  37 minutes ago      Running             busybox                   31                  466a33a89be81       busybox-pod
d7fbf3a41be84       19fded7fd82cb                                                                                  37 minutes ago      Running             color                     9                   9c12581679726       olive-7d4998978f-nmnqr
6ade6cf1afd10       a41cca3e15032                                                                                  37 minutes ago      Running             pod-info-app              5                   e8aa0ab81ec4a       pod-info-app-65f569fd44-tssr5
0d2010ddbe641       19fded7fd82cb                                                                                  37 minutes ago      Running             webapp-color              10                  449f2dfd7aeb3       purple-color-6994fb8f65-n9v4p
58045466750ed       cbb01a7bd410d                                                                                  37 minutes ago      Running             coredns                   12                  6a498b56af51d       coredns-6f6b679f8f-7sjhg
4789837239da6       19fded7fd82cb                                                                                  37 minutes ago      Running             webapp-color              10                  f0d3aec273d53       teal-color-555b59785-52kh5
8749a13b1fc05       19fded7fd82cb                                                                                  37 minutes ago      Running             color                     10                  ed58f30bf7cdb       maroon-6f56fdcfd9-gmskx
4397302e5a32e       a80c8fd6e5229                                                                                  37 minutes ago      Running             controller                8                   e0eab8b8e984d       ingress-nginx-controller-bc57996ff-vwprn
5d33932caf73d       19fded7fd82cb                                                                                  37 minutes ago      Running             webapp-color              10                  0f3c93a9870bb       fuchsia-color-7f46c97957-59r62
c3d14d6d3d85e       19fded7fd82cb                                                                                  37 minutes ago      Running             webapp-color              10                  53438b5b77dfe       navy-color-7596c47445-gdqsd
dbcffbd1f3db5       19fded7fd82cb                                                                                  37 minutes ago      Running             color                     10                  b933fdfafd389       aqua-69c54df8b9-9btjb
32cb72d82efe8       a41cca3e15032                                                                                  37 minutes ago      Running             pod-info-app              5                   94dad2e2d50f3       pod-info-app-65f569fd44-824nt
2d037cc3577e3       19fded7fd82cb                                                                                  37 minutes ago      Running             color                     9                   e2fc15979ceb9       maroon-6f56fdcfd9-jhxrq
2fc5aeed919f7       a41cca3e15032                                                                                  37 minutes ago      Running             pod-info-app              12                  2c75eaa10af5c       pod-info-app-65f569fd44-kz4h5
02a59f98d7963       19fded7fd82cb                                                                                  37 minutes ago      Running             webapp-color              10                  cab7d57d82008       navy-color-7596c47445-sl2m8
d682ffc8e45a4       19fded7fd82cb                                                                                  37 minutes ago      Running             color                     10                  96903513068ea       maroon-6f56fdcfd9-q7blx
e340952662ec3       a41cca3e15032                                                                                  37 minutes ago      Running             web-pod-info              12                  cf9cb049a7174       myapp-7d66457db5-sx6nb
8ba307646e068       19fded7fd82cb                                                                                  37 minutes ago      Running             color                     9                   551270d84ce97       maroon-6f56fdcfd9-5zqtv
a8b1dd748b5b8       19fded7fd82cb                                                                                  37 minutes ago      Running             webapp-color              10                  892cf2a283762       lime-color-6bdd87b769-zq96q
0fd2bad9f97ea       19fded7fd82cb                                                                                  37 minutes ago      Running             webapp-color              10                  22914414498f6       purple-color-6994fb8f65-5cp9b
a85520ef16c23       19fded7fd82cb                                                                                  37 minutes ago      Running             color                     9                   291526bb8c1b8       olive-7d4998978f-d9lkn
5ea9d58fee811       19fded7fd82cb                                                                                  37 minutes ago      Running             webapp-color              10                  afa9539df364f       fuchsia-color-7f46c97957-cfxfl
56ff533f54d28       19fded7fd82cb                                                                                  37 minutes ago      Running             webapp-color              10                  499255d27edb6       lime-color-6bdd87b769-wn6nr
0f120038b4d10       19fded7fd82cb                                                                                  37 minutes ago      Running             color                     10                  67a54e2a58c67       olive-7d4998978f-rcrh4
88ef00d17293d       6e38f40d628db                                                                                  37 minutes ago      Exited              storage-provisioner       23                  10280409a581c       storage-provisioner
7c5aad3e6be96       ad83b2ca7b09e                                                                                  37 minutes ago      Running             kube-proxy                12                  299e476689e9d       kube-proxy-gdfs7
7000a86d03c43       1766f54c897f0                                                                                  38 minutes ago      Running             kube-scheduler            12                  7fa846926b827       kube-scheduler-minikube
65c1bfdf4a46c       2e96e5913fc06                                                                                  38 minutes ago      Running             etcd                      12                  8d7091eae3317       etcd-minikube
18f2bb72d6d2d       604f5db92eaa8                                                                                  38 minutes ago      Running             kube-apiserver            12                  745929d66617e       kube-apiserver-minikube
52700b42ce1cd       045733566833c                                                                                  38 minutes ago      Exited              kube-controller-manager   13                  2e8696657ae4b       kube-controller-manager-minikube
186b6c441af1d       d42eb6c65d81d                                                                                  13 days ago         Exited              nginx                     3                   bdab30e3162cf       my-nginx-7bc4c7bc9c-69b5w
132d964455745       a80c8fd6e5229                                                                                  13 days ago         Exited              controller                7                   28899da4d3b4d       ingress-nginx-controller-bc57996ff-vwprn
a93e8eee020c3       19fded7fd82cb                                                                                  13 days ago         Exited              webapp-color              9                   809d127076500       lime-color-6bdd87b769-wn6nr
e9f4f2dc9a22b       a41cca3e15032                                                                                  13 days ago         Exited              web-pod-info              11                  cafd61ae0dea2       myapp-7d66457db5-sx6nb
baa27606b47e7       19fded7fd82cb                                                                                  13 days ago         Exited              color                     8                   b83de2e8773d0       maroon-6f56fdcfd9-5zqtv
16fd55b2c189f       19fded7fd82cb                                                                                  13 days ago         Exited              color                     8                   df96afafbff44       maroon-6f56fdcfd9-jhxrq
f6a1067ab4b83       19fded7fd82cb                                                                                  13 days ago         Exited              webapp-color              9                   74ee9902c74f4       purple-color-6994fb8f65-n9v4p
9c4c55b4fc21f       strm/helloworld-http@sha256:bd44b0ca80c26b5eba984bf498a9c3bab0eb1c59d30d8df3cb2c073937ee4e45   13 days ago         Exited              hello-hello               11                  3e9b86ac1fa6d       hello-hello-648574587d-54s9h
b5058bbee9a67       a41cca3e15032                                                                                  13 days ago         Exited              pod-info-app              4                   3397534ed3fb7       pod-info-app-65f569fd44-wbm4r
677fff473c05a       19fded7fd82cb                                                                                  13 days ago         Exited              webapp-color              9                   9669914aa459e       lime-color-6bdd87b769-zq96q
781c472a142ee       19fded7fd82cb                                                                                  13 days ago         Exited              webapp-color              9                   58616e994d81c       navy-color-7596c47445-gdqsd
4097b07626871       19fded7fd82cb                                                                                  13 days ago         Exited              webapp-color              9                   51e4a59aadb46       lime-color-6bdd87b769-tmczz
6dfc3edf1c63f       19fded7fd82cb                                                                                  13 days ago         Exited              color                     8                   031957a38b6f2       aqua-69c54df8b9-vg99d
08e0cbe209c21       19fded7fd82cb                                                                                  13 days ago         Exited              webapp-color              9                   8b1e1ca0973dc       teal-color-555b59785-52kh5
f778a66ef56c3       a41cca3e15032                                                                                  13 days ago         Exited              pod-info-app              4                   89a229bf2d09b       pod-info-app-65f569fd44-6xklz
e1c9dc1bba3b6       19fded7fd82cb                                                                                  13 days ago         Exited              color                     8                   3fbb9c406ac0f       olive-7d4998978f-nmnqr
3f89f81dd483c       cbb01a7bd410d                                                                                  13 days ago         Exited              coredns                   11                  88eea4fa633e4       coredns-6f6b679f8f-7sjhg
21a3cf90b41e3       19fded7fd82cb                                                                                  13 days ago         Exited              webapp-color              9                   96a3ce6ec641a       purple-color-6994fb8f65-5cp9b
2c2fe6ac82654       8c811b4aec35f                                                                                  13 days ago         Exited              busybox                   30                  802eb53623a0a       busybox-pod
8cac7d6ad708d       19fded7fd82cb                                                                                  13 days ago         Exited              color                     9                   45f2ef58c18b7       aqua-69c54df8b9-9btjb
c4b4c54dc4eaf       19fded7fd82cb                                                                                  13 days ago         Exited              color                     9                   61b45b3e4a224       olive-7d4998978f-cb2f7
aedf96b202165       19fded7fd82cb                                                                                  13 days ago         Exited              webapp-color              9                   ed52e187c599c       fuchsia-color-7f46c97957-cfxfl
36359a1fec066       19fded7fd82cb                                                                                  13 days ago         Exited              webapp-color              9                   635be8709841f       navy-color-7596c47445-sl2m8
01207faa0588c       19fded7fd82cb                                                                                  13 days ago         Exited              color                     8                   109c364133509       olive-7d4998978f-d9lkn
b2c15decb6ecd       a41cca3e15032                                                                                  13 days ago         Exited              pod-info-app              4                   5c276a28838ba       pod-info-app-65f569fd44-824nt
86d1da5ef4396       a41cca3e15032                                                                                  13 days ago         Exited              pod-info-app              4                   aa5e99d2d0cf9       pod-info-app-65f569fd44-tssr5
0b11c18e91064       a41cca3e15032                                                                                  13 days ago         Exited              pod-info-app              11                  39adba191652c       pod-info-app-65f569fd44-kz4h5
2a1fd62097122       ad83b2ca7b09e                                                                                  13 days ago         Exited              kube-proxy                11                  7b7d7c4a4910d       kube-proxy-gdfs7
12ae3eb999489       19fded7fd82cb                                                                                  13 days ago         Exited              color                     8                   89b7123565e29       aqua-69c54df8b9-9tcgb
e3d4fe190dd31       19fded7fd82cb                                                                                  13 days ago         Exited              color                     9                   703c193630ad1       maroon-6f56fdcfd9-gmskx
72f179365b5cd       19fded7fd82cb                                                                                  13 days ago         Exited              color                     9                   0299e93386404       maroon-6f56fdcfd9-q7blx
fe64fce2ad2eb       19fded7fd82cb                                                                                  13 days ago         Exited              webapp-color              9                   956e257c5f5d0       fuchsia-color-7f46c97957-59r62
75fb55c18820d       19fded7fd82cb                                                                                  13 days ago         Exited              color                     9                   614ca82193279       aqua-69c54df8b9-dz6z2
0fd06d233ec34       a41cca3e15032                                                                                  13 days ago         Exited              pod-info-app              11                  c4b7a1a90b30a       pod-info-app-65f569fd44-gq624
081e0a75205fd       19fded7fd82cb                                                                                  13 days ago         Exited              color                     9                   cc00d08be9b11       olive-7d4998978f-rcrh4
103a3061ff3d4       2e96e5913fc06                                                                                  13 days ago         Exited              etcd                      11                  06b8a02e61ab8       etcd-minikube
be996d7b38ca3       1766f54c897f0                                                                                  13 days ago         Exited              kube-scheduler            11                  fd5f01b717665       kube-scheduler-minikube
731a56c40a096       604f5db92eaa8                                                                                  13 days ago         Exited              kube-apiserver            11                  99ad2b7d35c98       kube-apiserver-minikube
5e74019a2a65a       ce263a8653f9c                                                                                  2 weeks ago         Exited              patch                     1                   81a3ef57e5e0d       ingress-nginx-admission-patch-pp7w6
04b2e96fc292d       ce263a8653f9c                                                                                  2 weeks ago         Exited              create                    0                   a97e1cb7b527e       ingress-nginx-admission-create-gpvjp
ece464a61b202       busybox@sha256:bbc3a03235220b170ba48a157dd097dd1379299370e1ed99ce976df0355d24f0                2 weeks ago         Exited              test                      0                   58274a20107c2       test


==> controller_ingress [132d96445574] <==
W1204 03:03:08.251450       7 controller.go:1216] Service "trouble-3/fuchsia-svc" does not have any active Endpoint.
I1204 03:03:08.251545       7 controller.go:193] "Configuration changes detected, backend reload required"
I1204 03:03:08.255269       7 leaderelection.go:260] successfully acquired lease ingress-nginx/ingress-nginx-leader
I1204 03:03:08.255321       7 status.go:85] "New leader elected" identity="ingress-nginx-controller-bc57996ff-vwprn"
I1204 03:03:08.258628       7 status.go:219] "POD is not ready" pod="ingress-nginx/ingress-nginx-controller-bc57996ff-vwprn" node="minikube"
I1204 03:03:08.262569       7 status.go:304] "updating Ingress status" namespace="trouble-3" ingress="trouble-3-ingress" currentValue=[{"ip":"192.168.49.2"}] newValue=[]
I1204 03:03:08.262654       7 status.go:304] "updating Ingress status" namespace="default" ingress="aqua-ingress" currentValue=[{"ip":"192.168.49.2"}] newValue=[]
I1204 03:03:08.262657       7 status.go:304] "updating Ingress status" namespace="default" ingress="olive-ingress" currentValue=[{"ip":"192.168.49.2"}] newValue=[]
I1204 03:03:08.262970       7 status.go:304] "updating Ingress status" namespace="trouble-2" ingress="trouble-2-ingress" currentValue=[{"ip":"192.168.49.2"}] newValue=[]
I1204 03:03:08.263226       7 status.go:304] "updating Ingress status" namespace="default" ingress="trouble-1-ingress" currentValue=[{"ip":"192.168.49.2"}] newValue=[]
I1204 03:03:08.263496       7 status.go:304] "updating Ingress status" namespace="default" ingress="maroon-ingress" currentValue=[{"ip":"192.168.49.2"}] newValue=[]
I1204 03:03:08.269780       7 event.go:377] Event(v1.ObjectReference{Kind:"Ingress", Namespace:"trouble-3", Name:"trouble-3-ingress", UID:"81b84b43-216d-4986-ba3c-70c5495017f1", APIVersion:"networking.k8s.io/v1", ResourceVersion:"83005", FieldPath:""}): type: 'Normal' reason: 'Sync' Scheduled for sync
I1204 03:03:08.270055       7 event.go:377] Event(v1.ObjectReference{Kind:"Ingress", Namespace:"trouble-2", Name:"trouble-2-ingress", UID:"abdb27ee-5075-4c38-a7c9-57d17852a24c", APIVersion:"networking.k8s.io/v1", ResourceVersion:"83006", FieldPath:""}): type: 'Normal' reason: 'Sync' Scheduled for sync
I1204 03:03:08.270099       7 event.go:377] Event(v1.ObjectReference{Kind:"Ingress", Namespace:"default", Name:"trouble-1-ingress", UID:"75aa7499-9b45-44fb-bf97-e8f694c974d1", APIVersion:"networking.k8s.io/v1", ResourceVersion:"83007", FieldPath:""}): type: 'Normal' reason: 'Sync' Scheduled for sync
I1204 03:03:08.294459       7 controller.go:213] "Backend successfully reloaded"
I1204 03:03:08.294581       7 controller.go:224] "Initial sync, sleeping for 1 second"
I1204 03:03:08.294694       7 event.go:377] Event(v1.ObjectReference{Kind:"Pod", Namespace:"ingress-nginx", Name:"ingress-nginx-controller-bc57996ff-vwprn", UID:"cb0ebb01-387b-4029-9f6c-343159312194", APIVersion:"v1", ResourceVersion:"82556", FieldPath:""}): type: 'Normal' reason: 'RELOAD' NGINX reload triggered due to a change in configuration
I1204 03:03:08.518663       7 event.go:377] Event(v1.ObjectReference{Kind:"Ingress", Namespace:"default", Name:"aqua-ingress", UID:"3eba194e-08d1-4ad6-9318-48c09245405f", APIVersion:"networking.k8s.io/v1", ResourceVersion:"83009", FieldPath:""}): type: 'Normal' reason: 'Sync' Scheduled for sync
I1204 03:03:08.554140       7 event.go:377] Event(v1.ObjectReference{Kind:"Ingress", Namespace:"default", Name:"maroon-ingress", UID:"9c3a7337-a501-45c1-b73a-9a743c819539", APIVersion:"networking.k8s.io/v1", ResourceVersion:"83011", FieldPath:""}): type: 'Normal' reason: 'Sync' Scheduled for sync
I1204 03:03:08.753926       7 event.go:377] Event(v1.ObjectReference{Kind:"Ingress", Namespace:"default", Name:"olive-ingress", UID:"e4811641-dcdd-4a4f-b453-1ad8ed4d1f62", APIVersion:"networking.k8s.io/v1", ResourceVersion:"83018", FieldPath:""}): type: 'Normal' reason: 'Sync' Scheduled for sync
W1204 03:03:12.108760       7 controller.go:1110] Error obtaining Endpoints for Service "default/lime-svc": no object matching key "default/lime-svc" in local store
W1204 03:03:12.108815       7 controller.go:1110] Error obtaining Endpoints for Service "default/purple-svc": no object matching key "default/purple-svc" in local store
W1204 03:03:12.108900       7 controller.go:1216] Service "trouble-3/maroon-svc" does not have any active Endpoint.
W1204 03:03:12.108923       7 controller.go:1216] Service "trouble-3/fuchsia-svc" does not have any active Endpoint.
W1204 03:03:15.444984       7 controller.go:1110] Error obtaining Endpoints for Service "default/lime-svc": no object matching key "default/lime-svc" in local store
W1204 03:03:15.445029       7 controller.go:1110] Error obtaining Endpoints for Service "default/purple-svc": no object matching key "default/purple-svc" in local store
W1204 03:03:15.445042       7 controller.go:1216] Service "trouble-3/maroon-svc" does not have any active Endpoint.
W1204 03:03:15.445051       7 controller.go:1216] Service "trouble-3/fuchsia-svc" does not have any active Endpoint.
W1204 03:03:18.775581       7 controller.go:1110] Error obtaining Endpoints for Service "default/lime-svc": no object matching key "default/lime-svc" in local store
W1204 03:03:18.775638       7 controller.go:1110] Error obtaining Endpoints for Service "default/purple-svc": no object matching key "default/purple-svc" in local store
W1204 03:03:18.775661       7 controller.go:1216] Service "trouble-3/maroon-svc" does not have any active Endpoint.
W1204 03:03:18.775676       7 controller.go:1216] Service "trouble-3/fuchsia-svc" does not have any active Endpoint.
W1204 03:03:22.111715       7 controller.go:1110] Error obtaining Endpoints for Service "default/lime-svc": no object matching key "default/lime-svc" in local store
W1204 03:03:22.111764       7 controller.go:1110] Error obtaining Endpoints for Service "default/purple-svc": no object matching key "default/purple-svc" in local store
W1204 03:03:22.111779       7 controller.go:1216] Service "trouble-3/maroon-svc" does not have any active Endpoint.
W1204 03:03:22.111790       7 controller.go:1216] Service "trouble-3/fuchsia-svc" does not have any active Endpoint.
I1204 03:04:08.262441       7 status.go:304] "updating Ingress status" namespace="trouble-2" ingress="trouble-2-ingress" currentValue=null newValue=[{"ip":"192.168.49.2"}]
I1204 03:04:08.262779       7 status.go:304] "updating Ingress status" namespace="default" ingress="trouble-1-ingress" currentValue=null newValue=[{"ip":"192.168.49.2"}]
I1204 03:04:08.262840       7 status.go:304] "updating Ingress status" namespace="default" ingress="aqua-ingress" currentValue=null newValue=[{"ip":"192.168.49.2"}]
I1204 03:04:08.262862       7 status.go:304] "updating Ingress status" namespace="trouble-3" ingress="trouble-3-ingress" currentValue=null newValue=[{"ip":"192.168.49.2"}]
I1204 03:04:08.262964       7 status.go:304] "updating Ingress status" namespace="default" ingress="olive-ingress" currentValue=null newValue=[{"ip":"192.168.49.2"}]
I1204 03:04:08.263015       7 status.go:304] "updating Ingress status" namespace="default" ingress="maroon-ingress" currentValue=null newValue=[{"ip":"192.168.49.2"}]
I1204 03:04:08.268421       7 event.go:377] Event(v1.ObjectReference{Kind:"Ingress", Namespace:"trouble-3", Name:"trouble-3-ingress", UID:"81b84b43-216d-4986-ba3c-70c5495017f1", APIVersion:"networking.k8s.io/v1", ResourceVersion:"83099", FieldPath:""}): type: 'Normal' reason: 'Sync' Scheduled for sync
W1204 03:04:08.268484       7 controller.go:1110] Error obtaining Endpoints for Service "default/lime-svc": no object matching key "default/lime-svc" in local store
W1204 03:04:08.268501       7 controller.go:1110] Error obtaining Endpoints for Service "default/purple-svc": no object matching key "default/purple-svc" in local store
W1204 03:04:08.268514       7 controller.go:1216] Service "trouble-3/maroon-svc" does not have any active Endpoint.
W1204 03:04:08.268524       7 controller.go:1216] Service "trouble-3/fuchsia-svc" does not have any active Endpoint.
I1204 03:04:08.271055       7 event.go:377] Event(v1.ObjectReference{Kind:"Ingress", Namespace:"default", Name:"trouble-1-ingress", UID:"75aa7499-9b45-44fb-bf97-e8f694c974d1", APIVersion:"networking.k8s.io/v1", ResourceVersion:"83100", FieldPath:""}): type: 'Normal' reason: 'Sync' Scheduled for sync
I1204 03:04:08.271085       7 event.go:377] Event(v1.ObjectReference{Kind:"Ingress", Namespace:"trouble-2", Name:"trouble-2-ingress", UID:"abdb27ee-5075-4c38-a7c9-57d17852a24c", APIVersion:"networking.k8s.io/v1", ResourceVersion:"83101", FieldPath:""}): type: 'Normal' reason: 'Sync' Scheduled for sync
I1204 03:04:08.271089       7 event.go:377] Event(v1.ObjectReference{Kind:"Ingress", Namespace:"default", Name:"aqua-ingress", UID:"3eba194e-08d1-4ad6-9318-48c09245405f", APIVersion:"networking.k8s.io/v1", ResourceVersion:"83102", FieldPath:""}): type: 'Normal' reason: 'Sync' Scheduled for sync
I1204 03:04:08.467987       7 event.go:377] Event(v1.ObjectReference{Kind:"Ingress", Namespace:"default", Name:"olive-ingress", UID:"e4811641-dcdd-4a4f-b453-1ad8ed4d1f62", APIVersion:"networking.k8s.io/v1", ResourceVersion:"83108", FieldPath:""}): type: 'Normal' reason: 'Sync' Scheduled for sync
I1204 03:04:08.670662       7 event.go:377] Event(v1.ObjectReference{Kind:"Ingress", Namespace:"default", Name:"maroon-ingress", UID:"9c3a7337-a501-45c1-b73a-9a743c819539", APIVersion:"networking.k8s.io/v1", ResourceVersion:"83110", FieldPath:""}): type: 'Normal' reason: 'Sync' Scheduled for sync
W1204 03:04:11.604835       7 controller.go:1110] Error obtaining Endpoints for Service "default/lime-svc": no object matching key "default/lime-svc" in local store
W1204 03:04:11.604879       7 controller.go:1110] Error obtaining Endpoints for Service "default/purple-svc": no object matching key "default/purple-svc" in local store
W1204 03:04:11.604895       7 controller.go:1216] Service "trouble-3/maroon-svc" does not have any active Endpoint.
W1204 03:04:11.604906       7 controller.go:1216] Service "trouble-3/fuchsia-svc" does not have any active Endpoint.
W1204 03:04:14.935965       7 controller.go:1110] Error obtaining Endpoints for Service "default/lime-svc": no object matching key "default/lime-svc" in local store
W1204 03:04:14.936001       7 controller.go:1110] Error obtaining Endpoints for Service "default/purple-svc": no object matching key "default/purple-svc" in local store
W1204 03:04:14.936016       7 controller.go:1216] Service "trouble-3/maroon-svc" does not have any active Endpoint.
W1204 03:04:14.936027       7 controller.go:1216] Service "trouble-3/fuchsia-svc" does not have any active Endpoint.


==> controller_ingress [4397302e5a32] <==
W1217 04:48:55.077427       7 controller.go:1216] Service "trouble-3/fuchsia-svc" does not have any active Endpoint.
I1217 04:48:55.077691       7 controller.go:193] "Configuration changes detected, backend reload required"
I1217 04:48:55.166664       7 leaderelection.go:260] successfully acquired lease ingress-nginx/ingress-nginx-leader
I1217 04:48:55.166730       7 status.go:85] "New leader elected" identity="ingress-nginx-controller-bc57996ff-vwprn"
I1217 04:48:55.173105       7 status.go:219] "POD is not ready" pod="ingress-nginx/ingress-nginx-controller-bc57996ff-vwprn" node="minikube"
I1217 04:48:55.266173       7 status.go:304] "updating Ingress status" namespace="default" ingress="olive-ingress" currentValue=[{"ip":"192.168.49.2"}] newValue=[]
I1217 04:48:55.266294       7 status.go:304] "updating Ingress status" namespace="default" ingress="aqua-ingress" currentValue=[{"ip":"192.168.49.2"}] newValue=[]
I1217 04:48:55.266362       7 status.go:304] "updating Ingress status" namespace="trouble-3" ingress="trouble-3-ingress" currentValue=[{"ip":"192.168.49.2"}] newValue=[]
I1217 04:48:55.266410       7 status.go:304] "updating Ingress status" namespace="trouble-2" ingress="trouble-2-ingress" currentValue=[{"ip":"192.168.49.2"}] newValue=[]
I1217 04:48:55.266462       7 status.go:304] "updating Ingress status" namespace="default" ingress="trouble-1-ingress" currentValue=[{"ip":"192.168.49.2"}] newValue=[]
I1217 04:48:55.266186       7 status.go:304] "updating Ingress status" namespace="default" ingress="maroon-ingress" currentValue=[{"ip":"192.168.49.2"}] newValue=[]
I1217 04:48:55.282532       7 event.go:377] Event(v1.ObjectReference{Kind:"Ingress", Namespace:"default", Name:"trouble-1-ingress", UID:"75aa7499-9b45-44fb-bf97-e8f694c974d1", APIVersion:"networking.k8s.io/v1", ResourceVersion:"85688", FieldPath:""}): type: 'Normal' reason: 'Sync' Scheduled for sync
I1217 04:48:55.283125       7 event.go:377] Event(v1.ObjectReference{Kind:"Ingress", Namespace:"default", Name:"aqua-ingress", UID:"3eba194e-08d1-4ad6-9318-48c09245405f", APIVersion:"networking.k8s.io/v1", ResourceVersion:"85689", FieldPath:""}): type: 'Normal' reason: 'Sync' Scheduled for sync
I1217 04:48:55.283163       7 event.go:377] Event(v1.ObjectReference{Kind:"Ingress", Namespace:"default", Name:"olive-ingress", UID:"e4811641-dcdd-4a4f-b453-1ad8ed4d1f62", APIVersion:"networking.k8s.io/v1", ResourceVersion:"85690", FieldPath:""}): type: 'Normal' reason: 'Sync' Scheduled for sync
I1217 04:48:55.283379       7 event.go:377] Event(v1.ObjectReference{Kind:"Ingress", Namespace:"trouble-3", Name:"trouble-3-ingress", UID:"81b84b43-216d-4986-ba3c-70c5495017f1", APIVersion:"networking.k8s.io/v1", ResourceVersion:"85691", FieldPath:""}): type: 'Normal' reason: 'Sync' Scheduled for sync
I1217 04:48:55.380263       7 event.go:377] Event(v1.ObjectReference{Kind:"Ingress", Namespace:"trouble-2", Name:"trouble-2-ingress", UID:"abdb27ee-5075-4c38-a7c9-57d17852a24c", APIVersion:"networking.k8s.io/v1", ResourceVersion:"85694", FieldPath:""}): type: 'Normal' reason: 'Sync' Scheduled for sync
I1217 04:48:55.489666       7 controller.go:213] "Backend successfully reloaded"
I1217 04:48:55.489803       7 controller.go:224] "Initial sync, sleeping for 1 second"
I1217 04:48:55.489989       7 event.go:377] Event(v1.ObjectReference{Kind:"Pod", Namespace:"ingress-nginx", Name:"ingress-nginx-controller-bc57996ff-vwprn", UID:"cb0ebb01-387b-4029-9f6c-343159312194", APIVersion:"v1", ResourceVersion:"85637", FieldPath:""}): type: 'Normal' reason: 'RELOAD' NGINX reload triggered due to a change in configuration
I1217 04:48:55.584019       7 event.go:377] Event(v1.ObjectReference{Kind:"Ingress", Namespace:"default", Name:"maroon-ingress", UID:"9c3a7337-a501-45c1-b73a-9a743c819539", APIVersion:"networking.k8s.io/v1", ResourceVersion:"85698", FieldPath:""}): type: 'Normal' reason: 'Sync' Scheduled for sync
W1217 04:48:58.832441       7 controller.go:1110] Error obtaining Endpoints for Service "default/lime-svc": no object matching key "default/lime-svc" in local store
W1217 04:48:58.832486       7 controller.go:1110] Error obtaining Endpoints for Service "default/purple-svc": no object matching key "default/purple-svc" in local store
W1217 04:48:58.832501       7 controller.go:1216] Service "trouble-3/maroon-svc" does not have any active Endpoint.
W1217 04:48:58.832512       7 controller.go:1216] Service "trouble-3/fuchsia-svc" does not have any active Endpoint.
W1217 04:49:02.166378       7 controller.go:1110] Error obtaining Endpoints for Service "default/lime-svc": no object matching key "default/lime-svc" in local store
W1217 04:49:02.166571       7 controller.go:1110] Error obtaining Endpoints for Service "default/purple-svc": no object matching key "default/purple-svc" in local store
W1217 04:49:02.166630       7 controller.go:1216] Service "trouble-3/maroon-svc" does not have any active Endpoint.
W1217 04:49:02.166659       7 controller.go:1216] Service "trouble-3/fuchsia-svc" does not have any active Endpoint.
W1217 04:49:05.500355       7 controller.go:1110] Error obtaining Endpoints for Service "default/lime-svc": no object matching key "default/lime-svc" in local store
W1217 04:49:05.500426       7 controller.go:1110] Error obtaining Endpoints for Service "default/purple-svc": no object matching key "default/purple-svc" in local store
W1217 04:49:05.500455       7 controller.go:1216] Service "trouble-3/maroon-svc" does not have any active Endpoint.
W1217 04:49:05.500474       7 controller.go:1216] Service "trouble-3/fuchsia-svc" does not have any active Endpoint.
W1217 04:49:08.835167       7 controller.go:1110] Error obtaining Endpoints for Service "default/lime-svc": no object matching key "default/lime-svc" in local store
W1217 04:49:08.835250       7 controller.go:1110] Error obtaining Endpoints for Service "default/purple-svc": no object matching key "default/purple-svc" in local store
W1217 04:49:08.835278       7 controller.go:1216] Service "trouble-3/maroon-svc" does not have any active Endpoint.
W1217 04:49:08.835302       7 controller.go:1216] Service "trouble-3/fuchsia-svc" does not have any active Endpoint.
I1217 04:49:55.181157       7 status.go:304] "updating Ingress status" namespace="default" ingress="trouble-1-ingress" currentValue=null newValue=[{"ip":"192.168.49.2"}]
I1217 04:49:55.181260       7 status.go:304] "updating Ingress status" namespace="default" ingress="olive-ingress" currentValue=null newValue=[{"ip":"192.168.49.2"}]
I1217 04:49:55.181263       7 status.go:304] "updating Ingress status" namespace="trouble-3" ingress="trouble-3-ingress" currentValue=null newValue=[{"ip":"192.168.49.2"}]
I1217 04:49:55.181156       7 status.go:304] "updating Ingress status" namespace="default" ingress="maroon-ingress" currentValue=null newValue=[{"ip":"192.168.49.2"}]
I1217 04:49:55.181337       7 status.go:304] "updating Ingress status" namespace="default" ingress="aqua-ingress" currentValue=null newValue=[{"ip":"192.168.49.2"}]
I1217 04:49:55.181157       7 status.go:304] "updating Ingress status" namespace="trouble-2" ingress="trouble-2-ingress" currentValue=null newValue=[{"ip":"192.168.49.2"}]
I1217 04:49:55.190046       7 event.go:377] Event(v1.ObjectReference{Kind:"Ingress", Namespace:"default", Name:"olive-ingress", UID:"e4811641-dcdd-4a4f-b453-1ad8ed4d1f62", APIVersion:"networking.k8s.io/v1", ResourceVersion:"85804", FieldPath:""}): type: 'Normal' reason: 'Sync' Scheduled for sync
W1217 04:49:55.190234       7 controller.go:1110] Error obtaining Endpoints for Service "default/lime-svc": no object matching key "default/lime-svc" in local store
W1217 04:49:55.190259       7 controller.go:1110] Error obtaining Endpoints for Service "default/purple-svc": no object matching key "default/purple-svc" in local store
I1217 04:49:55.190253       7 event.go:377] Event(v1.ObjectReference{Kind:"Ingress", Namespace:"default", Name:"maroon-ingress", UID:"9c3a7337-a501-45c1-b73a-9a743c819539", APIVersion:"networking.k8s.io/v1", ResourceVersion:"85805", FieldPath:""}): type: 'Normal' reason: 'Sync' Scheduled for sync
W1217 04:49:55.190283       7 controller.go:1216] Service "trouble-3/maroon-svc" does not have any active Endpoint.
W1217 04:49:55.190303       7 controller.go:1216] Service "trouble-3/fuchsia-svc" does not have any active Endpoint.
I1217 04:49:55.193744       7 event.go:377] Event(v1.ObjectReference{Kind:"Ingress", Namespace:"default", Name:"trouble-1-ingress", UID:"75aa7499-9b45-44fb-bf97-e8f694c974d1", APIVersion:"networking.k8s.io/v1", ResourceVersion:"85806", FieldPath:""}): type: 'Normal' reason: 'Sync' Scheduled for sync
I1217 04:49:55.194093       7 event.go:377] Event(v1.ObjectReference{Kind:"Ingress", Namespace:"trouble-3", Name:"trouble-3-ingress", UID:"81b84b43-216d-4986-ba3c-70c5495017f1", APIVersion:"networking.k8s.io/v1", ResourceVersion:"85807", FieldPath:""}): type: 'Normal' reason: 'Sync' Scheduled for sync
I1217 04:49:55.387368       7 event.go:377] Event(v1.ObjectReference{Kind:"Ingress", Namespace:"default", Name:"aqua-ingress", UID:"3eba194e-08d1-4ad6-9318-48c09245405f", APIVersion:"networking.k8s.io/v1", ResourceVersion:"85813", FieldPath:""}): type: 'Normal' reason: 'Sync' Scheduled for sync
I1217 04:49:55.588303       7 event.go:377] Event(v1.ObjectReference{Kind:"Ingress", Namespace:"trouble-2", Name:"trouble-2-ingress", UID:"abdb27ee-5075-4c38-a7c9-57d17852a24c", APIVersion:"networking.k8s.io/v1", ResourceVersion:"85815", FieldPath:""}): type: 'Normal' reason: 'Sync' Scheduled for sync
W1217 04:49:58.526292       7 controller.go:1110] Error obtaining Endpoints for Service "default/lime-svc": no object matching key "default/lime-svc" in local store
W1217 04:49:58.526384       7 controller.go:1110] Error obtaining Endpoints for Service "default/purple-svc": no object matching key "default/purple-svc" in local store
W1217 04:49:58.526418       7 controller.go:1216] Service "trouble-3/maroon-svc" does not have any active Endpoint.
W1217 04:49:58.526446       7 controller.go:1216] Service "trouble-3/fuchsia-svc" does not have any active Endpoint.
W1217 04:50:01.859557       7 controller.go:1110] Error obtaining Endpoints for Service "default/lime-svc": no object matching key "default/lime-svc" in local store
W1217 04:50:01.859652       7 controller.go:1110] Error obtaining Endpoints for Service "default/purple-svc": no object matching key "default/purple-svc" in local store
W1217 04:50:01.859680       7 controller.go:1216] Service "trouble-3/maroon-svc" does not have any active Endpoint.
W1217 04:50:01.859701       7 controller.go:1216] Service "trouble-3/fuchsia-svc" does not have any active Endpoint.


==> coredns [3f89f81dd483] <==
.:53
[INFO] plugin/reload: Running configuration SHA512 = 05e3eaddc414b2d71a69b2e2bc6f2681fc1f4d04bcdd3acc1a41457bb7db518208b95ddfc4c9fffedc59c25a8faf458be1af4915a4a3c0d6777cb7a346bc5d86
CoreDNS-1.11.1
linux/amd64, go1.20.7, ae2bbc2
[INFO] 127.0.0.1:46093 - 28567 "HINFO IN 8384718630804915883.1185749979482862343. udp 57 false 512" NXDOMAIN qr,rd,ra 132 0.186039946s


==> coredns [58045466750e] <==
.:53
[INFO] plugin/reload: Running configuration SHA512 = 05e3eaddc414b2d71a69b2e2bc6f2681fc1f4d04bcdd3acc1a41457bb7db518208b95ddfc4c9fffedc59c25a8faf458be1af4915a4a3c0d6777cb7a346bc5d86
CoreDNS-1.11.1
linux/amd64, go1.20.7, ae2bbc2
[INFO] 127.0.0.1:36439 - 47454 "HINFO IN 4337111397756150745.2038698812551146481. udp 57 false 512" NXDOMAIN qr,rd,ra 132 0.096426795s


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=210b148df93a80eb872ecbeb7e35281b3c582c61
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2024_11_29T15_32_12_0700
                    minikube.k8s.io/version=v1.34.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Fri, 29 Nov 2024 10:02:09 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Tue, 17 Dec 2024 05:26:30 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Tue, 17 Dec 2024 05:24:14 +0000   Fri, 29 Nov 2024 10:02:07 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Tue, 17 Dec 2024 05:24:14 +0000   Fri, 29 Nov 2024 10:02:07 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Tue, 17 Dec 2024 05:24:14 +0000   Fri, 29 Nov 2024 10:02:07 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Tue, 17 Dec 2024 05:24:14 +0000   Fri, 29 Nov 2024 10:02:09 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                18
  ephemeral-storage:  263174212Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             16101088Ki
  pods:               110
Allocatable:
  cpu:                18
  ephemeral-storage:  263174212Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             16101088Ki
  pods:               110
System Info:
  Machine ID:                 9b678b5f0f0d4c019d8f2781b8ff7bfd
  System UUID:                9b678b5f0f0d4c019d8f2781b8ff7bfd
  Boot ID:                    6f7348ae-6282-449b-a491-e0081a8bf6b6
  Kernel Version:             5.10.102.1-microsoft-standard-WSL2
  OS Image:                   Ubuntu 22.04.4 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://27.2.0
  Kubelet Version:            v1.31.0
  Kube-Proxy Version:         
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (40 in total)
  Namespace                   Name                                        CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                        ------------  ----------  ---------------  -------------  ---
  default                     aqua-69c54df8b9-9btjb                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         16d
  default                     aqua-69c54df8b9-dz6z2                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         16d
  default                     busybox-pod                                 0 (0%)        0 (0%)      0 (0%)           0 (0%)         17d
  default                     hello-hello-648574587d-54s9h                0 (0%)        0 (0%)      0 (0%)           0 (0%)         17d
  default                     maroon-6f56fdcfd9-gmskx                     0 (0%)        0 (0%)      0 (0%)           0 (0%)         16d
  default                     maroon-6f56fdcfd9-q7blx                     0 (0%)        0 (0%)      0 (0%)           0 (0%)         16d
  default                     my-nginx-7bc4c7bc9c-69b5w                   100m (0%)     150m (0%)   128Mi (0%)       192Mi (1%)     14d
  default                     myapp-7d66457db5-sx6nb                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         17d
  default                     olive-7d4998978f-cb2f7                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         16d
  default                     olive-7d4998978f-rcrh4                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         16d
  default                     pod-info-app-65f569fd44-6xklz               0 (0%)        0 (0%)      0 (0%)           0 (0%)         14d
  default                     pod-info-app-65f569fd44-824nt               0 (0%)        0 (0%)      0 (0%)           0 (0%)         14d
  default                     pod-info-app-65f569fd44-gq624               0 (0%)        0 (0%)      0 (0%)           0 (0%)         17d
  default                     pod-info-app-65f569fd44-kz4h5               0 (0%)        0 (0%)      0 (0%)           0 (0%)         17d
  default                     pod-info-app-65f569fd44-tssr5               0 (0%)        0 (0%)      0 (0%)           0 (0%)         14d
  default                     pod-info-app-65f569fd44-wbm4r               0 (0%)        0 (0%)      0 (0%)           0 (0%)         14d
  ingress-nginx               aqua-69c54df8b9-9tcgb                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         16d
  ingress-nginx               aqua-69c54df8b9-vg99d                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         16d
  ingress-nginx               ingress-nginx-controller-bc57996ff-vwprn    100m (0%)     0 (0%)      90Mi (0%)        0 (0%)         16d
  ingress-nginx               maroon-6f56fdcfd9-5zqtv                     0 (0%)        0 (0%)      0 (0%)           0 (0%)         16d
  ingress-nginx               maroon-6f56fdcfd9-jhxrq                     0 (0%)        0 (0%)      0 (0%)           0 (0%)         16d
  ingress-nginx               olive-7d4998978f-d9lkn                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         16d
  ingress-nginx               olive-7d4998978f-nmnqr                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         16d
  kube-system                 coredns-6f6b679f8f-7sjhg                    100m (0%)     0 (0%)      70Mi (0%)        170Mi (1%)     17d
  kube-system                 etcd-minikube                               100m (0%)     0 (0%)      100Mi (0%)       0 (0%)         17d
  kube-system                 kube-apiserver-minikube                     250m (1%)     0 (0%)      0 (0%)           0 (0%)         17d
  kube-system                 kube-controller-manager-minikube            200m (1%)     0 (0%)      0 (0%)           0 (0%)         17d
  kube-system                 kube-proxy-gdfs7                            0 (0%)        0 (0%)      0 (0%)           0 (0%)         17d
  kube-system                 kube-scheduler-minikube                     100m (0%)     0 (0%)      0 (0%)           0 (0%)         17d
  kube-system                 storage-provisioner                         0 (0%)        0 (0%)      0 (0%)           0 (0%)         17d
  trouble-1                   lime-color-6bdd87b769-tmczz                 0 (0%)        0 (0%)      0 (0%)           0 (0%)         16d
  trouble-1                   lime-color-6bdd87b769-wn6nr                 0 (0%)        0 (0%)      0 (0%)           0 (0%)         16d
  trouble-1                   lime-color-6bdd87b769-zq96q                 0 (0%)        0 (0%)      0 (0%)           0 (0%)         16d
  trouble-1                   purple-color-6994fb8f65-5cp9b               0 (0%)        0 (0%)      0 (0%)           0 (0%)         16d
  trouble-1                   purple-color-6994fb8f65-n9v4p               0 (0%)        0 (0%)      0 (0%)           0 (0%)         16d
  trouble-2                   navy-color-7596c47445-gdqsd                 0 (0%)        0 (0%)      0 (0%)           0 (0%)         16d
  trouble-2                   navy-color-7596c47445-sl2m8                 0 (0%)        0 (0%)      0 (0%)           0 (0%)         16d
  trouble-2                   teal-color-555b59785-52kh5                  0 (0%)        0 (0%)      0 (0%)           0 (0%)         16d
  trouble-3                   fuchsia-color-7f46c97957-59r62              0 (0%)        0 (0%)      0 (0%)           0 (0%)         16d
  trouble-3                   fuchsia-color-7f46c97957-cfxfl              0 (0%)        0 (0%)      0 (0%)           0 (0%)         16d
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                950m (5%)   150m (0%)
  memory             388Mi (2%)  362Mi (2%)
  ephemeral-storage  50Mi (0%)   2Gi (0%)
  hugepages-1Gi      0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
Events:
  Type     Reason                             Age                From             Message
  ----     ------                             ----               ----             -------
  Normal   Starting                           13d                kube-proxy       
  Normal   Starting                           37m                kube-proxy       
  Normal   NodeAllocatableEnforced            13d                kubelet          Updated Node Allocatable limit across pods
  Warning  PossibleMemoryBackedVolumesOnDisk  13d                kubelet          The tmpfs noswap option is not supported. Memory-backed volumes (e.g. secrets, emptyDirs, etc.) might be swapped to disk and should no longer be considered secure.
  Warning  CgroupV1                           13d                kubelet          Cgroup v1 support is in maintenance mode, please migrate to Cgroup v2.
  Normal   NodeHasSufficientMemory            13d (x7 over 13d)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure              13d (x7 over 13d)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID               13d (x7 over 13d)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   Starting                           13d                kubelet          Starting kubelet.
  Normal   RegisteredNode                     13d                node-controller  Node minikube event: Registered Node minikube in Controller
  Warning  PossibleMemoryBackedVolumesOnDisk  38m                kubelet          The tmpfs noswap option is not supported. Memory-backed volumes (e.g. secrets, emptyDirs, etc.) might be swapped to disk and should no longer be considered secure.
  Normal   Starting                           38m                kubelet          Starting kubelet.
  Warning  CgroupV1                           38m                kubelet          Cgroup v1 support is in maintenance mode, please migrate to Cgroup v2.
  Normal   NodeHasSufficientMemory            38m (x7 over 38m)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure              38m (x7 over 38m)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID               38m (x7 over 38m)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   NodeAllocatableEnforced            38m                kubelet          Updated Node Allocatable limit across pods
  Normal   RegisteredNode                     37m                node-controller  Node minikube event: Registered Node minikube in Controller


==> dmesg <==
[Dec17 04:47] PCI: Fatal: No config space access function found
[  +0.113617] PCI: System does not support PCI
[  +0.229511] kvm: already loaded the other module
[  +0.271564] hv_utils: cannot register PTP clock: 0
[  +2.002595] FS-Cache: Duplicate cookie detected
[  +0.001211] FS-Cache: O-cookie c=000000005d0c977a [p=0000000026ac1897 fl=222 nc=0 na=1]
[  +0.000856] FS-Cache: O-cookie d=00000000de17d861 n=0000000059f0aa9b
[  +0.000977] FS-Cache: O-key=[10] '34323934393337353637'
[  +0.000816] FS-Cache: N-cookie c=0000000031389eac [p=0000000026ac1897 fl=2 nc=0 na=1]
[  +0.000736] FS-Cache: N-cookie d=00000000de17d861 n=000000009983377f
[  +0.000738] FS-Cache: N-key=[10] '34323934393337353637'
[Dec17 04:48] cgroup: runc (749) created nested cgroup for controller "memory" which has incomplete hierarchy support. Nested cgroups may change behavior in the future.
[  +0.001984] cgroup: "memory" requires setting use_hierarchy to 1 on the root
[ +10.725810] tmpfs: Unknown parameter 'noswap'


==> etcd [103a3061ff3d] <==
{"level":"info","ts":"2024-12-04T03:02:57.227042Z","caller":"traceutil/trace.go:171","msg":"trace[201681890] transaction","detail":"{read_only:false; response_revision:82705; number_of_response:1; }","duration":"105.070172ms","start":"2024-12-04T03:02:57.121948Z","end":"2024-12-04T03:02:57.227018Z","steps":["trace[201681890] 'process raft request'  (duration: 95.606535ms)"],"step_count":1}
{"level":"info","ts":"2024-12-04T03:02:57.227200Z","caller":"traceutil/trace.go:171","msg":"trace[489596687] transaction","detail":"{read_only:false; response_revision:82706; number_of_response:1; }","duration":"105.216014ms","start":"2024-12-04T03:02:57.121947Z","end":"2024-12-04T03:02:57.227163Z","steps":["trace[489596687] 'process raft request'  (duration: 104.984822ms)"],"step_count":1}
{"level":"info","ts":"2024-12-04T03:02:57.227248Z","caller":"traceutil/trace.go:171","msg":"trace[1811589412] transaction","detail":"{read_only:false; response_revision:82707; number_of_response:1; }","duration":"105.20135ms","start":"2024-12-04T03:02:57.122040Z","end":"2024-12-04T03:02:57.227241Z","steps":["trace[1811589412] 'process raft request'  (duration: 104.9292ms)"],"step_count":1}
{"level":"info","ts":"2024-12-04T03:02:57.227484Z","caller":"traceutil/trace.go:171","msg":"trace[1858677566] transaction","detail":"{read_only:false; response_revision:82708; number_of_response:1; }","duration":"105.403688ms","start":"2024-12-04T03:02:57.122071Z","end":"2024-12-04T03:02:57.227474Z","steps":["trace[1858677566] 'process raft request'  (duration: 105.132957ms)"],"step_count":1}
{"level":"info","ts":"2024-12-04T03:02:57.227555Z","caller":"traceutil/trace.go:171","msg":"trace[1274917192] transaction","detail":"{read_only:false; response_revision:82711; number_of_response:1; }","duration":"101.573929ms","start":"2024-12-04T03:02:57.125974Z","end":"2024-12-04T03:02:57.227548Z","steps":["trace[1274917192] 'process raft request'  (duration: 101.528539ms)"],"step_count":1}
{"level":"info","ts":"2024-12-04T03:02:57.227599Z","caller":"traceutil/trace.go:171","msg":"trace[1707832526] transaction","detail":"{read_only:false; response_revision:82710; number_of_response:1; }","duration":"104.109173ms","start":"2024-12-04T03:02:57.123475Z","end":"2024-12-04T03:02:57.227584Z","steps":["trace[1707832526] 'process raft request'  (duration: 104.006046ms)"],"step_count":1}
{"level":"info","ts":"2024-12-04T03:02:57.227608Z","caller":"traceutil/trace.go:171","msg":"trace[840460482] transaction","detail":"{read_only:false; response_revision:82709; number_of_response:1; }","duration":"105.476074ms","start":"2024-12-04T03:02:57.122126Z","end":"2024-12-04T03:02:57.227602Z","steps":["trace[840460482] 'process raft request'  (duration: 105.319042ms)"],"step_count":1}
{"level":"info","ts":"2024-12-04T03:02:57.423495Z","caller":"traceutil/trace.go:171","msg":"trace[837272760] linearizableReadLoop","detail":"{readStateIndex:100172; appliedIndex:100170; }","duration":"100.745025ms","start":"2024-12-04T03:02:57.322731Z","end":"2024-12-04T03:02:57.423476Z","steps":["trace[837272760] 'read index received'  (duration: 11.35131ms)","trace[837272760] 'applied index is now lower than readState.Index'  (duration: 89.393103ms)"],"step_count":2}
{"level":"info","ts":"2024-12-04T03:02:57.423536Z","caller":"traceutil/trace.go:171","msg":"trace[1117187429] transaction","detail":"{read_only:false; response_revision:82713; number_of_response:1; }","duration":"105.525284ms","start":"2024-12-04T03:02:57.317989Z","end":"2024-12-04T03:02:57.423514Z","steps":["trace[1117187429] 'process raft request'  (duration: 14.582997ms)","trace[1117187429] 'compare'  (duration: 90.554652ms)"],"step_count":2}
{"level":"info","ts":"2024-12-04T03:02:57.423550Z","caller":"traceutil/trace.go:171","msg":"trace[1756279766] transaction","detail":"{read_only:false; response_revision:82714; number_of_response:1; }","duration":"101.830656ms","start":"2024-12-04T03:02:57.321703Z","end":"2024-12-04T03:02:57.423534Z","steps":["trace[1756279766] 'process raft request'  (duration: 101.659294ms)"],"step_count":1}
{"level":"warn","ts":"2024-12-04T03:02:57.423597Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"105.795529ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2024-12-04T03:02:57.423621Z","caller":"traceutil/trace.go:171","msg":"trace[1177000382] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:82714; }","duration":"105.839203ms","start":"2024-12-04T03:02:57.317776Z","end":"2024-12-04T03:02:57.423615Z","steps":["trace[1177000382] 'agreement among raft nodes before linearized reading'  (duration: 105.783255ms)"],"step_count":1}
{"level":"info","ts":"2024-12-04T03:02:57.618005Z","caller":"traceutil/trace.go:171","msg":"trace[2037232653] transaction","detail":"{read_only:false; response_revision:82718; number_of_response:1; }","duration":"189.902499ms","start":"2024-12-04T03:02:57.428079Z","end":"2024-12-04T03:02:57.617981Z","steps":["trace[2037232653] 'process raft request'  (duration: 189.640987ms)"],"step_count":1}
{"level":"info","ts":"2024-12-04T03:02:57.622766Z","caller":"traceutil/trace.go:171","msg":"trace[1737647294] transaction","detail":"{read_only:false; response_revision:82719; number_of_response:1; }","duration":"101.267259ms","start":"2024-12-04T03:02:57.521473Z","end":"2024-12-04T03:02:57.622740Z","steps":["trace[1737647294] 'process raft request'  (duration: 101.124972ms)"],"step_count":1}
{"level":"info","ts":"2024-12-04T03:02:59.630676Z","caller":"traceutil/trace.go:171","msg":"trace[910180158] transaction","detail":"{read_only:false; response_revision:82751; number_of_response:1; }","duration":"106.054454ms","start":"2024-12-04T03:02:59.524588Z","end":"2024-12-04T03:02:59.630642Z","steps":["trace[910180158] 'process raft request'  (duration: 93.183609ms)","trace[910180158] 'compare'  (duration: 12.332835ms)"],"step_count":2}
{"level":"info","ts":"2024-12-04T03:03:00.018127Z","caller":"traceutil/trace.go:171","msg":"trace[394600205] transaction","detail":"{read_only:false; response_revision:82757; number_of_response:1; }","duration":"100.158931ms","start":"2024-12-04T03:02:59.917949Z","end":"2024-12-04T03:03:00.018108Z","steps":["trace[394600205] 'compare'  (duration: 92.968301ms)"],"step_count":1}
{"level":"info","ts":"2024-12-04T03:03:00.117849Z","caller":"traceutil/trace.go:171","msg":"trace[1278553095] transaction","detail":"{read_only:false; response_revision:82759; number_of_response:1; }","duration":"192.417702ms","start":"2024-12-04T03:02:59.925408Z","end":"2024-12-04T03:03:00.117826Z","steps":["trace[1278553095] 'process raft request'  (duration: 192.252858ms)"],"step_count":1}
{"level":"info","ts":"2024-12-04T03:03:00.221180Z","caller":"traceutil/trace.go:171","msg":"trace[1210528288] transaction","detail":"{read_only:false; response_revision:82760; number_of_response:1; }","duration":"200.532463ms","start":"2024-12-04T03:03:00.020620Z","end":"2024-12-04T03:03:00.221152Z","steps":["trace[1210528288] 'process raft request'  (duration: 103.679878ms)","trace[1210528288] 'compare'  (duration: 96.648169ms)"],"step_count":2}
{"level":"info","ts":"2024-12-04T03:03:00.221240Z","caller":"traceutil/trace.go:171","msg":"trace[97289747] transaction","detail":"{read_only:false; response_revision:82761; number_of_response:1; }","duration":"103.361869ms","start":"2024-12-04T03:03:00.117857Z","end":"2024-12-04T03:03:00.221219Z","steps":["trace[97289747] 'process raft request'  (duration: 103.21474ms)"],"step_count":1}
{"level":"info","ts":"2024-12-04T03:03:00.422204Z","caller":"traceutil/trace.go:171","msg":"trace[777913682] transaction","detail":"{read_only:false; response_revision:82764; number_of_response:1; }","duration":"102.524856ms","start":"2024-12-04T03:03:00.319651Z","end":"2024-12-04T03:03:00.422176Z","steps":["trace[777913682] 'process raft request'  (duration: 102.407518ms)"],"step_count":1}
{"level":"info","ts":"2024-12-04T03:03:00.818093Z","caller":"traceutil/trace.go:171","msg":"trace[401052680] transaction","detail":"{read_only:false; response_revision:82767; number_of_response:1; }","duration":"196.571757ms","start":"2024-12-04T03:03:00.621497Z","end":"2024-12-04T03:03:00.818069Z","steps":["trace[401052680] 'process raft request'  (duration: 196.407333ms)"],"step_count":1}
{"level":"info","ts":"2024-12-04T03:03:01.223058Z","caller":"traceutil/trace.go:171","msg":"trace[145432466] transaction","detail":"{read_only:false; response_revision:82770; number_of_response:1; }","duration":"101.047954ms","start":"2024-12-04T03:03:01.121978Z","end":"2024-12-04T03:03:01.223026Z","steps":["trace[145432466] 'process raft request'  (duration: 95.647935ms)"],"step_count":1}
{"level":"info","ts":"2024-12-04T03:03:01.525157Z","caller":"traceutil/trace.go:171","msg":"trace[1228667593] transaction","detail":"{read_only:false; response_revision:82773; number_of_response:1; }","duration":"100.95773ms","start":"2024-12-04T03:03:01.424173Z","end":"2024-12-04T03:03:01.525131Z","steps":["trace[1228667593] 'process raft request'  (duration: 93.469115ms)"],"step_count":1}
{"level":"info","ts":"2024-12-04T03:03:01.922424Z","caller":"traceutil/trace.go:171","msg":"trace[1858005507] transaction","detail":"{read_only:false; response_revision:82776; number_of_response:1; }","duration":"100.057892ms","start":"2024-12-04T03:03:01.822342Z","end":"2024-12-04T03:03:01.922399Z","steps":["trace[1858005507] 'process raft request'  (duration: 95.244052ms)"],"step_count":1}
{"level":"info","ts":"2024-12-04T03:03:02.228119Z","caller":"traceutil/trace.go:171","msg":"trace[10591732] transaction","detail":"{read_only:false; response_revision:82780; number_of_response:1; }","duration":"101.542087ms","start":"2024-12-04T03:03:02.126552Z","end":"2024-12-04T03:03:02.228094Z","steps":["trace[10591732] 'process raft request'  (duration: 100.390448ms)"],"step_count":1}
{"level":"info","ts":"2024-12-04T03:03:02.519874Z","caller":"traceutil/trace.go:171","msg":"trace[1268895820] linearizableReadLoop","detail":"{readStateIndex:100240; appliedIndex:100239; }","duration":"101.600607ms","start":"2024-12-04T03:03:02.418251Z","end":"2024-12-04T03:03:02.519852Z","steps":["trace[1268895820] 'read index received'  (duration: 6.106504ms)","trace[1268895820] 'applied index is now lower than readState.Index'  (duration: 95.493356ms)"],"step_count":2}
{"level":"info","ts":"2024-12-04T03:03:02.519986Z","caller":"traceutil/trace.go:171","msg":"trace[193264633] transaction","detail":"{read_only:false; response_revision:82782; number_of_response:1; }","duration":"200.148858ms","start":"2024-12-04T03:03:02.319829Z","end":"2024-12-04T03:03:02.519978Z","steps":["trace[193264633] 'process raft request'  (duration: 104.551191ms)","trace[193264633] 'compare'  (duration: 95.356929ms)"],"step_count":2}
{"level":"warn","ts":"2024-12-04T03:03:02.520392Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"102.123743ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/masterleases/192.168.49.2\" ","response":"range_response_count:1 size:134"}
{"level":"info","ts":"2024-12-04T03:03:02.520434Z","caller":"traceutil/trace.go:171","msg":"trace[1522816794] range","detail":"{range_begin:/registry/masterleases/192.168.49.2; range_end:; response_count:1; response_revision:82782; }","duration":"102.179413ms","start":"2024-12-04T03:03:02.418244Z","end":"2024-12-04T03:03:02.520423Z","steps":["trace[1522816794] 'agreement among raft nodes before linearized reading'  (duration: 102.091167ms)"],"step_count":1}
{"level":"info","ts":"2024-12-04T03:03:02.721817Z","caller":"traceutil/trace.go:171","msg":"trace[1792632957] transaction","detail":"{read_only:false; response_revision:82784; number_of_response:1; }","duration":"103.09049ms","start":"2024-12-04T03:03:02.618693Z","end":"2024-12-04T03:03:02.721783Z","steps":["trace[1792632957] 'compare'  (duration: 95.039361ms)"],"step_count":1}
{"level":"info","ts":"2024-12-04T03:03:02.818696Z","caller":"traceutil/trace.go:171","msg":"trace[1452948283] transaction","detail":"{read_only:false; response_revision:82785; number_of_response:1; }","duration":"196.998555ms","start":"2024-12-04T03:03:02.621678Z","end":"2024-12-04T03:03:02.818677Z","steps":["trace[1452948283] 'process raft request'  (duration: 196.256578ms)"],"step_count":1}
{"level":"info","ts":"2024-12-04T03:03:03.217953Z","caller":"traceutil/trace.go:171","msg":"trace[260885125] transaction","detail":"{read_only:false; response_revision:82789; number_of_response:1; }","duration":"189.265859ms","start":"2024-12-04T03:03:03.028656Z","end":"2024-12-04T03:03:03.217922Z","steps":["trace[260885125] 'process raft request'  (duration: 189.095843ms)"],"step_count":1}
{"level":"info","ts":"2024-12-04T03:03:17.149421Z","caller":"fileutil/purge.go:96","msg":"purged","path":"/var/lib/minikube/etcd/member/snap/0000000000000008-000000000000c355.snap"}
{"level":"info","ts":"2024-12-04T03:12:48.172398Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":83315}
{"level":"info","ts":"2024-12-04T03:12:48.206721Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":83315,"took":"33.721566ms","hash":1584905222,"current-db-size-bytes":6287360,"current-db-size":"6.3 MB","current-db-size-in-use-bytes":2777088,"current-db-size-in-use":"2.8 MB"}
{"level":"info","ts":"2024-12-04T03:12:48.206825Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1584905222,"revision":83315,"compact-revision":82130}
{"level":"info","ts":"2024-12-04T03:16:27.225992Z","caller":"traceutil/trace.go:171","msg":"trace[479034783] transaction","detail":"{read_only:false; response_revision:83799; number_of_response:1; }","duration":"100.648051ms","start":"2024-12-04T03:16:27.125301Z","end":"2024-12-04T03:16:27.225949Z","steps":["trace[479034783] 'process raft request'  (duration: 100.415117ms)"],"step_count":1}
{"level":"info","ts":"2024-12-04T03:17:48.188370Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":83594}
{"level":"info","ts":"2024-12-04T03:17:48.196748Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":83594,"took":"7.654818ms","hash":1579357170,"current-db-size-bytes":6287360,"current-db-size":"6.3 MB","current-db-size-in-use-bytes":2338816,"current-db-size-in-use":"2.3 MB"}
{"level":"info","ts":"2024-12-04T03:17:48.196850Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1579357170,"revision":83594,"compact-revision":83315}
{"level":"info","ts":"2024-12-04T03:22:48.198105Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":83873}
{"level":"info","ts":"2024-12-04T03:22:48.203159Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":83873,"took":"4.634375ms","hash":1367659260,"current-db-size-bytes":6287360,"current-db-size":"6.3 MB","current-db-size-in-use-bytes":2334720,"current-db-size-in-use":"2.3 MB"}
{"level":"info","ts":"2024-12-04T03:22:48.203237Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1367659260,"revision":83873,"compact-revision":83594}
{"level":"info","ts":"2024-12-04T03:27:23.226301Z","caller":"traceutil/trace.go:171","msg":"trace[1315156018] transaction","detail":"{read_only:false; response_revision:84409; number_of_response:1; }","duration":"102.014436ms","start":"2024-12-04T03:27:23.124252Z","end":"2024-12-04T03:27:23.226266Z","steps":["trace[1315156018] 'process raft request'  (duration: 101.785681ms)"],"step_count":1}
{"level":"info","ts":"2024-12-04T03:27:48.210078Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":84153}
{"level":"info","ts":"2024-12-04T03:27:48.215282Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":84153,"took":"4.709469ms","hash":1884160300,"current-db-size-bytes":6287360,"current-db-size":"6.3 MB","current-db-size-in-use-bytes":2342912,"current-db-size-in-use":"2.3 MB"}
{"level":"info","ts":"2024-12-04T03:27:48.215374Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1884160300,"revision":84153,"compact-revision":83873}
{"level":"info","ts":"2024-12-04T03:31:57.550422Z","caller":"traceutil/trace.go:171","msg":"trace[1081268042] transaction","detail":"{read_only:false; response_revision:84665; number_of_response:1; }","duration":"143.068337ms","start":"2024-12-04T03:31:57.407333Z","end":"2024-12-04T03:31:57.550401Z","steps":["trace[1081268042] 'process raft request'  (duration: 142.692234ms)"],"step_count":1}
{"level":"info","ts":"2024-12-04T03:31:57.550392Z","caller":"traceutil/trace.go:171","msg":"trace[5902514] linearizableReadLoop","detail":"{readStateIndex:102479; appliedIndex:102478; }","duration":"126.151424ms","start":"2024-12-04T03:31:57.424047Z","end":"2024-12-04T03:31:57.550199Z","steps":["trace[5902514] 'read index received'  (duration: 126.018885ms)","trace[5902514] 'applied index is now lower than readState.Index'  (duration: 131.611µs)"],"step_count":2}
{"level":"warn","ts":"2024-12-04T03:31:57.551210Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"126.715973ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2024-12-04T03:31:57.551364Z","caller":"traceutil/trace.go:171","msg":"trace[1141177550] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:84665; }","duration":"127.331215ms","start":"2024-12-04T03:31:57.424020Z","end":"2024-12-04T03:31:57.551352Z","steps":["trace[1141177550] 'agreement among raft nodes before linearized reading'  (duration: 126.568165ms)"],"step_count":1}
{"level":"info","ts":"2024-12-04T03:32:48.228398Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":84432}
{"level":"info","ts":"2024-12-04T03:32:48.237718Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":84432,"took":"8.129814ms","hash":3865077689,"current-db-size-bytes":6287360,"current-db-size":"6.3 MB","current-db-size-in-use-bytes":2334720,"current-db-size-in-use":"2.3 MB"}
{"level":"info","ts":"2024-12-04T03:32:48.237820Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3865077689,"revision":84432,"compact-revision":84153}
{"level":"info","ts":"2024-12-04T03:37:48.239370Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":84712}
{"level":"info","ts":"2024-12-04T03:37:48.244037Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":84712,"took":"4.145104ms","hash":380927122,"current-db-size-bytes":6287360,"current-db-size":"6.3 MB","current-db-size-in-use-bytes":2326528,"current-db-size-in-use":"2.3 MB"}
{"level":"info","ts":"2024-12-04T03:37:48.244103Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":380927122,"revision":84712,"compact-revision":84432}
{"level":"info","ts":"2024-12-04T03:42:48.253157Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":84990}
{"level":"info","ts":"2024-12-04T03:42:48.257731Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":84990,"took":"4.224131ms","hash":1009463886,"current-db-size-bytes":6287360,"current-db-size":"6.3 MB","current-db-size-in-use-bytes":2318336,"current-db-size-in-use":"2.3 MB"}
{"level":"info","ts":"2024-12-04T03:42:48.257794Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1009463886,"revision":84990,"compact-revision":84712}


==> etcd [65c1bfdf4a46] <==
{"level":"warn","ts":"2024-12-17T04:48:41.273328Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"108.213919ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/minions\" limit:1 ","response":"range_response_count:0 size:6"}
{"level":"warn","ts":"2024-12-17T04:48:41.273385Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"195.729785ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2024-12-17T04:48:41.273401Z","caller":"traceutil/trace.go:171","msg":"trace[475487345] range","detail":"{range_begin:/registry/minions; range_end:; response_count:0; response_revision:85526; }","duration":"108.28577ms","start":"2024-12-17T04:48:41.165097Z","end":"2024-12-17T04:48:41.273383Z","steps":["trace[475487345] 'agreement among raft nodes before linearized reading'  (duration: 108.184808ms)"],"step_count":1}
{"level":"info","ts":"2024-12-17T04:48:41.273415Z","caller":"traceutil/trace.go:171","msg":"trace[1259407161] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:85526; }","duration":"195.756073ms","start":"2024-12-17T04:48:41.077649Z","end":"2024-12-17T04:48:41.273405Z","steps":["trace[1259407161] 'agreement among raft nodes before linearized reading'  (duration: 195.718325ms)"],"step_count":1}
{"level":"info","ts":"2024-12-17T04:48:41.273434Z","caller":"traceutil/trace.go:171","msg":"trace[475451959] transaction","detail":"{read_only:false; response_revision:85526; number_of_response:1; }","duration":"100.019287ms","start":"2024-12-17T04:48:41.173388Z","end":"2024-12-17T04:48:41.273407Z","steps":["trace[475451959] 'process raft request'  (duration: 99.629352ms)"],"step_count":1}
{"level":"warn","ts":"2024-12-17T04:48:41.273328Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"200.241684ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/endpointslices/default/kubernetes\" ","response":"range_response_count:1 size:476"}
{"level":"info","ts":"2024-12-17T04:48:41.273591Z","caller":"traceutil/trace.go:171","msg":"trace[475922682] range","detail":"{range_begin:/registry/endpointslices/default/kubernetes; range_end:; response_count:1; response_revision:85526; }","duration":"200.526729ms","start":"2024-12-17T04:48:41.073055Z","end":"2024-12-17T04:48:41.273582Z","steps":["trace[475922682] 'agreement among raft nodes before linearized reading'  (duration: 200.11639ms)"],"step_count":1}
{"level":"warn","ts":"2024-12-17T04:48:41.473733Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"107.548834ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128033961712352799 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/pods/default/olive-7d4998978f-rcrh4\" mod_revision:82939 > success:<request_put:<key:\"/registry/pods/default/olive-7d4998978f-rcrh4\" value_size:2978 >> failure:<request_range:<key:\"/registry/pods/default/olive-7d4998978f-rcrh4\" > >>","response":"size:18"}
{"level":"info","ts":"2024-12-17T04:48:41.473831Z","caller":"traceutil/trace.go:171","msg":"trace[1737521971] transaction","detail":"{read_only:false; response_revision:85528; number_of_response:1; }","duration":"199.81287ms","start":"2024-12-17T04:48:41.274004Z","end":"2024-12-17T04:48:41.473817Z","steps":["trace[1737521971] 'process raft request'  (duration: 92.110916ms)","trace[1737521971] 'compare'  (duration: 107.394912ms)"],"step_count":2}
{"level":"info","ts":"2024-12-17T04:48:41.475361Z","caller":"traceutil/trace.go:171","msg":"trace[1565471015] transaction","detail":"{read_only:false; response_revision:85529; number_of_response:1; }","duration":"106.910215ms","start":"2024-12-17T04:48:41.368428Z","end":"2024-12-17T04:48:41.475339Z","steps":["trace[1565471015] 'process raft request'  (duration: 106.732753ms)"],"step_count":1}
{"level":"info","ts":"2024-12-17T04:48:41.764283Z","caller":"traceutil/trace.go:171","msg":"trace[1371477107] transaction","detail":"{read_only:false; response_revision:85532; number_of_response:1; }","duration":"186.252162ms","start":"2024-12-17T04:48:41.578000Z","end":"2024-12-17T04:48:41.764252Z","steps":["trace[1371477107] 'process raft request'  (duration: 185.927351ms)"],"step_count":1}
{"level":"info","ts":"2024-12-17T04:48:42.075310Z","caller":"traceutil/trace.go:171","msg":"trace[1183458326] transaction","detail":"{read_only:false; response_revision:85536; number_of_response:1; }","duration":"100.623398ms","start":"2024-12-17T04:48:41.974654Z","end":"2024-12-17T04:48:42.075277Z","steps":["trace[1183458326] 'process raft request'  (duration: 97.559057ms)"],"step_count":1}
{"level":"info","ts":"2024-12-17T04:48:42.575050Z","caller":"traceutil/trace.go:171","msg":"trace[1470717885] transaction","detail":"{read_only:false; response_revision:85542; number_of_response:1; }","duration":"111.771748ms","start":"2024-12-17T04:48:42.463209Z","end":"2024-12-17T04:48:42.574981Z","steps":["trace[1470717885] 'process raft request'  (duration: 12.159247ms)","trace[1470717885] 'compare'  (duration: 99.282581ms)"],"step_count":2}
{"level":"info","ts":"2024-12-17T04:48:42.778999Z","caller":"traceutil/trace.go:171","msg":"trace[676147924] transaction","detail":"{read_only:false; response_revision:85543; number_of_response:1; }","duration":"105.836445ms","start":"2024-12-17T04:48:42.673134Z","end":"2024-12-17T04:48:42.778970Z","steps":["trace[676147924] 'process raft request'  (duration: 91.591014ms)","trace[676147924] 'compare'  (duration: 14.065908ms)"],"step_count":2}
{"level":"info","ts":"2024-12-17T04:48:42.785554Z","caller":"traceutil/trace.go:171","msg":"trace[1511856879] transaction","detail":"{read_only:false; response_revision:85544; number_of_response:1; }","duration":"107.114982ms","start":"2024-12-17T04:48:42.678420Z","end":"2024-12-17T04:48:42.785535Z","steps":["trace[1511856879] 'process raft request'  (duration: 107.007549ms)"],"step_count":1}
{"level":"warn","ts":"2024-12-17T04:48:43.271576Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"107.378246ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128033961712352833 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/pods/kube-system/kube-apiserver-minikube\" mod_revision:85417 > success:<request_put:<key:\"/registry/pods/kube-system/kube-apiserver-minikube\" value_size:7477 >> failure:<request_range:<key:\"/registry/pods/kube-system/kube-apiserver-minikube\" > >>","response":"size:18"}
{"level":"info","ts":"2024-12-17T04:48:43.271875Z","caller":"traceutil/trace.go:171","msg":"trace[590505657] transaction","detail":"{read_only:false; response_revision:85548; number_of_response:1; }","duration":"208.246223ms","start":"2024-12-17T04:48:43.063596Z","end":"2024-12-17T04:48:43.271843Z","steps":["trace[590505657] 'process raft request'  (duration: 100.512491ms)","trace[590505657] 'compare'  (duration: 107.225894ms)"],"step_count":2}
{"level":"info","ts":"2024-12-17T04:48:43.271927Z","caller":"traceutil/trace.go:171","msg":"trace[1121517321] linearizableReadLoop","detail":"{readStateIndex:103529; appliedIndex:103528; }","duration":"201.392409ms","start":"2024-12-17T04:48:43.070510Z","end":"2024-12-17T04:48:43.271902Z","steps":["trace[1121517321] 'read index received'  (duration: 93.617241ms)","trace[1121517321] 'applied index is now lower than readState.Index'  (duration: 107.774373ms)"],"step_count":2}
{"level":"info","ts":"2024-12-17T04:48:43.272017Z","caller":"traceutil/trace.go:171","msg":"trace[1702239379] transaction","detail":"{read_only:false; response_revision:85549; number_of_response:1; }","duration":"200.733317ms","start":"2024-12-17T04:48:43.071272Z","end":"2024-12-17T04:48:43.272005Z","steps":["trace[1702239379] 'process raft request'  (duration: 200.492641ms)"],"step_count":1}
{"level":"warn","ts":"2024-12-17T04:48:43.272227Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"201.706446ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2024-12-17T04:48:43.272263Z","caller":"traceutil/trace.go:171","msg":"trace[47605561] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:85549; }","duration":"201.749558ms","start":"2024-12-17T04:48:43.070503Z","end":"2024-12-17T04:48:43.272253Z","steps":["trace[47605561] 'agreement among raft nodes before linearized reading'  (duration: 201.670922ms)"],"step_count":1}
{"level":"warn","ts":"2024-12-17T04:48:43.862681Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"186.309892ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128033961712352845 > lease_revoke:<id:70cc938f9ec4961b>","response":"size:30"}
{"level":"info","ts":"2024-12-17T04:48:43.878493Z","caller":"traceutil/trace.go:171","msg":"trace[561243720] transaction","detail":"{read_only:false; response_revision:85553; number_of_response:1; }","duration":"211.259662ms","start":"2024-12-17T04:48:43.667208Z","end":"2024-12-17T04:48:43.878468Z","steps":["trace[561243720] 'process raft request'  (duration: 211.05832ms)"],"step_count":1}
{"level":"info","ts":"2024-12-17T04:48:45.770242Z","caller":"traceutil/trace.go:171","msg":"trace[780318162] transaction","detail":"{read_only:false; response_revision:85576; number_of_response:1; }","duration":"100.115453ms","start":"2024-12-17T04:48:45.670067Z","end":"2024-12-17T04:48:45.770183Z","steps":["trace[780318162] 'marshal mvccpb.KeyValue' {req_type:put; key:/registry/events/trouble-2/teal-color-555b59785-52kh5.1811dd8bf98716c5; req_size:763; } (duration: 93.552859ms)"],"step_count":1}
{"level":"info","ts":"2024-12-17T04:48:45.963545Z","caller":"traceutil/trace.go:171","msg":"trace[1999228987] transaction","detail":"{read_only:false; response_revision:85577; number_of_response:1; }","duration":"187.716064ms","start":"2024-12-17T04:48:45.775801Z","end":"2024-12-17T04:48:45.963517Z","steps":["trace[1999228987] 'process raft request'  (duration: 103.412847ms)","trace[1999228987] 'compare'  (duration: 83.99381ms)"],"step_count":2}
{"level":"info","ts":"2024-12-17T04:48:46.276880Z","caller":"traceutil/trace.go:171","msg":"trace[1666202882] transaction","detail":"{read_only:false; response_revision:85581; number_of_response:1; }","duration":"101.250048ms","start":"2024-12-17T04:48:46.175607Z","end":"2024-12-17T04:48:46.276857Z","steps":["trace[1666202882] 'process raft request'  (duration: 101.021467ms)"],"step_count":1}
{"level":"info","ts":"2024-12-17T04:48:46.675688Z","caller":"traceutil/trace.go:171","msg":"trace[1208691417] linearizableReadLoop","detail":"{readStateIndex:103568; appliedIndex:103567; }","duration":"101.275699ms","start":"2024-12-17T04:48:46.574379Z","end":"2024-12-17T04:48:46.675654Z","steps":["trace[1208691417] 'read index received'  (duration: 91.094451ms)","trace[1208691417] 'applied index is now lower than readState.Index'  (duration: 10.179974ms)"],"step_count":2}
{"level":"warn","ts":"2024-12-17T04:48:46.675859Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"101.450264ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2024-12-17T04:48:46.675933Z","caller":"traceutil/trace.go:171","msg":"trace[2047620361] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:85584; }","duration":"101.545635ms","start":"2024-12-17T04:48:46.574375Z","end":"2024-12-17T04:48:46.675921Z","steps":["trace[2047620361] 'agreement among raft nodes before linearized reading'  (duration: 101.390715ms)"],"step_count":1}
{"level":"info","ts":"2024-12-17T04:48:46.676247Z","caller":"traceutil/trace.go:171","msg":"trace[737093955] transaction","detail":"{read_only:false; response_revision:85584; number_of_response:1; }","duration":"103.261813ms","start":"2024-12-17T04:48:46.572975Z","end":"2024-12-17T04:48:46.676237Z","steps":["trace[737093955] 'process raft request'  (duration: 92.53465ms)"],"step_count":1}
{"level":"info","ts":"2024-12-17T04:48:46.971599Z","caller":"traceutil/trace.go:171","msg":"trace[877398069] transaction","detail":"{read_only:false; response_revision:85586; number_of_response:1; }","duration":"108.073668ms","start":"2024-12-17T04:48:46.863498Z","end":"2024-12-17T04:48:46.971571Z","steps":["trace[877398069] 'process raft request'  (duration: 107.951103ms)"],"step_count":1}
{"level":"warn","ts":"2024-12-17T04:48:47.279425Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"211.247896ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 serializable:true keys_only:true ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2024-12-17T04:48:47.279511Z","caller":"traceutil/trace.go:171","msg":"trace[1405843977] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:85587; }","duration":"211.358007ms","start":"2024-12-17T04:48:47.068134Z","end":"2024-12-17T04:48:47.279492Z","steps":["trace[1405843977] 'range keys from in-memory index tree'  (duration: 211.225917ms)"],"step_count":1}
{"level":"warn","ts":"2024-12-17T04:48:47.362499Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"171.22046ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128033961712352892 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/events/trouble-3/fuchsia-color-7f46c97957-59r62.1811dd8c76c36263\" mod_revision:0 > success:<request_put:<key:\"/registry/events/trouble-3/fuchsia-color-7f46c97957-59r62.1811dd8c76c36263\" value_size:655 lease:8128033961712352380 >> failure:<>>","response":"size:18"}
{"level":"info","ts":"2024-12-17T04:48:47.363156Z","caller":"traceutil/trace.go:171","msg":"trace[163424347] transaction","detail":"{read_only:false; response_revision:85588; number_of_response:1; }","duration":"284.318618ms","start":"2024-12-17T04:48:47.078809Z","end":"2024-12-17T04:48:47.363128Z","steps":["trace[163424347] 'process raft request'  (duration: 112.347129ms)","trace[163424347] 'compare'  (duration: 88.024999ms)"],"step_count":2}
{"level":"warn","ts":"2024-12-17T04:48:48.182788Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"109.739151ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2024-12-17T04:48:48.182883Z","caller":"traceutil/trace.go:171","msg":"trace[1095228232] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:85591; }","duration":"109.861272ms","start":"2024-12-17T04:48:48.073004Z","end":"2024-12-17T04:48:48.182865Z","steps":["trace[1095228232] 'range keys from in-memory index tree'  (duration: 109.664108ms)"],"step_count":1}
{"level":"info","ts":"2024-12-17T04:48:49.462625Z","caller":"traceutil/trace.go:171","msg":"trace[2030114071] transaction","detail":"{read_only:false; response_revision:85596; number_of_response:1; }","duration":"282.028873ms","start":"2024-12-17T04:48:49.180563Z","end":"2024-12-17T04:48:49.462592Z","steps":["trace[2030114071] 'process raft request'  (duration: 281.83061ms)"],"step_count":1}
{"level":"info","ts":"2024-12-17T04:48:49.865160Z","caller":"traceutil/trace.go:171","msg":"trace[579346522] transaction","detail":"{read_only:false; response_revision:85599; number_of_response:1; }","duration":"101.773691ms","start":"2024-12-17T04:48:49.763365Z","end":"2024-12-17T04:48:49.865138Z","steps":["trace[579346522] 'process raft request'  (duration: 101.66251ms)"],"step_count":1}
{"level":"info","ts":"2024-12-17T04:48:49.876306Z","caller":"traceutil/trace.go:171","msg":"trace[796435276] transaction","detail":"{read_only:false; response_revision:85600; number_of_response:1; }","duration":"109.0696ms","start":"2024-12-17T04:48:49.767203Z","end":"2024-12-17T04:48:49.876273Z","steps":["trace[796435276] 'process raft request'  (duration: 105.581346ms)"],"step_count":1}
{"level":"info","ts":"2024-12-17T04:48:50.271608Z","caller":"traceutil/trace.go:171","msg":"trace[303464151] transaction","detail":"{read_only:false; response_revision:85606; number_of_response:1; }","duration":"103.324473ms","start":"2024-12-17T04:48:50.168252Z","end":"2024-12-17T04:48:50.271576Z","steps":["trace[303464151] 'process raft request'  (duration: 103.159541ms)"],"step_count":1}
{"level":"info","ts":"2024-12-17T04:48:50.472658Z","caller":"traceutil/trace.go:171","msg":"trace[135048690] transaction","detail":"{read_only:false; response_revision:85609; number_of_response:1; }","duration":"101.978387ms","start":"2024-12-17T04:48:50.370652Z","end":"2024-12-17T04:48:50.472630Z","steps":["trace[135048690] 'process raft request'  (duration: 92.018964ms)"],"step_count":1}
{"level":"info","ts":"2024-12-17T04:58:27.308372Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":86011}
{"level":"info","ts":"2024-12-17T04:58:27.339809Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":86011,"took":"30.650659ms","hash":3156114433,"current-db-size-bytes":6287360,"current-db-size":"6.3 MB","current-db-size-in-use-bytes":3092480,"current-db-size-in-use":"3.1 MB"}
{"level":"info","ts":"2024-12-17T04:58:27.339933Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3156114433,"revision":86011,"compact-revision":84990}
{"level":"info","ts":"2024-12-17T05:03:27.318045Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":86291}
{"level":"info","ts":"2024-12-17T05:03:27.338277Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":86291,"took":"19.647452ms","hash":2121917378,"current-db-size-bytes":6287360,"current-db-size":"6.3 MB","current-db-size-in-use-bytes":2240512,"current-db-size-in-use":"2.2 MB"}
{"level":"info","ts":"2024-12-17T05:03:27.338369Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2121917378,"revision":86291,"compact-revision":86011}
{"level":"info","ts":"2024-12-17T05:08:27.331727Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":86569}
{"level":"info","ts":"2024-12-17T05:08:27.358489Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":86569,"took":"26.317836ms","hash":3543872541,"current-db-size-bytes":6287360,"current-db-size":"6.3 MB","current-db-size-in-use-bytes":2240512,"current-db-size-in-use":"2.2 MB"}
{"level":"info","ts":"2024-12-17T05:08:27.358621Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3543872541,"revision":86569,"compact-revision":86291}
{"level":"info","ts":"2024-12-17T05:13:27.350376Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":86849}
{"level":"info","ts":"2024-12-17T05:13:27.379845Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":86849,"took":"28.842664ms","hash":2255968753,"current-db-size-bytes":6287360,"current-db-size":"6.3 MB","current-db-size-in-use-bytes":2252800,"current-db-size-in-use":"2.3 MB"}
{"level":"info","ts":"2024-12-17T05:13:27.380003Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2255968753,"revision":86849,"compact-revision":86569}
{"level":"info","ts":"2024-12-17T05:18:27.361013Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":87128}
{"level":"info","ts":"2024-12-17T05:18:27.387435Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":87128,"took":"25.674843ms","hash":3390437360,"current-db-size-bytes":6287360,"current-db-size":"6.3 MB","current-db-size-in-use-bytes":2252800,"current-db-size-in-use":"2.3 MB"}
{"level":"info","ts":"2024-12-17T05:18:27.387549Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3390437360,"revision":87128,"compact-revision":86849}
{"level":"info","ts":"2024-12-17T05:23:27.376498Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":87408}
{"level":"info","ts":"2024-12-17T05:23:27.405044Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":87408,"took":"27.594488ms","hash":1614520354,"current-db-size-bytes":6287360,"current-db-size":"6.3 MB","current-db-size-in-use-bytes":2248704,"current-db-size-in-use":"2.2 MB"}
{"level":"info","ts":"2024-12-17T05:23:27.405177Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1614520354,"revision":87408,"compact-revision":87128}


==> kernel <==
 05:26:34 up 39 min,  0 users,  load average: 0.57, 0.37, 0.30
Linux minikube 5.10.102.1-microsoft-standard-WSL2 #1 SMP Wed Mar 2 00:30:59 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.4 LTS"


==> kube-apiserver [18f2bb72d6d2] <==
I1217 04:48:29.387049       1 handler.go:286] Adding GroupVersion apiregistration.k8s.io v1 to ResourceManager
W1217 04:48:29.387111       1 genericapiserver.go:765] Skipping API apiregistration.k8s.io/v1beta1 because it has no resources.
I1217 04:48:30.281756       1 secure_serving.go:213] Serving securely on [::]:8443
I1217 04:48:30.282073       1 remote_available_controller.go:411] Starting RemoteAvailability controller
I1217 04:48:30.282124       1 cache.go:32] Waiting for caches to sync for RemoteAvailability controller
I1217 04:48:30.282321       1 controller.go:78] Starting OpenAPI AggregationController
I1217 04:48:30.282399       1 controller.go:80] Starting OpenAPI V3 AggregationController
I1217 04:48:30.282477       1 controller.go:119] Starting legacy_token_tracking_controller
I1217 04:48:30.282492       1 shared_informer.go:313] Waiting for caches to sync for configmaps
I1217 04:48:30.283638       1 local_available_controller.go:156] Starting LocalAvailability controller
I1217 04:48:30.283678       1 cache.go:32] Waiting for caches to sync for LocalAvailability controller
I1217 04:48:30.283708       1 aggregator.go:169] waiting for initial CRD sync...
I1217 04:48:30.284597       1 system_namespaces_controller.go:66] Starting system namespaces controller
I1217 04:48:30.284779       1 dynamic_serving_content.go:135] "Starting controller" name="serving-cert::/var/lib/minikube/certs/apiserver.crt::/var/lib/minikube/certs/apiserver.key"
I1217 04:48:30.285140       1 dynamic_cafile_content.go:160] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I1217 04:48:30.285219       1 dynamic_cafile_content.go:160] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I1217 04:48:30.285370       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I1217 04:48:30.290463       1 dynamic_serving_content.go:135] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I1217 04:48:30.290499       1 customresource_discovery_controller.go:292] Starting DiscoveryController
I1217 04:48:30.291986       1 cluster_authentication_trust_controller.go:443] Starting cluster_authentication_trust_controller controller
I1217 04:48:30.290531       1 controller.go:142] Starting OpenAPI controller
I1217 04:48:30.290552       1 controller.go:90] Starting OpenAPI V3 controller
I1217 04:48:30.290568       1 naming_controller.go:294] Starting NamingConditionController
I1217 04:48:30.290609       1 establishing_controller.go:81] Starting EstablishingController
I1217 04:48:30.290641       1 nonstructuralschema_controller.go:195] Starting NonStructuralSchemaConditionController
I1217 04:48:30.290651       1 apiapproval_controller.go:189] Starting KubernetesAPIApprovalPolicyConformantConditionController
I1217 04:48:30.290660       1 crd_finalizer.go:269] Starting CRDFinalizer
I1217 04:48:30.290778       1 apiservice_controller.go:100] Starting APIServiceRegistrationController
I1217 04:48:30.292147       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I1217 04:48:30.291670       1 gc_controller.go:78] Starting apiserver lease garbage collector
I1217 04:48:30.292053       1 shared_informer.go:313] Waiting for caches to sync for cluster_authentication_trust_controller
I1217 04:48:30.292246       1 dynamic_cafile_content.go:160] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I1217 04:48:30.292360       1 dynamic_cafile_content.go:160] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I1217 04:48:30.296074       1 crdregistration_controller.go:114] Starting crd-autoregister controller
I1217 04:48:30.296150       1 shared_informer.go:313] Waiting for caches to sync for crd-autoregister
I1217 04:48:30.297100       1 apf_controller.go:377] Starting API Priority and Fairness config controller
I1217 04:48:30.377421       1 shared_informer.go:320] Caches are synced for *generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]
I1217 04:48:30.377505       1 policy_source.go:224] refreshing policies
I1217 04:48:30.462619       1 cache.go:39] Caches are synced for RemoteAvailability controller
I1217 04:48:30.462759       1 apf_controller.go:382] Running API Priority and Fairness config worker
I1217 04:48:30.462766       1 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
I1217 04:48:30.463399       1 shared_informer.go:320] Caches are synced for configmaps
I1217 04:48:30.463446       1 shared_informer.go:320] Caches are synced for crd-autoregister
I1217 04:48:30.463468       1 shared_informer.go:320] Caches are synced for cluster_authentication_trust_controller
I1217 04:48:30.463485       1 aggregator.go:171] initial CRD sync complete...
I1217 04:48:30.463493       1 autoregister_controller.go:144] Starting autoregister controller
I1217 04:48:30.463499       1 cache.go:32] Waiting for caches to sync for autoregister controller
I1217 04:48:30.463505       1 cache.go:39] Caches are synced for autoregister controller
I1217 04:48:30.463514       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I1217 04:48:30.463872       1 cache.go:39] Caches are synced for LocalAvailability controller
I1217 04:48:30.469672       1 handler_discovery.go:450] Starting ResourceDiscoveryManager
I1217 04:48:30.469771       1 shared_informer.go:320] Caches are synced for node_authorizer
I1217 04:48:30.475593       1 controller.go:615] quota admission added evaluator for: leases.coordination.k8s.io
E1217 04:48:30.562918       1 controller.go:97] Error removing old endpoints from kubernetes service: no API server IP addresses were listed in storage, refusing to erase all endpoints for the kubernetes Service
I1217 04:48:31.465336       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
E1217 04:48:41.068203       1 controller.go:195] "Failed to update lease" err="Operation cannot be fulfilled on leases.coordination.k8s.io \"apiserver-eqt674mfxb4j56mrjjkoe7b7ii\": StorageError: invalid object, Code: 4, Key: /registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii, ResourceVersion: 0, AdditionalErrorMsg: Precondition failed: UID in precondition: f79e99d0-4d81-4166-b3a9-09146e149df5, UID in object meta: "
I1217 04:48:42.265933       1 controller.go:615] quota admission added evaluator for: deployments.apps
I1217 04:48:57.873460       1 controller.go:615] quota admission added evaluator for: endpoints
I1217 04:48:57.873461       1 controller.go:615] quota admission added evaluator for: endpoints
I1217 04:48:57.982928       1 controller.go:615] quota admission added evaluator for: endpointslices.discovery.k8s.io


==> kube-apiserver [731a56c40a09] <==
W1204 03:02:48.979006       1 genericapiserver.go:765] Skipping API events.k8s.io/v1beta1 because it has no resources.
I1204 03:02:48.984650       1 handler.go:286] Adding GroupVersion apiregistration.k8s.io v1 to ResourceManager
W1204 03:02:48.984678       1 genericapiserver.go:765] Skipping API apiregistration.k8s.io/v1beta1 because it has no resources.
I1204 03:02:49.321806       1 dynamic_cafile_content.go:160] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I1204 03:02:49.321923       1 dynamic_cafile_content.go:160] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I1204 03:02:49.321935       1 secure_serving.go:213] Serving securely on [::]:8443
I1204 03:02:49.321949       1 dynamic_serving_content.go:135] "Starting controller" name="serving-cert::/var/lib/minikube/certs/apiserver.crt::/var/lib/minikube/certs/apiserver.key"
I1204 03:02:49.322035       1 local_available_controller.go:156] Starting LocalAvailability controller
I1204 03:02:49.322058       1 cache.go:32] Waiting for caches to sync for LocalAvailability controller
I1204 03:02:49.322075       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I1204 03:02:49.322174       1 controller.go:80] Starting OpenAPI V3 AggregationController
I1204 03:02:49.322332       1 dynamic_serving_content.go:135] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I1204 03:02:49.322378       1 aggregator.go:169] waiting for initial CRD sync...
I1204 03:02:49.322393       1 crdregistration_controller.go:114] Starting crd-autoregister controller
I1204 03:02:49.322397       1 shared_informer.go:313] Waiting for caches to sync for crd-autoregister
I1204 03:02:49.322344       1 dynamic_cafile_content.go:160] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I1204 03:02:49.322434       1 remote_available_controller.go:411] Starting RemoteAvailability controller
I1204 03:02:49.322441       1 cache.go:32] Waiting for caches to sync for RemoteAvailability controller
I1204 03:02:49.322537       1 customresource_discovery_controller.go:292] Starting DiscoveryController
I1204 03:02:49.322569       1 controller.go:142] Starting OpenAPI controller
I1204 03:02:49.322606       1 controller.go:90] Starting OpenAPI V3 controller
I1204 03:02:49.322616       1 naming_controller.go:294] Starting NamingConditionController
I1204 03:02:49.322625       1 establishing_controller.go:81] Starting EstablishingController
I1204 03:02:49.322632       1 dynamic_cafile_content.go:160] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I1204 03:02:49.322654       1 nonstructuralschema_controller.go:195] Starting NonStructuralSchemaConditionController
I1204 03:02:49.322664       1 apiapproval_controller.go:189] Starting KubernetesAPIApprovalPolicyConformantConditionController
I1204 03:02:49.322673       1 crd_finalizer.go:269] Starting CRDFinalizer
I1204 03:02:49.323745       1 system_namespaces_controller.go:66] Starting system namespaces controller
I1204 03:02:49.323818       1 controller.go:119] Starting legacy_token_tracking_controller
I1204 03:02:49.323822       1 shared_informer.go:313] Waiting for caches to sync for configmaps
I1204 03:02:49.325064       1 controller.go:78] Starting OpenAPI AggregationController
I1204 03:02:49.325213       1 gc_controller.go:78] Starting apiserver lease garbage collector
I1204 03:02:49.325212       1 cluster_authentication_trust_controller.go:443] Starting cluster_authentication_trust_controller controller
I1204 03:02:49.325261       1 apiservice_controller.go:100] Starting APIServiceRegistrationController
I1204 03:02:49.325287       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I1204 03:02:49.325305       1 apf_controller.go:377] Starting API Priority and Fairness config controller
I1204 03:02:49.325265       1 shared_informer.go:313] Waiting for caches to sync for cluster_authentication_trust_controller
I1204 03:02:49.422433       1 shared_informer.go:320] Caches are synced for crd-autoregister
I1204 03:02:49.422517       1 aggregator.go:171] initial CRD sync complete...
I1204 03:02:49.422525       1 autoregister_controller.go:144] Starting autoregister controller
I1204 03:02:49.422530       1 cache.go:32] Waiting for caches to sync for autoregister controller
I1204 03:02:49.422601       1 cache.go:39] Caches are synced for autoregister controller
I1204 03:02:49.423841       1 shared_informer.go:320] Caches are synced for configmaps
I1204 03:02:49.426086       1 shared_informer.go:320] Caches are synced for cluster_authentication_trust_controller
I1204 03:02:49.426126       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I1204 03:02:49.426135       1 apf_controller.go:382] Running API Priority and Fairness config worker
I1204 03:02:49.426246       1 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
I1204 03:02:49.426366       1 shared_informer.go:320] Caches are synced for *generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]
I1204 03:02:49.426448       1 policy_source.go:224] refreshing policies
I1204 03:02:49.427128       1 controller.go:615] quota admission added evaluator for: leases.coordination.k8s.io
I1204 03:02:49.432225       1 handler_discovery.go:450] Starting ResourceDiscoveryManager
I1204 03:02:49.436021       1 shared_informer.go:320] Caches are synced for node_authorizer
I1204 03:02:49.522146       1 cache.go:39] Caches are synced for LocalAvailability controller
I1204 03:02:49.522803       1 cache.go:39] Caches are synced for RemoteAvailability controller
E1204 03:02:49.537934       1 controller.go:97] Error removing old endpoints from kubernetes service: no API server IP addresses were listed in storage, refusing to erase all endpoints for the kubernetes Service
I1204 03:02:50.328347       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I1204 03:02:52.526971       1 controller.go:615] quota admission added evaluator for: deployments.apps
I1204 03:02:55.822834       1 controller.go:615] quota admission added evaluator for: endpoints
I1204 03:02:55.922958       1 controller.go:615] quota admission added evaluator for: endpointslices.discovery.k8s.io
E1204 03:02:59.720317       1 controller.go:195] "Failed to update lease" err="Operation cannot be fulfilled on leases.coordination.k8s.io \"apiserver-eqt674mfxb4j56mrjjkoe7b7ii\": StorageError: invalid object, Code: 4, Key: /registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii, ResourceVersion: 0, AdditionalErrorMsg: Precondition failed: UID in precondition: e2a7323a-60c3-4d13-a99a-085f137f00f4, UID in object meta: "


==> kube-controller-manager [52700b42ce1c] <==
I1217 04:48:25.985210       1 serving.go:386] Generated self-signed cert in-memory
I1217 04:48:29.066207       1 controllermanager.go:197] "Starting" version="v1.31.0"
I1217 04:48:29.066289       1 controllermanager.go:199] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1217 04:48:29.074029       1 dynamic_cafile_content.go:160] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I1217 04:48:29.074221       1 secure_serving.go:213] Serving securely on 127.0.0.1:10257
I1217 04:48:29.074333       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I1217 04:48:29.074549       1 dynamic_cafile_content.go:160] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
E1217 04:48:40.467898       1 controllermanager.go:242] "Error building controller context" err="failed to wait for apiserver being healthy: timed out waiting for the condition: failed to get apiserver /healthz status: an error on the server (\"[+]ping ok\\n[+]log ok\\n[+]etcd ok\\n[+]poststarthook/start-apiserver-admission-initializer ok\\n[+]poststarthook/generic-apiserver-start-informers ok\\n[+]poststarthook/priority-and-fairness-config-consumer ok\\n[+]poststarthook/priority-and-fairness-filter ok\\n[+]poststarthook/storage-object-count-tracker-hook ok\\n[+]poststarthook/start-apiextensions-informers ok\\n[+]poststarthook/start-apiextensions-controllers ok\\n[+]poststarthook/crd-informer-synced ok\\n[+]poststarthook/start-system-namespaces-controller ok\\n[+]poststarthook/start-cluster-authentication-info-controller ok\\n[+]poststarthook/start-kube-apiserver-identity-lease-controller ok\\n[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok\\n[+]poststarthook/start-legacy-token-tracking-controller ok\\n[+]poststarthook/start-service-ip-repair-controllers ok\\n[-]poststarthook/rbac/bootstrap-roles failed: reason withheld\\n[+]poststarthook/scheduling/bootstrap-system-priority-classes ok\\n[+]poststarthook/priority-and-fairness-config-producer ok\\n[+]poststarthook/bootstrap-controller ok\\n[+]poststarthook/aggregator-reload-proxy-client-cert ok\\n[+]poststarthook/start-kube-aggregator-informers ok\\n[+]poststarthook/apiservice-status-local-available-controller ok\\n[+]poststarthook/apiservice-status-remote-available-controller ok\\n[+]poststarthook/apiservice-registration-controller ok\\n[+]poststarthook/apiservice-discovery-controller ok\\n[+]poststarthook/kube-apiserver-autoregistration ok\\n[+]autoregister-completion ok\\n[+]poststarthook/apiservice-openapi-controller ok\\n[+]poststarthook/apiservice-openapiv3-controller ok\\nhealthz check failed\") has prevented the request from succeeding"


==> kube-controller-manager [5cf207d4db21] <==
I1217 04:48:57.564434       1 shared_informer.go:320] Caches are synced for GC
I1217 04:48:57.564587       1 shared_informer.go:320] Caches are synced for endpoint
I1217 04:48:57.564617       1 shared_informer.go:320] Caches are synced for PVC protection
I1217 04:48:57.564637       1 shared_informer.go:320] Caches are synced for ReplicationController
I1217 04:48:57.564659       1 shared_informer.go:320] Caches are synced for ephemeral
I1217 04:48:57.564707       1 shared_informer.go:320] Caches are synced for job
I1217 04:48:57.564763       1 shared_informer.go:320] Caches are synced for persistent volume
I1217 04:48:57.567215       1 shared_informer.go:320] Caches are synced for legacy-service-account-token-cleaner
I1217 04:48:57.568218       1 shared_informer.go:320] Caches are synced for cronjob
I1217 04:48:57.568265       1 shared_informer.go:320] Caches are synced for attach detach
I1217 04:48:57.568319       1 shared_informer.go:320] Caches are synced for crt configmap
I1217 04:48:57.570826       1 shared_informer.go:320] Caches are synced for expand
I1217 04:48:57.571984       1 shared_informer.go:320] Caches are synced for namespace
I1217 04:48:57.572287       1 shared_informer.go:320] Caches are synced for endpoint_slice
I1217 04:48:57.572342       1 shared_informer.go:320] Caches are synced for ClusterRoleAggregator
I1217 04:48:57.575204       1 shared_informer.go:320] Caches are synced for validatingadmissionpolicy-status
I1217 04:48:57.662514       1 shared_informer.go:320] Caches are synced for stateful set
I1217 04:48:57.690552       1 shared_informer.go:320] Caches are synced for daemon sets
I1217 04:48:57.694019       1 shared_informer.go:320] Caches are synced for taint
I1217 04:48:57.694282       1 node_lifecycle_controller.go:1232] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I1217 04:48:57.694407       1 node_lifecycle_controller.go:884] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I1217 04:48:57.694531       1 node_lifecycle_controller.go:1078] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I1217 04:48:57.773905       1 shared_informer.go:320] Caches are synced for resource quota
I1217 04:48:57.782847       1 shared_informer.go:320] Caches are synced for resource quota
I1217 04:48:57.863188       1 shared_informer.go:320] Caches are synced for deployment
I1217 04:48:57.863248       1 shared_informer.go:320] Caches are synced for ReplicaSet
I1217 04:48:57.863253       1 shared_informer.go:320] Caches are synced for disruption
I1217 04:48:57.863448       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/aqua-69c54df8b9" duration="118.849µs"
I1217 04:48:57.863448       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/hello-hello-648574587d" duration="54.375µs"
I1217 04:48:57.863505       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/maroon-6f56fdcfd9" duration="30.046µs"
I1217 04:48:57.863535       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-7d66457db5" duration="22.366µs"
I1217 04:48:57.863568       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/pod-info-app-65f569fd44" duration="25.539µs"
I1217 04:48:57.863618       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="ingress-nginx/aqua-69c54df8b9" duration="41.671µs"
I1217 04:48:57.863696       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-7d56585cd5" duration="59.334µs"
I1217 04:48:57.863697       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/olive-7d4998978f" duration="156.111µs"
I1217 04:48:57.863738       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-bc57996ff" duration="24.093µs"
I1217 04:48:57.863765       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="ingress-nginx/maroon-6f56fdcfd9" duration="31.966µs"
I1217 04:48:57.863777       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="ingress-nginx/olive-7d4998978f" duration="24.891µs"
I1217 04:48:57.863808       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="trouble-1/lime-color-6bdd87b769" duration="20.901µs"
I1217 04:48:57.863815       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="32.308µs"
I1217 04:48:57.863835       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="trouble-2/navy-color-7596c47445" duration="15.597µs"
I1217 04:48:57.863842       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="trouble-2/teal-color-555b59785" duration="18.11µs"
I1217 04:48:57.863854       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="trouble-3/fuchsia-color-7f46c97957" duration="11.834µs"
I1217 04:48:57.863849       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="trouble-1/purple-color-6994fb8f65" duration="27.074µs"
I1217 04:48:57.863864       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="trouble-3/maroon-color-cb6d8fb5" duration="13.075µs"
I1217 04:48:58.188633       1 shared_informer.go:320] Caches are synced for garbage collector
I1217 04:48:58.189178       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/my-nginx-7bc4c7bc9c" duration="325.678569ms"
I1217 04:48:58.189338       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/my-nginx-7bc4c7bc9c" duration="63.551µs"
I1217 04:48:58.191451       1 shared_informer.go:320] Caches are synced for garbage collector
I1217 04:48:58.191490       1 garbagecollector.go:157] "All resource monitors have synced. Proceeding to collect garbage" logger="garbage-collector-controller"
I1217 04:48:58.276178       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/my-nginx-7bc4c7bc9c" duration="68.316µs"
I1217 04:49:06.512142       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/my-nginx-7bc4c7bc9c" duration="11.209628ms"
I1217 04:49:06.512261       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/my-nginx-7bc4c7bc9c" duration="77.945µs"
I1217 04:53:38.226723       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1217 04:58:45.183965       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1217 05:03:51.447571       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1217 05:08:57.154679       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1217 05:14:02.514627       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1217 05:19:09.420366       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1217 05:24:14.231649       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"


==> kube-proxy [2a1fd6209712] <==
E1204 03:03:01.917777       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
E1204 03:03:01.918386       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
I1204 03:03:02.529637       1 server_linux.go:66] "Using iptables proxy"
I1204 03:03:04.021717       1 server.go:677] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E1204 03:03:04.021930       1 server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I1204 03:03:04.424595       1 server.go:243] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I1204 03:03:04.424684       1 server_linux.go:169] "Using iptables Proxier"
I1204 03:03:04.431330       1 proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
E1204 03:03:04.431962       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv4"
E1204 03:03:04.432541       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv6"
I1204 03:03:04.517601       1 server.go:483] "Version info" version="v1.31.0"
I1204 03:03:04.517670       1 server.go:485] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1204 03:03:04.521986       1 config.go:197] "Starting service config controller"
I1204 03:03:04.521998       1 config.go:104] "Starting endpoint slice config controller"
I1204 03:03:04.522013       1 config.go:326] "Starting node config controller"
I1204 03:03:04.525020       1 shared_informer.go:313] Waiting for caches to sync for node config
I1204 03:03:04.525266       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I1204 03:03:04.525493       1 shared_informer.go:313] Waiting for caches to sync for service config
I1204 03:03:04.625607       1 shared_informer.go:320] Caches are synced for endpoint slice config
I1204 03:03:04.625972       1 shared_informer.go:320] Caches are synced for service config
I1204 03:03:04.625923       1 shared_informer.go:320] Caches are synced for node config


==> kube-proxy [7c5aad3e6be9] <==
E1217 04:48:42.573320       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
E1217 04:48:42.573840       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
I1217 04:48:42.968490       1 server_linux.go:66] "Using iptables proxy"
I1217 04:48:44.871740       1 server.go:677] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E1217 04:48:44.872023       1 server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I1217 04:48:45.574124       1 server.go:243] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I1217 04:48:45.574214       1 server_linux.go:169] "Using iptables Proxier"
I1217 04:48:45.673665       1 proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
E1217 04:48:45.675777       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv4"
E1217 04:48:45.762449       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv6"
I1217 04:48:45.768716       1 server.go:483] "Version info" version="v1.31.0"
I1217 04:48:45.768766       1 server.go:485] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1217 04:48:45.775259       1 config.go:197] "Starting service config controller"
I1217 04:48:45.776218       1 config.go:104] "Starting endpoint slice config controller"
I1217 04:48:45.777878       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I1217 04:48:45.777914       1 shared_informer.go:313] Waiting for caches to sync for service config
I1217 04:48:45.777965       1 config.go:326] "Starting node config controller"
I1217 04:48:45.777973       1 shared_informer.go:313] Waiting for caches to sync for node config
I1217 04:48:45.879119       1 shared_informer.go:320] Caches are synced for endpoint slice config
I1217 04:48:45.879178       1 shared_informer.go:320] Caches are synced for node config
I1217 04:48:45.879192       1 shared_informer.go:320] Caches are synced for service config


==> kube-scheduler [7000a86d03c4] <==
I1217 04:48:25.568376       1 serving.go:386] Generated self-signed cert in-memory
W1217 04:48:30.379066       1 requestheader_controller.go:196] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W1217 04:48:30.379150       1 authentication.go:370] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W1217 04:48:30.379168       1 authentication.go:371] Continuing without authentication configuration. This may treat all requests as anonymous.
W1217 04:48:30.379176       1 authentication.go:372] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I1217 04:48:30.569656       1 server.go:167] "Starting Kubernetes Scheduler" version="v1.31.0"
I1217 04:48:30.569741       1 server.go:169] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1217 04:48:30.574481       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1217 04:48:30.574584       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I1217 04:48:30.574690       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I1217 04:48:30.578273       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I1217 04:48:30.771755       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file


==> kube-scheduler [be996d7b38ca] <==
I1204 03:02:46.447079       1 serving.go:386] Generated self-signed cert in-memory
W1204 03:02:49.423330       1 requestheader_controller.go:196] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W1204 03:02:49.423382       1 authentication.go:370] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W1204 03:02:49.423397       1 authentication.go:371] Continuing without authentication configuration. This may treat all requests as anonymous.
W1204 03:02:49.423405       1 authentication.go:372] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I1204 03:02:49.531127       1 server.go:167] "Starting Kubernetes Scheduler" version="v1.31.0"
I1204 03:02:49.531200       1 server.go:169] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1204 03:02:49.533431       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1204 03:02:49.533559       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I1204 03:02:49.535278       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I1204 03:02:49.535867       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I1204 03:02:49.717941       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file


==> kubelet <==
Dec 17 04:48:21 minikube kubelet[1898]: E1217 04:48:21.296799    1898 log.go:32] "RemoveContainer from runtime service failed" err="rpc error: code = Unknown desc = failed to remove container \"b5ae3bb3fbd14f7af4492a02b0cbb4d3cf8e96848cb7c5b0261daa66269106d7\": Error response from daemon: removal of container b5ae3bb3fbd14f7af4492a02b0cbb4d3cf8e96848cb7c5b0261daa66269106d7 is already in progress" containerID="b5ae3bb3fbd14f7af4492a02b0cbb4d3cf8e96848cb7c5b0261daa66269106d7"
Dec 17 04:48:21 minikube kubelet[1898]: E1217 04:48:21.296895    1898 kuberuntime_gc.go:150] "Failed to remove container" err="rpc error: code = Unknown desc = failed to remove container \"b5ae3bb3fbd14f7af4492a02b0cbb4d3cf8e96848cb7c5b0261daa66269106d7\": Error response from daemon: removal of container b5ae3bb3fbd14f7af4492a02b0cbb4d3cf8e96848cb7c5b0261daa66269106d7 is already in progress" containerID="b5ae3bb3fbd14f7af4492a02b0cbb4d3cf8e96848cb7c5b0261daa66269106d7"
Dec 17 04:48:21 minikube kubelet[1898]: I1217 04:48:21.296933    1898 scope.go:117] "RemoveContainer" containerID="27d9891a0fb6983cf8e49ea91625be34d02bf1c441642bd3f99624f7865c8530"
Dec 17 04:48:21 minikube kubelet[1898]: E1217 04:48:21.365457    1898 kuberuntime_gc.go:150] "Failed to remove container" err="failed to get container status \"27d9891a0fb6983cf8e49ea91625be34d02bf1c441642bd3f99624f7865c8530\": rpc error: code = Unknown desc = Error response from daemon: No such container: 27d9891a0fb6983cf8e49ea91625be34d02bf1c441642bd3f99624f7865c8530" containerID="27d9891a0fb6983cf8e49ea91625be34d02bf1c441642bd3f99624f7865c8530"
Dec 17 04:48:21 minikube kubelet[1898]: I1217 04:48:21.381421    1898 scope.go:117] "RemoveContainer" containerID="b5ae3bb3fbd14f7af4492a02b0cbb4d3cf8e96848cb7c5b0261daa66269106d7"
Dec 17 04:48:21 minikube kubelet[1898]: E1217 04:48:21.382865    1898 log.go:32] "ContainerStatus from runtime service failed" err="rpc error: code = Unknown desc = Error response from daemon: No such container: b5ae3bb3fbd14f7af4492a02b0cbb4d3cf8e96848cb7c5b0261daa66269106d7" containerID="b5ae3bb3fbd14f7af4492a02b0cbb4d3cf8e96848cb7c5b0261daa66269106d7"
Dec 17 04:48:21 minikube kubelet[1898]: I1217 04:48:21.382925    1898 pod_container_deletor.go:53] "DeleteContainer returned error" containerID={"Type":"docker","ID":"b5ae3bb3fbd14f7af4492a02b0cbb4d3cf8e96848cb7c5b0261daa66269106d7"} err="failed to get container status \"b5ae3bb3fbd14f7af4492a02b0cbb4d3cf8e96848cb7c5b0261daa66269106d7\": rpc error: code = Unknown desc = Error response from daemon: No such container: b5ae3bb3fbd14f7af4492a02b0cbb4d3cf8e96848cb7c5b0261daa66269106d7"
Dec 17 04:48:23 minikube kubelet[1898]: I1217 04:48:23.680626    1898 kubelet_node_status.go:72] "Attempting to register node" node="minikube"
Dec 17 04:48:27 minikube kubelet[1898]: E1217 04:48:27.369267    1898 eviction_manager.go:285] "Eviction manager: failed to get summary stats" err="failed to get node info: node \"minikube\" not found"
Dec 17 04:48:30 minikube kubelet[1898]: I1217 04:48:30.576760    1898 kubelet_node_status.go:111] "Node was previously registered" node="minikube"
Dec 17 04:48:30 minikube kubelet[1898]: I1217 04:48:30.576891    1898 kubelet_node_status.go:75] "Successfully registered node" node="minikube"
Dec 17 04:48:30 minikube kubelet[1898]: I1217 04:48:30.576920    1898 kuberuntime_manager.go:1633] "Updating runtime config through cri with podcidr" CIDR="10.244.0.0/24"
Dec 17 04:48:30 minikube kubelet[1898]: I1217 04:48:30.577977    1898 kubelet_network.go:61] "Updating Pod CIDR" originalPodCIDR="" newPodCIDR="10.244.0.0/24"
Dec 17 04:48:30 minikube kubelet[1898]: E1217 04:48:30.996880    1898 kubelet.go:1915] "Failed creating a mirror pod for" err="pods \"kube-apiserver-minikube\" already exists" pod="kube-system/kube-apiserver-minikube"
Dec 17 04:48:31 minikube kubelet[1898]: I1217 04:48:31.073843    1898 apiserver.go:52] "Watching apiserver"
Dec 17 04:48:31 minikube kubelet[1898]: I1217 04:48:31.179197    1898 desired_state_of_world_populator.go:154] "Finished populating initial desired state of world"
Dec 17 04:48:31 minikube kubelet[1898]: E1217 04:48:31.179604    1898 kubelet.go:1915] "Failed creating a mirror pod for" err="pods \"kube-scheduler-minikube\" already exists" pod="kube-system/kube-scheduler-minikube"
Dec 17 04:48:31 minikube kubelet[1898]: I1217 04:48:31.277565    1898 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"xtables-lock\" (UniqueName: \"kubernetes.io/host-path/e4675888-a664-4f0f-9b8a-f2360aba5e4e-xtables-lock\") pod \"kube-proxy-gdfs7\" (UID: \"e4675888-a664-4f0f-9b8a-f2360aba5e4e\") " pod="kube-system/kube-proxy-gdfs7"
Dec 17 04:48:31 minikube kubelet[1898]: I1217 04:48:31.277850    1898 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host-path/e4675888-a664-4f0f-9b8a-f2360aba5e4e-lib-modules\") pod \"kube-proxy-gdfs7\" (UID: \"e4675888-a664-4f0f-9b8a-f2360aba5e4e\") " pod="kube-system/kube-proxy-gdfs7"
Dec 17 04:48:31 minikube kubelet[1898]: I1217 04:48:31.277926    1898 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tmp\" (UniqueName: \"kubernetes.io/host-path/f39d2542-0b37-4407-b74c-7be5b1ae4548-tmp\") pod \"storage-provisioner\" (UID: \"f39d2542-0b37-4407-b74c-7be5b1ae4548\") " pod="kube-system/storage-provisioner"
Dec 17 04:48:31 minikube kubelet[1898]: I1217 04:48:31.974774    1898 scope.go:117] "RemoveContainer" containerID="5e85f879db8ecb53633ad5da0e986179a2eb451474842cfa49364a53d51b6349"
Dec 17 04:48:36 minikube kubelet[1898]: I1217 04:48:36.977351    1898 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="22914414498f6db87c24ac6209d69430e390e1258e6b778f7b8697dad66aea6b"
Dec 17 04:48:38 minikube kubelet[1898]: E1217 04:48:38.766074    1898 summary_sys_containers.go:51] "Failed to get system container stats" err="failed to get cgroup stats for \"/kubepods\": failed to get container info for \"/kubepods\": unknown container \"/kubepods\"" containerName="/kubepods"
Dec 17 04:48:38 minikube kubelet[1898]: E1217 04:48:38.766169    1898 helpers.go:854] "Eviction manager: failed to construct signal" err="system container \"pods\" not found in metrics" signal="allocatableMemory.available"
Dec 17 04:48:38 minikube kubelet[1898]: I1217 04:48:38.768922    1898 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="10280409a581cf57e788d1f343e1534b438c4f26e3ff11eebad6cb40bf13f6bf"
Dec 17 04:48:39 minikube kubelet[1898]: I1217 04:48:39.173407    1898 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="291526bb8c1b8447a931808606453a6a744c691714a344c0e66e5e72071844a4"
Dec 17 04:48:39 minikube kubelet[1898]: I1217 04:48:39.268795    1898 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="892cf2a28376274b1c59b68145352d5c86ebfa267457aa7927af850247dfb436"
Dec 17 04:48:39 minikube kubelet[1898]: I1217 04:48:39.469506    1898 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="299e476689e9dabacc4ce984166dba6b4f5b63881b4d62faf69d5030f77e2e38"
Dec 17 04:48:39 minikube kubelet[1898]: I1217 04:48:39.681071    1898 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="551270d84ce97b84595a95b08926d3a099af762ee452fda3a1e0f8866f830053"
Dec 17 04:48:40 minikube kubelet[1898]: I1217 04:48:40.467435    1898 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="499255d27edb61d6f3b1cc6bae49b03b64c85aa1386f795c9ff7c134e4ca6431"
Dec 17 04:48:49 minikube kubelet[1898]: I1217 04:48:49.383498    1898 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="5e85f879db8ecb53633ad5da0e986179a2eb451474842cfa49364a53d51b6349"
Dec 17 04:48:49 minikube kubelet[1898]: I1217 04:48:49.673768    1898 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="2c75eaa10af5cd0bad2eb48cd3677207be000055190da305fb8dd4506a75d8c2"
Dec 17 04:48:49 minikube kubelet[1898]: I1217 04:48:49.869904    1898 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="afa9539df364fdb76c8526930205a6b2d1bef7c84fb3063c0a05e5752a91379b"
Dec 17 04:48:49 minikube kubelet[1898]: E1217 04:48:49.967374    1898 summary_sys_containers.go:51] "Failed to get system container stats" err="failed to get cgroup stats for \"/kubepods\": failed to get container info for \"/kubepods\": unknown container \"/kubepods\"" containerName="/kubepods"
Dec 17 04:48:49 minikube kubelet[1898]: E1217 04:48:49.967479    1898 helpers.go:854] "Eviction manager: failed to construct signal" err="system container \"pods\" not found in metrics" signal="allocatableMemory.available"
Dec 17 04:48:50 minikube kubelet[1898]: I1217 04:48:50.068670    1898 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="cab7d57d82008f75fd72bbd641f103e7f5cae617d72a5de4cbebed0a65e47102"
Dec 17 04:48:50 minikube kubelet[1898]: I1217 04:48:50.273350    1898 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="67a54e2a58c677b83e766e46f9c41567b069632efa3a6bee802819aedb84a0d0"
Dec 17 04:48:53 minikube kubelet[1898]: I1217 04:48:53.189909    1898 scope.go:117] "RemoveContainer" containerID="e8441b85f0b6157ccb58b7b24734ab5695e9a6fa88257aa3cc0ba860b2bb7cc5"
Dec 17 04:48:53 minikube kubelet[1898]: I1217 04:48:53.190174    1898 scope.go:117] "RemoveContainer" containerID="52700b42ce1cdb8a9652c8ded65d48add90c97664178888cb47e491d57ecba66"
Dec 17 04:48:54 minikube kubelet[1898]: I1217 04:48:54.070096    1898 scope.go:117] "RemoveContainer" containerID="59dab591506a23c4d4735337cab9eaa1b5c802766008a071037f17a0148ab757"
Dec 17 04:48:54 minikube kubelet[1898]: I1217 04:48:54.070412    1898 scope.go:117] "RemoveContainer" containerID="88ef00d17293d0bee6016f451de54d7df437a12a7e706927f65d1464008a329e"
Dec 17 04:48:56 minikube kubelet[1898]: I1217 04:48:56.602351    1898 scope.go:117] "RemoveContainer" containerID="186b6c441af1dcc1731973d2b3fd0e9e19738bb059bd6e8b9d9c255e2d46436d"
Dec 17 04:49:00 minikube kubelet[1898]: E1217 04:49:00.091796    1898 summary_sys_containers.go:51] "Failed to get system container stats" err="failed to get cgroup stats for \"/kubepods\": failed to get container info for \"/kubepods\": unknown container \"/kubepods\"" containerName="/kubepods"
Dec 17 04:49:00 minikube kubelet[1898]: E1217 04:49:00.091876    1898 helpers.go:854] "Eviction manager: failed to construct signal" err="system container \"pods\" not found in metrics" signal="allocatableMemory.available"
Dec 17 04:49:10 minikube kubelet[1898]: E1217 04:49:10.270337    1898 summary_sys_containers.go:51] "Failed to get system container stats" err="failed to get cgroup stats for \"/kubepods\": failed to get container info for \"/kubepods\": unknown container \"/kubepods\"" containerName="/kubepods"
Dec 17 04:49:10 minikube kubelet[1898]: E1217 04:49:10.270406    1898 helpers.go:854] "Eviction manager: failed to construct signal" err="system container \"pods\" not found in metrics" signal="allocatableMemory.available"
Dec 17 04:53:17 minikube kubelet[1898]: I1217 04:53:17.365278    1898 container_manager_linux.go:806] "CPUAccounting not enabled for process" pid=1898
Dec 17 04:53:17 minikube kubelet[1898]: I1217 04:53:17.365408    1898 container_manager_linux.go:809] "MemoryAccounting not enabled for process" pid=1898
Dec 17 04:58:17 minikube kubelet[1898]: I1217 04:58:17.365890    1898 container_manager_linux.go:806] "CPUAccounting not enabled for process" pid=1898
Dec 17 04:58:17 minikube kubelet[1898]: I1217 04:58:17.366016    1898 container_manager_linux.go:809] "MemoryAccounting not enabled for process" pid=1898
Dec 17 05:03:17 minikube kubelet[1898]: I1217 05:03:17.366501    1898 container_manager_linux.go:806] "CPUAccounting not enabled for process" pid=1898
Dec 17 05:03:17 minikube kubelet[1898]: I1217 05:03:17.366599    1898 container_manager_linux.go:809] "MemoryAccounting not enabled for process" pid=1898
Dec 17 05:08:17 minikube kubelet[1898]: I1217 05:08:17.367514    1898 container_manager_linux.go:806] "CPUAccounting not enabled for process" pid=1898
Dec 17 05:08:17 minikube kubelet[1898]: I1217 05:08:17.367620    1898 container_manager_linux.go:809] "MemoryAccounting not enabled for process" pid=1898
Dec 17 05:13:17 minikube kubelet[1898]: I1217 05:13:17.368086    1898 container_manager_linux.go:806] "CPUAccounting not enabled for process" pid=1898
Dec 17 05:13:17 minikube kubelet[1898]: I1217 05:13:17.368172    1898 container_manager_linux.go:809] "MemoryAccounting not enabled for process" pid=1898
Dec 17 05:18:17 minikube kubelet[1898]: I1217 05:18:17.368592    1898 container_manager_linux.go:806] "CPUAccounting not enabled for process" pid=1898
Dec 17 05:18:17 minikube kubelet[1898]: I1217 05:18:17.368731    1898 container_manager_linux.go:809] "MemoryAccounting not enabled for process" pid=1898
Dec 17 05:23:17 minikube kubelet[1898]: I1217 05:23:17.369701    1898 container_manager_linux.go:806] "CPUAccounting not enabled for process" pid=1898
Dec 17 05:23:17 minikube kubelet[1898]: I1217 05:23:17.369830    1898 container_manager_linux.go:809] "MemoryAccounting not enabled for process" pid=1898


==> storage-provisioner [0e94c4563ca5] <==
I1217 04:48:55.167898       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I1217 04:48:55.269176       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I1217 04:48:55.269922       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I1217 04:49:12.772578       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I1217 04:49:12.772912       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_700a4f75-6f65-4883-af6a-4aaf9fe79898!
I1217 04:49:12.773025       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"d522b7af-8405-409e-bebd-ae2faa69398b", APIVersion:"v1", ResourceVersion:"85764", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_700a4f75-6f65-4883-af6a-4aaf9fe79898 became leader
I1217 04:49:12.874950       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_700a4f75-6f65-4883-af6a-4aaf9fe79898!
I1217 04:49:12.875108       1 controller.go:1472] delete "pvc-f63389dc-baa1-4aab-a11a-a3f1e0f2b946": started
I1217 04:49:12.875118       1 storage_provisioner.go:98] Deleting volume &PersistentVolume{ObjectMeta:{pvc-f63389dc-baa1-4aab-a11a-a3f1e0f2b946    49ce91d0-f131-48df-91cc-574866ab04a1 64614 0 2024-12-03 04:23:09 +0000 UTC <nil> <nil> map[] map[hostPathProvisionerIdentity:0dc4273a-9751-4b31-88ab-e5e7a5912df1 pv.kubernetes.io/provisioned-by:k8s.io/minikube-hostpath] [] [kubernetes.io/pv-protection]  [{storage-provisioner Update v1 2024-12-03 04:23:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:hostPathProvisionerIdentity":{},"f:pv.kubernetes.io/provisioned-by":{}}},"f:spec":{"f:accessModes":{},"f:capacity":{".":{},"f:storage":{}},"f:claimRef":{".":{},"f:apiVersion":{},"f:kind":{},"f:name":{},"f:namespace":{},"f:resourceVersion":{},"f:uid":{}},"f:hostPath":{".":{},"f:path":{},"f:type":{}},"f:persistentVolumeReclaimPolicy":{},"f:storageClassName":{},"f:volumeMode":{}}}} {kube-controller-manager Update v1 2024-12-03 04:36:19 +0000 UTC FieldsV1 {"f:status":{"f:phase":{}}}}]},Spec:PersistentVolumeSpec{Capacity:ResourceList{storage: {{10737418240 0} {<nil>} 10Gi BinarySI},},PersistentVolumeSource:PersistentVolumeSource{GCEPersistentDisk:nil,AWSElasticBlockStore:nil,HostPath:&HostPathVolumeSource{Path:/tmp/hostpath-provisioner/default/my-release-wordpress,Type:*,},Glusterfs:nil,NFS:nil,RBD:nil,ISCSI:nil,Cinder:nil,CephFS:nil,FC:nil,Flocker:nil,FlexVolume:nil,AzureFile:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Local:nil,StorageOS:nil,CSI:nil,},AccessModes:[ReadWriteOnce],ClaimRef:&ObjectReference{Kind:PersistentVolumeClaim,Namespace:default,Name:my-release-wordpress,UID:f63389dc-baa1-4aab-a11a-a3f1e0f2b946,APIVersion:v1,ResourceVersion:63336,FieldPath:,},PersistentVolumeReclaimPolicy:Delete,StorageClassName:standard,MountOptions:[],VolumeMode:*Filesystem,NodeAffinity:nil,},Status:PersistentVolumeStatus{Phase:Released,Message:,Reason:,},}
I1217 04:49:12.880646       1 controller.go:1478] delete "pvc-f63389dc-baa1-4aab-a11a-a3f1e0f2b946": volume deletion ignored: ignored because identity annotation on PV does not match ours
I1217 05:04:12.786416       1 controller.go:1472] delete "pvc-f63389dc-baa1-4aab-a11a-a3f1e0f2b946": started
I1217 05:04:12.786468       1 storage_provisioner.go:98] Deleting volume &PersistentVolume{ObjectMeta:{pvc-f63389dc-baa1-4aab-a11a-a3f1e0f2b946    49ce91d0-f131-48df-91cc-574866ab04a1 64614 0 2024-12-03 04:23:09 +0000 UTC <nil> <nil> map[] map[hostPathProvisionerIdentity:0dc4273a-9751-4b31-88ab-e5e7a5912df1 pv.kubernetes.io/provisioned-by:k8s.io/minikube-hostpath] [] [kubernetes.io/pv-protection]  [{storage-provisioner Update v1 2024-12-03 04:23:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:hostPathProvisionerIdentity":{},"f:pv.kubernetes.io/provisioned-by":{}}},"f:spec":{"f:accessModes":{},"f:capacity":{".":{},"f:storage":{}},"f:claimRef":{".":{},"f:apiVersion":{},"f:kind":{},"f:name":{},"f:namespace":{},"f:resourceVersion":{},"f:uid":{}},"f:hostPath":{".":{},"f:path":{},"f:type":{}},"f:persistentVolumeReclaimPolicy":{},"f:storageClassName":{},"f:volumeMode":{}}}} {kube-controller-manager Update v1 2024-12-03 04:36:19 +0000 UTC FieldsV1 {"f:status":{"f:phase":{}}}}]},Spec:PersistentVolumeSpec{Capacity:ResourceList{storage: {{10737418240 0} {<nil>} 10Gi BinarySI},},PersistentVolumeSource:PersistentVolumeSource{GCEPersistentDisk:nil,AWSElasticBlockStore:nil,HostPath:&HostPathVolumeSource{Path:/tmp/hostpath-provisioner/default/my-release-wordpress,Type:*,},Glusterfs:nil,NFS:nil,RBD:nil,ISCSI:nil,Cinder:nil,CephFS:nil,FC:nil,Flocker:nil,FlexVolume:nil,AzureFile:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Local:nil,StorageOS:nil,CSI:nil,},AccessModes:[ReadWriteOnce],ClaimRef:&ObjectReference{Kind:PersistentVolumeClaim,Namespace:default,Name:my-release-wordpress,UID:f63389dc-baa1-4aab-a11a-a3f1e0f2b946,APIVersion:v1,ResourceVersion:63336,FieldPath:,},PersistentVolumeReclaimPolicy:Delete,StorageClassName:standard,MountOptions:[],VolumeMode:*Filesystem,NodeAffinity:nil,},Status:PersistentVolumeStatus{Phase:Released,Message:,Reason:,},}
I1217 05:04:12.786651       1 controller.go:1478] delete "pvc-f63389dc-baa1-4aab-a11a-a3f1e0f2b946": volume deletion ignored: ignored because identity annotation on PV does not match ours
I1217 05:19:12.787676       1 controller.go:1472] delete "pvc-f63389dc-baa1-4aab-a11a-a3f1e0f2b946": started
I1217 05:19:12.787787       1 storage_provisioner.go:98] Deleting volume &PersistentVolume{ObjectMeta:{pvc-f63389dc-baa1-4aab-a11a-a3f1e0f2b946    49ce91d0-f131-48df-91cc-574866ab04a1 64614 0 2024-12-03 04:23:09 +0000 UTC <nil> <nil> map[] map[hostPathProvisionerIdentity:0dc4273a-9751-4b31-88ab-e5e7a5912df1 pv.kubernetes.io/provisioned-by:k8s.io/minikube-hostpath] [] [kubernetes.io/pv-protection]  [{storage-provisioner Update v1 2024-12-03 04:23:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:hostPathProvisionerIdentity":{},"f:pv.kubernetes.io/provisioned-by":{}}},"f:spec":{"f:accessModes":{},"f:capacity":{".":{},"f:storage":{}},"f:claimRef":{".":{},"f:apiVersion":{},"f:kind":{},"f:name":{},"f:namespace":{},"f:resourceVersion":{},"f:uid":{}},"f:hostPath":{".":{},"f:path":{},"f:type":{}},"f:persistentVolumeReclaimPolicy":{},"f:storageClassName":{},"f:volumeMode":{}}}} {kube-controller-manager Update v1 2024-12-03 04:36:19 +0000 UTC FieldsV1 {"f:status":{"f:phase":{}}}}]},Spec:PersistentVolumeSpec{Capacity:ResourceList{storage: {{10737418240 0} {<nil>} 10Gi BinarySI},},PersistentVolumeSource:PersistentVolumeSource{GCEPersistentDisk:nil,AWSElasticBlockStore:nil,HostPath:&HostPathVolumeSource{Path:/tmp/hostpath-provisioner/default/my-release-wordpress,Type:*,},Glusterfs:nil,NFS:nil,RBD:nil,ISCSI:nil,Cinder:nil,CephFS:nil,FC:nil,Flocker:nil,FlexVolume:nil,AzureFile:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Local:nil,StorageOS:nil,CSI:nil,},AccessModes:[ReadWriteOnce],ClaimRef:&ObjectReference{Kind:PersistentVolumeClaim,Namespace:default,Name:my-release-wordpress,UID:f63389dc-baa1-4aab-a11a-a3f1e0f2b946,APIVersion:v1,ResourceVersion:63336,FieldPath:,},PersistentVolumeReclaimPolicy:Delete,StorageClassName:standard,MountOptions:[],VolumeMode:*Filesystem,NodeAffinity:nil,},Status:PersistentVolumeStatus{Phase:Released,Message:,Reason:,},}
I1217 05:19:12.788153       1 controller.go:1478] delete "pvc-f63389dc-baa1-4aab-a11a-a3f1e0f2b946": volume deletion ignored: ignored because identity annotation on PV does not match ours


==> storage-provisioner [88ef00d17293] <==
I1217 04:48:41.770491       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F1217 04:48:42.563213       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": tls: server selected unsupported protocol version 301

